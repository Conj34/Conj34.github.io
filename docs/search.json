[
  {
    "objectID": "main_pages/research.html",
    "href": "main_pages/research.html",
    "title": "Research",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\n\nPapersTalksTheses and ProjectsTripsOther\n\n\n\n\n\n\n\n\n\n\n\n\n\nTitle\nPeople\nDate\nInstit.\nStatus\n\n\n\n\nDensity of small diameter subgraphs in \\(K_r\\)-free graphs (II)\nD.M.\n2023-\nLSE\n\n\n\nPartition universality for hypergraphs\nP. Allen, J. Böttcher, D.M.\n2023-\nLSE\n\n\n\nConvergence of policy-gradiente methods in repeated games\nG. Ashkenazi-Golan, D.M., E. Plumb\n2023-\nLSE\n\n\n\nProduct-free subsets of \\([n]\\)\nL. Mattos, D.M., O. Parczyk\n2023-\nLSE\n\n\n\nMaker Breaker Games in the square lattice\nF. Illingworth, D.M., A. Pokrovskiy\n2023-\nLSE\n\n\n\nGraphs with large minimum degree and no small odd cycles are 3-colourable\nJ. Böttcher, N. Frankl, D. M., O. Parczyk, J. Skokan\n2020-22\nLSE\n\n\n\nThe Ramsey numbers of squares of paths and cycles\nPeter Allen, D.M., Barnaby Roberts, Jozef Skokan\n2020-22\nLSE\n\n\n\nDensity of small diameter subgraphs in \\(K_r\\)-free graphs\nEng Keat Hng, D.M.\n2021\nLSE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTitle\nDate\nOccasion\nPlace\n\n\n\n\nThe Ramsey numbers of \\(P_{3n}^2\\) and \\(C_{3n}^2\\)\nSep ’23\nDMV (Invited)\nIlmenau\n\n\n\nApr ’23\nInvited seminar\nCharles Univ. Prague\n\n\n\nJun ’22\nInvited seminar\nTU Hamburg\n\n\n\nJul ’22\nICGT\nMontpellier\n\n\nChromatic profile of \\(\\{C_3,\\dots{},C_{2k-1}\\}\\)\nMar ’23\nPCC\nBirmingham\n\n\n\nJul ’22\nRSA\nPoznan\n\n\nAbout the Pentagon Conjecture\nNov ’20\nLSE Seminar\nLondon\n\n\nMinimal Ramsey Graphs for Ciclicity\nMay ’19\nETHZ Mittagsseminar\nZurich\n\n\nStudio della dimensione di Hausdorff del Moto Browniano\nJul ’18\nBSc defense\nPisa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat\nWhen\nWhere\nWhy\n\n\n\n\nGraph-theoretical Tools in Statistics\n2022\nLSE\nMini-internship\n\n\nA discussion about the Pentagon Problem\n2020\nETHZ\nMSc Thesis\n\n\nSeparator Theorems\n2019\nETHZ\nSemester Paper\n\n\nA Glimpse of Young’s Tableaux\n2019\nETHZ\nReading Course\n\n\nOn the Hausdorff Dimension of Brownian Motion\n2018\nPisa Univ.\nBSc Thesis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat\nWhen\nWhere\nInstit.\nType\nFunded\n\n\n\n\nDMV\nOct ’23\nIlmenau\n\nConference (invited)\nLSE\n\n\nCPMC\nSept ’23\nZagreb\n\nWorkshop\nLSE\n\n\nEurocomb\nAug ’23\nPrague\n\nConference\nLSE\n\n\nEEML\nJul ’23\nKošice\n\nSummer School\nLSE\n\n\nALGA\nJun ’23\nRagusa\n\nConference\nCIVICA Grant\n\n\nUCLW\nMay ’23\nLondon\nUCL\nWorkshop\n\n\n\nPSSC\nApr ’23\nPrague\nCharles Uni.\nSummer School\nCharles Uni.\n\n\nPCC\nMar ’23\nBirmingham\n\nConference\nLSE\n\n\nResearch Visit\nAug ’22\nBerlin\nFU Berlin\nVisit\nLMS Grant\n\n\nRSA\nAug ’22\nGniezno\n\nConference\nLSE\n\n\nResearch visit\nJul ’22\nPrague\nCzech Academy of Science\nVisit\nCzech Academy of Science\n\n\nPSSDM\nJul ’22\nPrague\nCharles Uni.\nSummer School\nSummer School\n\n\nICGT\nJul ’22\nMontpellier\n\nConference\nLSE\n\n\nResearch visit\nMay ’22\nHamburg\nTU Hamburg\nVisit\nTU Hamburg\n\n\n\n\n\n\nOrganiser\n\nPCC (Main organiser), Apr 2023, University of London and BCC. (National conference for postgraduate students).\nLSE PhD CGO Seminar (Main Organiser), 2022/23.\n\nGrants and invitations\n\nDMV Invited Speaker. Ilmenau. 2023.\nLMS Computer Science Small Grant (700£). London Mathematical Society. Funded visit to Berlin. 2022.\nLSE Contribution Award. Dept of Mathematics. LSE. 2021."
  },
  {
    "objectID": "main_pages/curriculum.html",
    "href": "main_pages/curriculum.html",
    "title": "Curriculum",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\nRelevant documents:\n\nCourse content of many courses I followed and all courses I taught.\nTranscript of Records of BSc and MSc.\nNormal CV.\n\n\n\n\n\n\ngantt\n    title Education\n    dateFormat DD-MM-YYYY\n    axisFormat   %b %Y\n    section BSc\n    Pisa Univ - GPA 110/110: L1, 01-09-2015, 1048d\n    section MSc\n    ETHZ - GPA 5.8/6: L1, 01-09-2018, 700d\n    section PhD\n    London School of Economics: L1, 01-09-2020, 1450d\n\n\n\n\n\n\n\n\n\n\njourney\n    title Technical Skills\n      Maths: 9: Research, Taught\n      ML, RL and Stats: 8: Taught, Research\n      Python: 7: Hobby, Taught\n      Finance: 4: Taught"
  },
  {
    "objectID": "main_pages/miscellanea.html",
    "href": "main_pages/miscellanea.html",
    "title": "Miscellanea",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\nAmongst the things I like: Books, Movies, Cooking.\n\nBooks\n\nI like reading books. This is a great website to find your next read (if you’re stuck for ideas); using the list provided on this website, I decided I want to put some work in reading the works of Dostoyevsky, Kafka, Hemingway, Faulkner, Woolf, Dickens, Sheakespere, Henry James, Beckett, Updike, Coetzee and Alice Munro. This will take me many years, but you got to start somewhere.\n\nLately I’ve been using a lot the platform Audible, which allows you to listen to unabridged readings.\n\n Read since Jan. 2023 \n\n\n\n\n\n\n\n\n\n\n\nTitle\nAuthor\nStatus\nCompl. on\n\n\n\n\nLa Storia\nElsa Morante\n\n\n\n\nOedipus Rex\nSophocles\n\nAug ’23\n\n\nPantera\nStefano Benni\n\nAug ’23\n\n\nBlues in Sedici\nStefano Benni\n\nAug ’23\n\n\nThe Pastures of Heaven\nJohn Steinbeck\n\nAug ’23\n\n\nThe Remains of the Day\nKazuo Ishiguro\n\nJul ’23\n\n\nThe Return of the King\nJ.R.R. Tolkien\n\nJul ’23\n\n\nSwimming in the Dark\nTomasz Jedrowski\n\nJul ’23\n\n\nThe Two Towers\nJ.R.R. Tolkien\n\nJun ’23\n\n\nThe Fellowship of the Ring\nJ.R.R. Tolkien\n\nJun ’23\n\n\nIl Bar sotto il Mare\nStefano Benni\n\nMay ’23\n\n\n2001: A Space Odyssey\nArthur C. Clarke\n\nMay ’23\n\n\nNever let Me go\nKazuo Ishiguro\n\nMay ’23\n\n\nNocturnes\nKazuo Ishiguro\n\nMay ’23\n\n\nThe Time Machine\nH.G. Wells\n\nMay ’23\n\n\nA little Life\nHanya Yanagihara\n\nMay ’23\n\n\nA Room with a View\nE.M. Forster\n\nApr ’23\n\n\nDrive your Plow over the Bones of the Dead\nOlga Tokarczuk\n\nMar ’23\n\n\n\n\nNext up (Suggestions appreciated) \n\n\n\n\n\n\n\n\n\nTitle\nAuthor\n\n\n\n\nEnsaio sobre a cegueira\nJosé Saramago\n\n\nTo the Lighthouse\nVirginia Woolf\n\n\nAtonement\nIan McEwan\n\n\nThe Sense of an Ending\nJulian Barnes\n\n\nHowards End\nE.M. Forster\n\n\nWar of the Worlds\nH. G. Wells\n\n\nSphere\nMichael Crichton\n\n\nThe Line of Beauty\nAlan Hollinghurst"
  },
  {
    "objectID": "Documents_files/ST455/Week8/Seminar8/HW8_solution.html",
    "href": "Documents_files/ST455/Week8/Seminar8/HW8_solution.html",
    "title": "Solution to HW8",
    "section": "",
    "text": "Consider the following example with 7 states:\nF - A - B - C - D - E - S\nHere, F and S corresponds to the terminal states. On each state, we can choose either to move to the left, or move to the right. Rewards is 0 if we move from E to S and -1 otherwise. The initial location is given by state C, in the middle."
  },
  {
    "objectID": "Documents_files/ST455/Week8/Seminar8/HW8_solution.html#first-let-us-review-the-reinforce-algorithm",
    "href": "Documents_files/ST455/Week8/Seminar8/HW8_solution.html#first-let-us-review-the-reinforce-algorithm",
    "title": "Solution to HW8",
    "section": "First, let us review the REINFORCE algorithm",
    "text": "First, let us review the REINFORCE algorithm\n\nIn our example, we combine a table lookup model with a logistic regression model to model \\(\\pi\\). Specifically, for each state \\(s\\), we can model the probability of moving to the right (or to the left) as \\[\\exp(\\theta_s)/[1+\\exp(\\theta_s)].\\] With some calculations, it is easy to show that the policy score equals \\[\\begin{eqnarray*}\n    \\nabla_{\\theta_s} \\log(\\pi(s,\\textrm{right};\\theta_s))&=&1-\\frac{\\exp(\\theta_s)}{1+\\exp(\\theta_s)}=\\frac{1}{1+\\exp(\\theta_s)},\\\\\n    \\nabla_{\\theta_s} \\log(\\pi(s,\\textrm{left};\\theta_s))&=&-\\frac{\\exp(\\theta_s)}{1+\\exp(\\theta_s)}=-\\frac{1}{1+\\exp(-\\theta_s)}.\n\\end{eqnarray*}\\] We will use these policy scores to update the policy parameter."
  },
  {
    "objectID": "Documents_files/ST455/Week8/Seminar8/HW8_solution.html#second-let-us-implement-the-random-walk-environment-we-use-similar-code-in-seminar-4",
    "href": "Documents_files/ST455/Week8/Seminar8/HW8_solution.html#second-let-us-implement-the-random-walk-environment-we-use-similar-code-in-seminar-4",
    "title": "Solution to HW8",
    "section": "Second, let us implement the random walk environment (we use similar code in Seminar 4)",
    "text": "Second, let us implement the random walk environment (we use similar code in Seminar 4)\n\n# actions\nleft = 0\nright = 1\n\nclass RandomWalk:\n    def __init__(self, initial_state):\n        self.initial_state = initial_state\n        self.state = self.initial_state\n        self.reward = 0.0\n        self.is_terminal = False\n\n    # write step function that returns obs(next state), reward, is_done\n    def step(self, action):\n        if self.state == 5 and action == right:\n            self.state += 1\n            self.is_terminal = True\n            self.reward = 0.0\n        elif self.state == 1 and action == left:\n            self.state -= 1\n            self.is_terminal = True\n            self.reward = -1.0\n        else:\n            if action == left:\n                self.state -= 1\n                self.is_terminal = False\n                self.reward = -1.0\n\n            else:\n                self.state += 1\n                self.is_terminal = False\n                self.reward = -1.0\n\n        return self.state, self.reward, self.is_terminal\n\n    def reset(self):\n        self.state = self.initial_state\n        self.reward = 0.0\n        self.is_terminal = False\n        return self.state"
  },
  {
    "objectID": "Documents_files/ST455/Week8/Seminar8/HW8_solution.html#next-let-us-implement-the-reinforce-algorithm",
    "href": "Documents_files/ST455/Week8/Seminar8/HW8_solution.html#next-let-us-implement-the-reinforce-algorithm",
    "title": "Solution to HW8",
    "section": "Next, let us implement the REINFORCE algorithm",
    "text": "Next, let us implement the REINFORCE algorithm\n\nimport numpy as np\nimport numpy.random as nr\n\ninitial_state = 3  \nepisodes = 1000\nenv = RandomWalk(initial_state)\n\ndef policy(theta):\n    rand = np.random.random()\n    a = 1 if (rand &lt; 1/(1+np.exp(-theta))) else 0\n    return a\n\nalpha = 0.1 # step size\ninitial_state = 3  \nenv = RandomWalk(initial_state)\n\nrewards = np.zeros((episodes, 2))\nnruns = 100\nfor r in range(nruns): \n    Theta = np.zeros(7)\n    for i in range(episodes):\n        state = env.reset()\n        done = False\n        g = 0.0\n        sar = []\n\n        while not done:\n            a = policy(Theta[state])\n            next_state, r, done = env.step(a)\n            sar.append([state, a, r])\n            g += r\n            state = next_state\n\n        rewards[i,0] += g\n\n        ## REINFORCE update\n        for state, a, r in sar:\n            theta = Theta[state]\n            if (a==1):\n                Theta[state] += alpha * g * (1/(1+np.exp(theta)))\n            else:\n                Theta[state] -= alpha * g * (1/(1+np.exp(-theta)))\n            g = g-r\n            \n    Theta = np.zeros(7)\n    for i in range(episodes):\n        state = env.reset()\n        done = False\n        g = 0.0\n        sar = []\n\n        while not done:\n            a = policy(Theta[state])\n            next_state, r, done = env.step(a)\n            sar.append([state, a, r])\n            g += r\n            state = next_state\n\n        rewards[i,1] += g\n\n        ## REINFORCE update\n        for state, a, r in sar:\n            theta = Theta[state]\n            if (a==1):\n                Theta[state] += alpha * (g+2) * (1/(1+np.exp(theta)))\n            else:\n                Theta[state] -= alpha * (g+2) * (1/(1+np.exp(-theta)))\n            g = g-r      \n            \nrewards = rewards / nruns\n\n/var/folders/ry/0r22ct6562n2khj823m6xj_00000gn/T/ipykernel_43227/160178280.py:40: RuntimeWarning: overflow encountered in exp\n  Theta[state] += alpha * g * (1/(1+np.exp(theta)))\n\n\n\nnp.transpose(rewards)\n\narray([[ -7.7 , -22.39,  -9.05, ...,  -2.28,  -2.27,  -2.28],\n       [ -7.65,  -7.46,  -6.52, ...,  -2.06,  -2.1 ,  -2.06]])\n\n\n\nTheta\n\narray([ 0.        , -3.19055275,  1.14485435,  4.6867126 ,  5.16057924,\n        5.32235695,  0.        ])"
  },
  {
    "objectID": "Documents_files/ST455/Week8/Seminar8/HW8_solution.html#finally-let-us-visualise-these-rewards",
    "href": "Documents_files/ST455/Week8/Seminar8/HW8_solution.html#finally-let-us-visualise-these-rewards",
    "title": "Solution to HW8",
    "section": "Finally, let us visualise these rewards",
    "text": "Finally, let us visualise these rewards\n\nimport matplotlib.pyplot as plt\ndef plot_return(rewards):\n    plt.figure(figsize=(8, 6), dpi=80)\n    for a in rewards:\n        plt.plot(a, linewidth=3)\n    plt.xlabel('Episode')\n    plt.ylim(-5, 0)\n    plt.ylabel('Rewards')\n    legend_str = [\"G\", \"G+2\"]\n    plt.legend(legend_str)\n    plt.show()\n\n\nplot_return(np.transpose(rewards))"
  },
  {
    "objectID": "Documents_files/ST455/Week1/Seminar1/HW1-Solution.html",
    "href": "Documents_files/ST455/Week1/Seminar1/HW1-Solution.html",
    "title": "HW1",
    "section": "",
    "text": "import qrcode\nqrcode.make()\n\n\n\n\nImplement the Gradient Based Methods (not covered in the lecture, to be discussed in the seminar; see below for the algorithm) in the four arm bernoulli bandit example (Lecture 1, Page 27).\n\nSet the termination time to 1000\nConsider three choices of \\(\\eta\\), corresponding to 0.1, 1 and 10\nPlot the average reward and regret for each choice of \\(\\eta\\)\n\n  \n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib import rc\n\n# you might see an error that the module \"tkinter\" is not installed. If on Mac Os you can install it through the terminal via \"brew install python-tk@3.9\". General help can as always be found on stackoverflow: \"https://stackoverflow.com/questions/25905540/importerror-no-module-named-tkinter\" \n\nnp.random.seed(10)\n\nbandit_probabilities = [0.10, 0.40, 0.10, 0.10]\n\nnumber_of_bandits = len(bandit_probabilities) # = n_actions\n    \naction_space = np.arange(number_of_bandits) # =[0,1,2,3]\n\nnumber_of_trials = 20 \ntimesteps = 1000\n\narms = np.zeros(timesteps, dtype=int)\n\ndef step(action):\n    rand = np.random.random()  # [0.0,1.0)\n    reward = 1.0 if (rand &lt; bandit_probabilities[action]) else 0.0\n    return reward\n\ndef compute_regret(bandit_probabilities, arms, time_steps):\n    probs = [bandit_probabilities[arm] for arm in arms]\n    return np.cumsum(np.ones(time_steps)) * np.amax(bandit_probabilities) - np.cumsum(probs)\n\n\ndef cumulative_average_mean(r, n):\n    return np.cumsum(r) / np.cumsum(np.ones(n))\n\n\ndef plot_rewards(reward_list, n):\n    for r in reward_list:\n        plt.plot(cumulative_average_mean(r[0], n), linewidth=3)\n    plt.xlabel('Steps')\n    plt.ylabel('Average reward')\n    legend_str = [r[1] for r in reward_list]\n    plt.legend(legend_str)\n    plt.show()\n\ndef plot_regrets(regrets):\n    for a in regrets:\n        plt.plot(a[0], linewidth=3)\n    plt.xlabel('Steps')\n    plt.ylim(0.0,50.0)\n    plt.ylabel('Regret')\n    legend_str = [r[1] for r in regrets]\n    plt.legend(legend_str)\n    plt.show()\n\n\ngradient_method_constants = [0.1, 1.0, 10]\n\ndef gradient_based_policy(actions, probabilities):\n    return np.random.choice(actions, p=probabilities)\n   \ndef apply_gradient_based_method(n_bandits, action_space, n_trials, timesteps,  arms, eta_parameters):\n    rewards = np.zeros((len(eta_parameters), n_trials, timesteps), dtype=float)\n    regrets = np.zeros((len(eta_parameters), n_trials, timesteps), dtype=float)\n\n    for eta_parameter_counter, eta_parameter in enumerate(eta_parameters):\n        for trial in range(n_trials):\n            n = np.zeros(n_bandits, dtype=int)\n            q = np.zeros(n_bandits, dtype=float)\n            preference_scores = np.zeros(n_bandits, dtype=float)\n            trial_rewards = rewards[eta_parameter_counter, trial, :]\n\n            for t in range(timesteps):\n                probabilities = np.asarray([np.exp(preference_score) for preference_score in preference_scores])\n                probabilities /= np.sum(probabilities)\n                action = gradient_based_policy(action_space, probabilities)\n\n                r = step(action)\n\n                # updating action counter and expected reward Q\n                n[action] += 1\n                q[action] = q[action] + 1.0 / (n[action] + 1) * (r - q[action])\n                \n                average_trial_reward = np.sum(trial_rewards)/(t+1)\n                action_indicator = np.zeros(n_bandits, dtype=float)\n                action_indicator[action] = 1.0\n                preference_scores += eta_parameter * (r - average_trial_reward) * (action_indicator - probabilities)\n                trial_rewards[t] += r\n                arms[t] = action\n\n            regret = compute_regret(bandit_probabilities, arms, timesteps)\n            regrets[eta_parameter_counter, trial, :] += regret\n\n    rewards = np.mean(rewards, axis=1)\n    regrets = np.mean(regrets, axis=1)\n    \n    rewards = list(rewards) \n    regrets = list(regrets)\n    \n    legend_entries = [\"gradient $\\eta=\" + str(eta_parameter) + \"$\" for eta_parameter in eta_parameters] \n\n    rewards = list(zip(rewards, legend_entries))\n    regrets = list(zip(regrets, legend_entries))\n\n    return rewards,regrets\n\n\nrewards_gradient, regrets_gradient = apply_gradient_based_method(number_of_bandits, action_space, number_of_trials, timesteps, arms, gradient_method_constants)\nplot_rewards(rewards_gradient, timesteps)\nplot_regrets(regrets_gradient)"
  },
  {
    "objectID": "Documents_files/ST455/Week1/Seminar1/Seminar1-Part2.html",
    "href": "Documents_files/ST455/Week1/Seminar1/Seminar1-Part2.html",
    "title": "Part 2: Multi-Armed bandits",
    "section": "",
    "text": "As discussed in Lecture 1, the \\(k\\)-armed bandit problem is a classic learning problem that well demonstrates the dilemma between exploration and exploitation when making decisions.\nIn this seminar, our goal is to implement and evaluate the performance of the \\(\\varepsilon\\)-greedy algorithm, the UCB algorithm and the Thompson sampling algorithm for the four Bernoulli arms example covered in the lecture.\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib import rc\n\n# you might see an error that the module \"tkinter\" is not installed. If on Mac Os you can install it through the terminal via \"brew install python-tk@3.9\". General help can as always be found on stackoverflow: \"https://stackoverflow.com/questions/25905540/importerror-no-module-named-tkinter\" \n\nnp.random.seed(10)\n\nbandit_probabilities = [0.10, 0.70, 0.50, 0.30]\n\nnumber_of_bandits = len(bandit_probabilities) # = n_actions\n    \naction_space = np.arange(number_of_bandits) # =[0,1,2,3]\n\nnumber_of_trials = 20 \ntimesteps = 10000\n\narms = np.zeros(timesteps, dtype=int)\n\ndef step(action):\n    rand = np.random.random()  # [0.0,1.0)\n    reward = 1.0 if (rand &lt; bandit_probabilities[action]) else 0.0\n    return reward\n\ndef compute_regret(bandit_probabilities, arms, time_steps):\n    probs = [bandit_probabilities[arm] for arm in arms]\n    return np.cumsum(np.ones(time_steps)) * np.amax(bandit_probabilities) - np.cumsum(probs)\n\ndef cumulative_average_mean(r, n):\n    return np.cumsum(r) / np.cumsum(np.ones(n))\n\n\ndef plot_rewards(reward_list, n):\n    for r in reward_list:\n        plt.plot(cumulative_average_mean(r[0], n), linewidth=3)\n    plt.xlabel('Steps')\n    plt.ylabel('Average reward')\n    legend_str = [r[1] for r in reward_list]\n    plt.legend(legend_str)\n    plt.show()\n\ndef plot_regrets(regrets):\n    for a in regrets:\n        plt.plot(a[0], linewidth=3)\n    plt.xlabel('Steps')\n    plt.ylim(0.0,50.0)\n    plt.ylabel('Regret')\n    legend_str = [r[1] for r in regrets]\n    plt.legend(legend_str)\n    plt.show()"
  },
  {
    "objectID": "Documents_files/ST455/Week1/Seminar1/Seminar1-Part2.html#joint-plots",
    "href": "Documents_files/ST455/Week1/Seminar1/Seminar1-Part2.html#joint-plots",
    "title": "Part 2: Multi-Armed bandits",
    "section": "Joint plots",
    "text": "Joint plots\n\nepsilons = [0.1]\nrewards_epsilon_greedy, regrets_epsilon_greedy = apply_epsilon_greedy(number_of_bandits, \n                                                                      action_space, \n                                                                      number_of_trials, \n                                                                      timesteps, \n                                                                      epsilons, \n                                                                      arms)\n\nucb_constants = [0.1]\nrewards_ucb, regrets_ucb = apply_upper_confidence_bound_policy(number_of_bandits, action_space, number_of_trials, \n                                                               timesteps, arms, ucb_constants)\n\nalpha_beta_priors = [[1.0, 10.0]]\nrewards_thompson, regrets_thompson = apply_thompson_sampling_policy(number_of_bandits, \n                                                                    action_space, \n                                                                    number_of_trials, \n                                                                    timesteps, \n                                                                    arms, \n                                                                    alpha_beta_priors)\n\nreward_list = rewards_epsilon_greedy + rewards_ucb + rewards_thompson\nplot_rewards(reward_list, timesteps)\n\nregret_list = regrets_epsilon_greedy + regrets_ucb + regrets_thompson\nplot_regrets(regret_list)\n\n\n\n\n\n\n\n\nBonus: Gradient Based Methods.\nNot convered in the lecture, but another popular method is to apply gradient based policies.\n\n\n\ngradient_based"
  },
  {
    "objectID": "Documents_files/ST455/Week1/Seminar1/Seminar1-Part2.html#hw1-implement-gradient-based-methods",
    "href": "Documents_files/ST455/Week1/Seminar1/Seminar1-Part2.html#hw1-implement-gradient-based-methods",
    "title": "Part 2: Multi-Armed bandits",
    "section": "HW1: Implement Gradient Based Methods",
    "text": "HW1: Implement Gradient Based Methods"
  },
  {
    "objectID": "Documents_files/ST455/Week1/Seminar1/Seminar1-Part1.html",
    "href": "Documents_files/ST455/Week1/Seminar1/Seminar1-Part1.html",
    "title": "Part 1: Introduction to OpenAI Gym",
    "section": "",
    "text": "import sys\nimport gym\nprint(\"Python\", sys.version)\nprint(\"Gym\", gym.__version__)\n\nPython 3.9.7 (default, Sep 16 2021, 08:50:36) \n[Clang 10.0.0 ]\nGym 0.26.2\nThe gym library developed by OpenAI is a collection of test problems — environments — that you can use to test your reinforcement learning algorithms."
  },
  {
    "objectID": "Documents_files/ST455/Week1/Seminar1/Seminar1-Part1.html#set-up-openai-gym",
    "href": "Documents_files/ST455/Week1/Seminar1/Seminar1-Part1.html#set-up-openai-gym",
    "title": "Part 1: Introduction to OpenAI Gym",
    "section": "Set up OpenAI Gym",
    "text": "Set up OpenAI Gym\n\nInstallation\nTo get started, simply type pip install gym in the terminal (Anaconda prompt in Windows):\npip install gym\nWarning: To install the gym library, you will need to have Python3.5 or above. Then install the libraries required:\npip install pyglet\npip install pyopengl\npip install pyvirtualdisplay"
  },
  {
    "objectID": "Documents_files/ST455/Week1/Seminar1/Seminar1-Part1.html#available-environments",
    "href": "Documents_files/ST455/Week1/Seminar1/Seminar1-Part1.html#available-environments",
    "title": "Part 1: Introduction to OpenAI Gym",
    "section": "Available environments",
    "text": "Available environments\nEnvironments available in the gym include classic control, Atari and 2D and 3D robots tasks, from easy to difficult level.\n\nClassic control\nClassic control problems from RL literature.\n\nCartPole\nA pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The actions one could take are pushing the cart to the left or right. The goal is to prevent the pendulum from falling over. A reward of +1 is given for every step that the pendulum remains upright.\n\nfrom IPython.display import Video\n\nVideo('./graphs/Cart-Pole.mp4')\n\n\n      Your browser does not support the video element.\n    \n\n\n\n\nMountain Car\nA car is stuck in a valley. The goal is to drive up the mountain and reach where the flat is. However, the car’s engine is not strong enough climb to the top in a single run. The way to succeed is to drive back and forth to build up momentum. The actions in the problem is also driving the car to the left or right. A reward of -1 is given for every step until reaching the goal.\n\nVideo('./graphs/mountain_car.mp4')\n\n\n      Your browser does not support the video element.\n    \n\n\n\n\nAcrobot\nThe acrobot system includes two joints and two links, where the joint between the two links is actuated. Initially, the links are hanging downwards, and the goal is to swing the end of the lower link up to a given height.\n\nVideo('./graphs/acrobot.mp4')\n\n\n      Your browser does not support the video element.\n    \n\n\n\n\n\nAtari\nTeach the agent to learn to play Atari games\n\nMs Pac-Man\n\nVideo('./graphs/atari1.mp4')\n\n\n      Your browser does not support the video element.\n    \n\n\n\n\nBoxing\n\nVideo('./graphs/boxing.mp4', width=250)\n\n\n      Your browser does not support the video element.\n    \n\n\n\n\n\nMuJoCo\nContinuous control tasks, running in a fast physics simulator.\n\nAnt v2\nMake a four-legged creature walk as fast as possible.\n\nVideo('./graphs/ant_v2.mp4', width=450)\n\n\n      Your browser does not support the video element.\n    \n\n\n\n\nHumanoid-v2\nMake a three-dimensional bipedal robot walk forward as fast as possible. This link gives you an idea about the goal we try to achieve.\n\nVideo('./graphs/Learning_to_walk.mp4', width=450)\n\n\n      Your browser does not support the video element.\n    \n\n\n\n\n\nStable Retro\nA fork of gym retro – newly released platform for RL research on games.\n\nVideo('./graphs/ferrari.mp4')\n\n\n      Your browser does not support the video element.\n    \n\n\n\nVideo('./graphs/gradius3.mp4')\n\n\n      Your browser does not support the video element.\n    \n\n\n\nVideo('./graphs/multi_games.mp4')\n\n\n      Your browser does not support the video element.\n    \n\n\nTo check the list of environments available in your installation,\n\nfrom gym import envs \nprint(envs.registry.values())\n\ndict_values([EnvSpec(id='CartPole-v0', entry_point='gym.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=195.0, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPole', version=0), EnvSpec(id='CartPole-v1', entry_point='gym.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=475.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPole', version=1), EnvSpec(id='MountainCar-v0', entry_point='gym.envs.classic_control.mountain_car:MountainCarEnv', reward_threshold=-110.0, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='MountainCar', version=0), EnvSpec(id='MountainCarContinuous-v0', entry_point='gym.envs.classic_control.continuous_mountain_car:Continuous_MountainCarEnv', reward_threshold=90.0, nondeterministic=False, max_episode_steps=999, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='MountainCarContinuous', version=0), EnvSpec(id='Pendulum-v1', entry_point='gym.envs.classic_control.pendulum:PendulumEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pendulum', version=1), EnvSpec(id='Acrobot-v1', entry_point='gym.envs.classic_control.acrobot:AcrobotEnv', reward_threshold=-100.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Acrobot', version=1), EnvSpec(id='LunarLander-v2', entry_point='gym.envs.box2d.lunar_lander:LunarLander', reward_threshold=200, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='LunarLander', version=2), EnvSpec(id='LunarLanderContinuous-v2', entry_point='gym.envs.box2d.lunar_lander:LunarLander', reward_threshold=200, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'continuous': True}, namespace=None, name='LunarLanderContinuous', version=2), EnvSpec(id='BipedalWalker-v3', entry_point='gym.envs.box2d.bipedal_walker:BipedalWalker', reward_threshold=300, nondeterministic=False, max_episode_steps=1600, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='BipedalWalker', version=3), EnvSpec(id='BipedalWalkerHardcore-v3', entry_point='gym.envs.box2d.bipedal_walker:BipedalWalker', reward_threshold=300, nondeterministic=False, max_episode_steps=2000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'hardcore': True}, namespace=None, name='BipedalWalkerHardcore', version=3), EnvSpec(id='CarRacing-v2', entry_point='gym.envs.box2d.car_racing:CarRacing', reward_threshold=900, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CarRacing', version=2), EnvSpec(id='Blackjack-v1', entry_point='gym.envs.toy_text.blackjack:BlackjackEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'sab': True, 'natural': False}, namespace=None, name='Blackjack', version=1), EnvSpec(id='FrozenLake-v1', entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv', reward_threshold=0.7, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'map_name': '4x4'}, namespace=None, name='FrozenLake', version=1), EnvSpec(id='FrozenLake8x8-v1', entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv', reward_threshold=0.85, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'map_name': '8x8'}, namespace=None, name='FrozenLake8x8', version=1), EnvSpec(id='CliffWalking-v0', entry_point='gym.envs.toy_text.cliffwalking:CliffWalkingEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CliffWalking', version=0), EnvSpec(id='Taxi-v3', entry_point='gym.envs.toy_text.taxi:TaxiEnv', reward_threshold=8, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Taxi', version=3), EnvSpec(id='Reacher-v2', entry_point='gym.envs.mujoco:ReacherEnv', reward_threshold=-3.75, nondeterministic=False, max_episode_steps=50, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Reacher', version=2), EnvSpec(id='Reacher-v4', entry_point='gym.envs.mujoco.reacher_v4:ReacherEnv', reward_threshold=-3.75, nondeterministic=False, max_episode_steps=50, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Reacher', version=4), EnvSpec(id='Pusher-v2', entry_point='gym.envs.mujoco:PusherEnv', reward_threshold=0.0, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pusher', version=2), EnvSpec(id='Pusher-v4', entry_point='gym.envs.mujoco.pusher_v4:PusherEnv', reward_threshold=0.0, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pusher', version=4), EnvSpec(id='InvertedPendulum-v2', entry_point='gym.envs.mujoco:InvertedPendulumEnv', reward_threshold=950.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedPendulum', version=2), EnvSpec(id='InvertedPendulum-v4', entry_point='gym.envs.mujoco.inverted_pendulum_v4:InvertedPendulumEnv', reward_threshold=950.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedPendulum', version=4), EnvSpec(id='InvertedDoublePendulum-v2', entry_point='gym.envs.mujoco:InvertedDoublePendulumEnv', reward_threshold=9100.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedDoublePendulum', version=2), EnvSpec(id='InvertedDoublePendulum-v4', entry_point='gym.envs.mujoco.inverted_double_pendulum_v4:InvertedDoublePendulumEnv', reward_threshold=9100.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedDoublePendulum', version=4), EnvSpec(id='HalfCheetah-v2', entry_point='gym.envs.mujoco:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=2), EnvSpec(id='HalfCheetah-v3', entry_point='gym.envs.mujoco.half_cheetah_v3:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=3), EnvSpec(id='HalfCheetah-v4', entry_point='gym.envs.mujoco.half_cheetah_v4:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=4), EnvSpec(id='Hopper-v2', entry_point='gym.envs.mujoco:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=2), EnvSpec(id='Hopper-v3', entry_point='gym.envs.mujoco.hopper_v3:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=3), EnvSpec(id='Hopper-v4', entry_point='gym.envs.mujoco.hopper_v4:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=4), EnvSpec(id='Swimmer-v2', entry_point='gym.envs.mujoco:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=2), EnvSpec(id='Swimmer-v3', entry_point='gym.envs.mujoco.swimmer_v3:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=3), EnvSpec(id='Swimmer-v4', entry_point='gym.envs.mujoco.swimmer_v4:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=4), EnvSpec(id='Walker2d-v2', entry_point='gym.envs.mujoco:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=2), EnvSpec(id='Walker2d-v3', entry_point='gym.envs.mujoco.walker2d_v3:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=3), EnvSpec(id='Walker2d-v4', entry_point='gym.envs.mujoco.walker2d_v4:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=4), EnvSpec(id='Ant-v2', entry_point='gym.envs.mujoco:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=2), EnvSpec(id='Ant-v3', entry_point='gym.envs.mujoco.ant_v3:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=3), EnvSpec(id='Ant-v4', entry_point='gym.envs.mujoco.ant_v4:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=4), EnvSpec(id='Humanoid-v2', entry_point='gym.envs.mujoco:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=2), EnvSpec(id='Humanoid-v3', entry_point='gym.envs.mujoco.humanoid_v3:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=3), EnvSpec(id='Humanoid-v4', entry_point='gym.envs.mujoco.humanoid_v4:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=4), EnvSpec(id='HumanoidStandup-v2', entry_point='gym.envs.mujoco:HumanoidStandupEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HumanoidStandup', version=2), EnvSpec(id='HumanoidStandup-v4', entry_point='gym.envs.mujoco.humanoidstandup_v4:HumanoidStandupEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HumanoidStandup', version=4)])\n\n\nNotice that\npip install gym\ndoes not install all gym dependencies. You can install these dependencies for one family like\npip install gym\\[atari\\]\npip install gym\\[mojuco\\]"
  },
  {
    "objectID": "Documents_files/ST455/Week1/Seminar1/Seminar1-Part1.html#cart-pole-balancing",
    "href": "Documents_files/ST455/Week1/Seminar1/Seminar1-Part1.html#cart-pole-balancing",
    "title": "Part 1: Introduction to OpenAI Gym",
    "section": "Cart-pole balancing",
    "text": "Cart-pole balancing\n\nTask\nA pole is attached by an un-actuated joint to a cart which moves along a frictionless track. The task is to apply forces to the cart along the track, and the goal is to prevent it from falling over by increasing and reducing the cart’s velocity.\n\n\nAction\nTwo discrete actions: a=0 indicates pushing cart to the left and a=1 pushes the cart to the right. The amount the velocity reduced or increased does not only depends on the direction you are moving but also on the angle the pole is pointing.\n\n\nReward\nReward is +1 for every step taken, including the termination step.\n\n\nObservation\nThere are 4 observations returned by the environment after each action taken by an agent: - Cart position: a number between -4.8 and 4.8 - Cart velocity: a number between -infand inf - Pole angle: an angle between -24° and 24° - Pole velocity at tip: a number between -infand inf\n\n\nTermination\n\nCart position is smaller or greater than -2.4 or 2.4\nPole Angle is smaller or greater than -12° or 12°\nEpisode length is longer than 200\n\n\n\nSolved requirements\nThe problem is considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials."
  },
  {
    "objectID": "Documents_files/ST455/Week1/Seminar1/Seminar1-Part1.html#basics-in-gym",
    "href": "Documents_files/ST455/Week1/Seminar1/Seminar1-Part1.html#basics-in-gym",
    "title": "Part 1: Introduction to OpenAI Gym",
    "section": "Basics in Gym",
    "text": "Basics in Gym\n\nImport an environment\nTo start with, let’s choose the cart-pole environment. The following command returns the environment passed as argument.\ngym.make('environment_id')\nEvery environment comes with an action_space and an observation_space. For example,\n\nimport gym\n\nenv = gym.make('CartPole-v1')\nprint(env.action_space)\nprint(env.observation_space)\n\nDiscrete(2)\nBox([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n\n\nAction space shows the number of actions one can take and observation space. In this case, the actions include left and right. The third output in Box indicates that you have 4 state variables: cart position, cart velocity, pole angle and pole position. The first two outputs gives the lower and upper bounds of the 4 states. The last output shows the data type of the states. When angle value lies outside (-0.209, 0.209) (computed by ), you will fail the episode.\n\n\nStart the process\nStart the process by calling env.reset(). This function creates a new environment and returns an initial observation.\nenv.render() displays a popup window that renders an update for each step in the loop.\n\n\nThe gym Step function\nThe environment’s step function takes an action at each time step. The action is passed as an argument to the function. step function returns four values, namely observation, reward, done and info.\nobservation: an environment-specific object representing your observation of the environment.\nreward: amount of reward achieved by the previous action.\ndone: a boolean value. When it’s true, it indicates the episode has terminated and the environment needs to be reset.\ninfo: diagnostic information useful for debugging.\n\n\nTaking a random action\nenv.action_space.sample() takes a random action."
  },
  {
    "objectID": "Documents_files/ST455/Week1/Seminar1/Seminar1-Part1.html#get-something-running",
    "href": "Documents_files/ST455/Week1/Seminar1/Seminar1-Part1.html#get-something-running",
    "title": "Part 1: Introduction to OpenAI Gym",
    "section": "Get something running",
    "text": "Get something running\nLet’s try to run the CartPole-v1 environment for 100 timesteps taking random actions. We will need to import pyglet. Try pip install pyglet\n\nimport gym  \n  \nenv = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")  \nenv.reset()  \n  \nfor _ in range(10):  \n    env.render()  \n    print(env.step(env.action_space.sample()))\n    \nenv.close()\n\n(array([-0.03129847,  0.14590824, -0.01053288, -0.335416  ], dtype=float32), 1.0, False, False, {})\n(array([-0.0283803 , -0.04906224, -0.0172412 , -0.0460731 ], dtype=float32), 1.0, False, False, {})\n(array([-0.02936155, -0.24393278, -0.01816266,  0.24112059], dtype=float32), 1.0, False, False, {})\n(array([-0.0342402 , -0.43879065, -0.01334025,  0.52801967], dtype=float32), 1.0, False, False, {})\n(array([-0.04301602, -0.63372236, -0.00277986,  0.8164693 ], dtype=float32), 1.0, False, False, {})\n(array([-0.05569047, -0.82880616,  0.01354953,  1.1082766 ], dtype=float32), 1.0, False, False, {})\n(array([-0.07226659, -1.0241035 ,  0.03571506,  1.4051793 ], dtype=float32), 1.0, False, False, {})\n(array([-0.09274866, -1.2196503 ,  0.06381864,  1.7088102 ], dtype=float32), 1.0, False, False, {})\n(array([-0.11714166, -1.0253172 ,  0.09799485,  1.436654  ], dtype=float32), 1.0, False, False, {})\n(array([-0.13764802, -1.2215011 ,  0.12672792,  1.7582825 ], dtype=float32), 1.0, False, False, {})\n\n\nYou may see a window pop up rendering the Cart-pole problem.\nAlternatively, we can record the frames.\n\nimport matplotlib.pyplot as plt\nimport matplotlib.animation\nimport matplotlib\nfrom IPython.display import HTML\n\nenv = gym.make('CartPole-v1', render_mode = 'rgb_array')\nenv.reset()\n\nframes = []\nfor i in range(100):\n    frames.append(env.render())\n    env.step(env.action_space.sample())\n\nplt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\npatch = plt.imshow(frames[0])\nplt.axis('off')\nanimate = lambda i: patch.set_data(frames[i])\nani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\nHTML(ani.to_jshtml())\n\nc:\\Users\\phlam\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:177: UserWarning: WARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\n  logger.warn(\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "Documents_files/ST455/Week1/Seminar1/Seminar1-Part1.html#run-episodes",
    "href": "Documents_files/ST455/Week1/Seminar1/Seminar1-Part1.html#run-episodes",
    "title": "Part 1: Introduction to OpenAI Gym",
    "section": "Run episodes",
    "text": "Run episodes\nRun 10 episodes with random actions,\n\nepisodes = 10\nfor i in range(episodes):\n    env.reset()\n    done = False\n    rewards = 0\n    while not done and rewards &lt; 200:\n        frames.append(env.render())\n        obs, r, done, info, _ = env.step(env.action_space.sample())\n        rewards += r\n    print('Episode:', i, 'Rewards:', rewards)\n    print('Observations:', obs)\n\nEpisode: 0 Rewards: 41.0\nObservations: [ 0.47135496  2.0987859  -0.22315665 -2.2745278 ]\nEpisode: 1 Rewards: 14.0\nObservations: [ 0.14764233  1.5837066  -0.226416   -2.4973404 ]\nEpisode: 2 Rewards: 10.0\nObservations: [ 0.14101417  1.5659921  -0.22949736 -2.5554643 ]\nEpisode: 3 Rewards: 26.0\nObservations: [ 0.18779577  1.1871033  -0.22108863 -1.8402065 ]\nEpisode: 4 Rewards: 13.0\nObservations: [ 0.12957971  1.0211047  -0.22130933 -1.77571   ]\nEpisode: 5 Rewards: 23.0\nObservations: [-0.19671164 -1.4145039   0.24613997  2.3371205 ]\nEpisode: 6 Rewards: 45.0\nObservations: [-0.00488641 -0.5700494   0.21394622  1.5247782 ]\nEpisode: 7 Rewards: 28.0\nObservations: [-0.28604752 -0.74622136  0.22440116  0.967424  ]\nEpisode: 8 Rewards: 10.0\nObservations: [-0.16276747 -0.81677544  0.23219758  1.4328974 ]\nEpisode: 9 Rewards: 12.0\nObservations: [ 0.12486421  0.7429131  -0.2247896  -1.4634321 ]\n\n\nThe third variable in the observations corresponds to angles which seemed to be the reason why the episodes failed. Therefore, we design a naive improvement on the previous algorithm. Instead of choosing random actions. we take actions based on the pole angles, i.e., when the angle is negative, we push the cart to the left, otherwise to the right.\n\ndef action(angle):\n    if angle &lt; 0:\n        return 0\n    else:\n        return 1\n\nAnd modify the execution part:\n\nimport gym\nenv = gym.make('CartPole-v1', render_mode=\"rgb_array\")\nenv.reset()\nepisodes = 10\nfor i in range(episodes):\n    obs = env.reset()\n    obs = obs[0]\n    done = False\n    rewards = 0.0\n    while not done and rewards &lt; 500:\n        env.render()\n        a = action(obs[2])\n        obs, r, done, info, _ = env.step(a)\n        rewards += r\n    print('Episode:', i, 'Rewards:', rewards)\n    print('Observations:', obs)\n\nEpisode: 0 Rewards: 35.0\nObservations: [-0.11436404 -0.15936136  0.21196793  0.36794078]\nEpisode: 1 Rewards: 45.0\nObservations: [-0.14433978 -0.5636829   0.2129816   0.75778276]\nEpisode: 2 Rewards: 27.0\nObservations: [ 0.11824524 -0.18272817 -0.21033023  0.14561911]\nEpisode: 3 Rewards: 52.0\nObservations: [-0.18496755 -1.1168673   0.22844273  1.3003998 ]\nEpisode: 4 Rewards: 41.0\nObservations: [-0.17361063 -0.95284     0.21318184  1.1478326 ]\nEpisode: 5 Rewards: 38.0\nObservations: [-0.2489437  -0.00727316  0.21561201  0.12524061]\nEpisode: 6 Rewards: 50.0\nObservations: [ 0.20572841  0.43325907 -0.21713859 -0.4835294 ]\nEpisode: 7 Rewards: 31.0\nObservations: [-0.17238179 -0.21859929  0.21089686  0.3343301 ]\nEpisode: 8 Rewards: 35.0\nObservations: [ 0.21897517  0.23384616 -0.21375969 -0.37648988]\nEpisode: 9 Rewards: 43.0\nObservations: [-0.12067515 -0.1484282   0.21793127  0.27770728]"
  },
  {
    "objectID": "Documents_files/ST455/Week1/Seminar1/HW1.html",
    "href": "Documents_files/ST455/Week1/Seminar1/HW1.html",
    "title": "HW1",
    "section": "",
    "text": "Implement the Gradient Based Methods (not covered in the lecture, to be discussed in the seminar; see below for the algorithm) in the four arm bernoulli bandit example (Lecture 1, Page 27).\n\nSet the termination time to 1000\nNumber of trials = 20\nConsider three choices of \\(\\eta\\), corresponding to 0.1, 1 and 10\nPlot the average reward and regret for each choice of \\(\\eta\\)\n\n\n\n\ngradient_based\n\n\n\n## your code here"
  },
  {
    "objectID": "Documents_files/ST455/Week7/Seminar7/Seminar7.html",
    "href": "Documents_files/ST455/Week7/Seminar7/Seminar7.html",
    "title": "Seminar 7: TD Learning (Case Study)",
    "section": "",
    "text": "In this seminar, we will design a toy example to simulate the ridesharing market and implement the MDP order dispatch policy in this example. We will discuss DQN in the next seminar class. Although the algorithm is being motivated by applications in ridesharing platforms, similar ideas can be employed in other two sided marketplaces (e.g., food delivery companies) that involve sequential decision making over time and space as well.\nFirst, let us import all the required modules. Here, the linear_sum_assignment function from scipy.optimize allows us to solve matching problem to assign drivers to call orders by maximizing the advantage function or minimizing the total distance. It is designed to solve the general linear sum assignment problem, e.g., \\[\\begin{eqnarray*}\n    \\sum_{i=1}^m \\sum_{j=1}^n C_{i,j} a_{i,j},\n\\end{eqnarray*}\\] where \\(C_{i,j}\\) is the cost of matching vertex \\(i\\) of the first set (e.g., drivers) to vertex \\(j\\) of the second set (e.g., orders) and \\(a_{i,j}\\) equals 1 if \\(i\\) and \\(j\\) are matched and 0 otherwise. In addition, each row is assignment to at most one column, and each column to at most one row. If it has more rows than columns, then not every row needs to be assigned to a column, and vice versa.\n\nfrom itertools import count\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\nfrom scipy.optimize import linear_sum_assignment \nfrom scipy.sparse.csgraph import min_weight_full_bipartite_matching\n\nWe next detail the experimental setup. Consider orders and drivers operating in a simple map of 9 × 9 spatial grids with 20 time steps. Meanwhile, orders can only be dispatched to drivers in Manhattan distance that are no greater than 2. An order will be canceled if not being assigned to any driver for a long time, with the cancelation time modeled as a truncated Gaussian in the range 0 to 5 with mean 2.5 and standard deviation 2 along the temporal axis.\nThe Manhattan distance is different from the Euclidean distance. See the following figure for details. Take from here\n\n\nNUMBER_OF_GRID_TILES_X = 9\nNUMBER_OF_GRID_TILES_Y= 9\nNUMBER_OF_TIME_STEPS = 20 \n\nMAX_MANHATTAN_DISTANCE = 2\n\ndef manhattan_distance(p1,p2):\n    return np.abs(p1[0]-p2[0]) + np.abs(p1[1]-p2[1])\n\nWe fix the number of orders to 100. The following code allows us to generate waiting time for all the orders.\n\nNUMBER_OF_ORDERS = 100\n\nLOWER_BOUND_WAITING_TIME = 0\nUPPER_BOUND_WAITING_TIME = 5\nMEAN_WAITING_TIME = 2.5\nSTANDARD_DEVIATION_WAITING_TIME = 2\n\nwaiting_time_sampler = stats.truncnorm(\n    (LOWER_BOUND_WAITING_TIME - MEAN_WAITING_TIME) / STANDARD_DEVIATION_WAITING_TIME, (UPPER_BOUND_WAITING_TIME - MEAN_WAITING_TIME) / STANDARD_DEVIATION_WAITING_TIME, loc=MEAN_WAITING_TIME, scale=STANDARD_DEVIATION_WAITING_TIME)\n\nwaiting_times = waiting_time_sampler.rvs(NUMBER_OF_ORDERS)\n\nFor data generation, we want to simulate realistic traffic patterns with a morning-peak and a night-peak, centralized on different locations of residential areas and working areas, respectively. Therefore, orders’ starting locations are sampled according to a two-component mixture of Gaussians and then truncated to integers in the spatiotemporal grids. Afterwards, orders’ destinations and drivers’ initial locations are randomly sampled from a discrete uniform distribution defined on the grids. Parameters of the mixture of Gaussians are as follows.\n\\[\\begin{eqnarray*}\n    \\pi^{(1)}=1/3, &\\,\\,& \\pi^{(2)}=2/3;\\\\\n    \\mu^{(1)}=[3,3,5], &\\,\\,& \\mu^{(2)}=[6,6,15];\\\\\n    \\sigma^{(1)}=[2,2,3], &\\,\\,& \\sigma^{(2)}=[2,2,3].\n\\end{eqnarray*}\\]\nThe following code allows us to generate random samples from the two-component mixture distribution.\n\nPROBABILITY_FIRST_GAUSSIAN = 1./3\nPROBABILITY_SECOND_GAUSSIAN = 2./3\n\nMEAN_FIRST_GAUSSIAN = [3,3,5]\nMEAN_SECOND_GAUSSIAN = [6,6,15]\n\nSTANDARD_DEVIATION_FIRST_GAUSSIAN = [2,2,3]\nSTANDARD_DEVIATION_SECOND_GAUSSIAN = [2,2,3]\n\nLOWER_LIMITS_BY_DIMENSION = [0 for _ in range(3)]\nUPPER_LIMITS_BY_DIMENSION = [NUMBER_OF_GRID_TILES_X - 1, NUMBER_OF_GRID_TILES_Y - 1, NUMBER_OF_TIME_STEPS - 1]\n\nclass TruncatedMultivariateNormalInteger():\n    def __init__(self, normals):\n        self._normals = []\n        for [lower, upper, mean, standard_deviation] in normals:\n            X = stats.truncnorm(\n    (lower - mean) / standard_deviation, (upper - mean) / standard_deviation, loc=mean, scale=standard_deviation)\n            self._normals.append(X)\n    # size equals 3 (e.g., 3 independent truncated normals per mixture component) in our example\n    def rvs(self, size):\n        return np.array([[normal.rvs(size=1) for normal in self._normals] for _ in range(size)])\n\nclass MixtureModel():\n    def __init__(self, submodels, weights, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.submodels = submodels\n        self.weights = weights/np.sum(weights) \n\n    def rvs(self, size):\n        rvs = [] \n        for i in range(size):\n            random_model = np.random.choice(range(len(self.submodels)), p=self.weights)\n            rvs.append(self.submodels[random_model].rvs(size=1))\n        return np.round(np.squeeze(np.array(rvs))).astype(int)\n    \nfirst_truncated_multivariate_normal = TruncatedMultivariateNormalInteger([[LOWER_LIMITS_BY_DIMENSION[i], UPPER_LIMITS_BY_DIMENSION[i], MEAN_FIRST_GAUSSIAN[i], STANDARD_DEVIATION_FIRST_GAUSSIAN[i]] for i in range(3)])\n\nsecond_truncated_multivariate_normal = TruncatedMultivariateNormalInteger([[LOWER_LIMITS_BY_DIMENSION[i], UPPER_LIMITS_BY_DIMENSION[i], MEAN_SECOND_GAUSSIAN[i], STANDARD_DEVIATION_SECOND_GAUSSIAN[i]] for i in range(3)])\n\nmixture_gaussian_model = MixtureModel([first_truncated_multivariate_normal, second_truncated_multivariate_normal],[1./3,2./3])\n\nThe following code allows us to randomly generate the orders’ destinations and drivers’ initial locations from a discrete uniform distribution on the grids\n\ndef spawn_uniformly_x_y_location():\n    return [np.random.choice(range(NUMBER_OF_GRID_TILES_X)), np.random.choice(range(NUMBER_OF_GRID_TILES_Y))]\n\nWe next calculate the reward associated with each trip. In this example, we use the completion rate (e.g., the percentage of orders being completed) as our objective. As such, for each order, the immediate reward is a binary variable, depending on whether it is being completed or not.\nWe adopt a discounted setting. The discounted factor is fixed to 0.9. Recall from the lecture that the discounted reward is calculated in the following manner.\n\nThe following function calculates the discounted reward based on the observed reward.\n\nDISCOUNT_FACTOR = 0.9\n\ndef discounted_reward_mdp(gamma, T, R):\n    total_gamma = 0\n    discounted_gamma = 1\n    for _ in range(T):\n        total_gamma += discounted_gamma \n        discounted_gamma *= gamma\n    return total_gamma * R / T\n\nNext, we start to implement the MDP order dispatch policy. Recall from the lecture that the algorithm consists of two steps, corresponding to policy evaluation based on temporal difference learning and order dispatch by maximizing the total advantage function. Consider the policy evaluation step first. Let us review the policy evaluation algorithm.\n \nThe following function implements the policy evaluation algorithm.\n\n# Elements in state_transactions are quadruples state, action, reward, next_state\n# action is a quadruple consisting of [idle = 0 / serve = 1, serving position [x,y], destination posotion [x,y]]\n# V and N are 9*9*20 (3-d) matrices\n# delta_t corresponds to the serving time, calculated based on the Manhanttan distance\ntime_ = 2 # The last component of state\ndef policy_evaluation(state_transactions, V, N, starting_index, method):\n    if V is None:\n        V = np.zeros(np.array(UPPER_LIMITS_BY_DIMENSION) - np.array(LOWER_LIMITS_BY_DIMENSION) + [1,1,1])\n    if N is None:\n        N = np.zeros(np.array(UPPER_LIMITS_BY_DIMENSION) - np.array(LOWER_LIMITS_BY_DIMENSION) + [1,1,1])\n    for t in range(NUMBER_OF_TIME_STEPS, -1, -1):\n        for state, action, reward, next_state in state_transactions[starting_index:]:\n            if state[time_] == t:\n                N[tuple(state)] += 1\n                delta_t = 1\n                if action[0] == 1:\n                    delta_t += manhattan_distance(state[:2], action[1]) + manhattan_distance(action[1], action[2])    \n                future_value = 0\n                if next_state[time_] &lt; NUMBER_OF_TIME_STEPS:\n                    future_value = np.power(DISCOUNT_FACTOR, delta_t) * V[tuple(next_state)]\n                if method == 'mdp':\n                    modified_reward = discounted_reward_mdp(DISCOUNT_FACTOR, delta_t ,reward)\n                elif method == 'myopic':\n                    modified_reward = reward\n                V[tuple(state)] += 1./(N[tuple(state)]) * (future_value + modified_reward - V[tuple(state)])\n    return V, N\n\nratios_of_served_orders = []\n\nFinally, let us review the order dispatch step.\n \nThe definition of the advantage function is given below.\n\nThe following code implements the entire algorithm when being applied to the toy example. In a first phase it creates a historical dataset that lasts for 450 days (450 episodes) by running the ‘distance’ method (e.g., the closest-driver policy). Then, with this historical data the value function \\(V\\) gets computed, based on which a new matching algorithm is devised. Next, we generate another 50-day dataset (50 epsidodes) to evaluate the performance of different policies.\n\nBASE_REWARD_PER_TRIP = 1\nREWARD_FOR_DISTANCE_PARAMETER = 0\n\ndef real_time_order_dispatch_algorithm():\n\n    NUM_EPISODES = 500 \n    BENCHMARK_RUNS = 50 \n    NUM_INDEPENDENT_RUNS = NUM_EPISODES - BENCHMARK_RUNS \n\n    # Consider three settings where the number of drivers equals 25, 50 and 75, respectively. Recall that the total number of orders is given by 100.\n    number_of_drivers_list = [25,50,75]\n    # Consider three methods, distance (closest driver policy), myopic policy (gamma=0) and MDP policy\n    method_list = ['distance', 'myopic', 'mdp']\n    # Consider two measurements, the completion rate and the average distance between orders and drivers\n    measurement_keypoints = ['ratio served', 'average distance to driver']\n\n    stored_mdp_V_functions = [] \n    \n    # Measures the performance for each algorithm\n    benchmark_data = np.zeros((len(number_of_drivers_list), len(method_list), len(measurement_keypoints),BENCHMARK_RUNS))\n\n    for number_of_drivers_ind, number_of_drivers in enumerate(number_of_drivers_list):\n        for method_ind, method in enumerate(method_list):\n            transition_data = []\n            if method in ['mdp', 'myopic']:\n                ## Initialize the value and the state counter\n                V, N = policy_evaluation(transition_data, None, None, 0, method)\n                starting_index = 0\n\n            for episode in range(NUM_EPISODES): \n                \n                order_driver_distances = []\n\n                # Generate 450 episodes as historical data and then 50 episodes to evaluate difference policies\n                if episode &gt;= NUM_INDEPENDENT_RUNS and method in ['mdp', 'myopic']:\n                    V, N = policy_evaluation(transition_data, V, N, starting_index, method)\n                    starting_index = len(transition)\n\n                destinations = []\n                for _ in range(NUMBER_OF_ORDERS):\n                    # destination is drawn uniformly randomly from the grid\n                    destinations.append(spawn_uniformly_x_y_location())\n                \n                # in orders first entry is boolean corresponding to wether it is served.\n                orders = list(map(list, zip([False] * NUMBER_OF_ORDERS, mixture_gaussian_model.rvs(NUMBER_OF_ORDERS), np.round(waiting_times).astype(int), destinations, range(NUMBER_OF_ORDERS))))\n                drivers = []\n                for i in range(number_of_drivers):\n                    # first entry describes the first time the driver is available again\n                    # driver's location is drawn uniformly randomly from the grid\n                    drivers.append([0, spawn_uniformly_x_y_location(), i])\n                    \n                for t in range(NUMBER_OF_TIME_STEPS):\n                    # obtain active orders\n                    active_orders = [order for order in orders if (order[0] == False) and (order[1][2] &lt;= t) and (order[1][2] + order[2] &gt;= t)]\n                    available_drivers = [driver for driver in drivers if driver[0] &lt;= t]\n                    # print(len(active_orders), len(available_drivers)) \n                    # print(drivers)\n\n                    allowed_match = np.ones((len(active_orders), len(available_drivers)), dtype=bool)\n                    for order_count, active_order in enumerate(active_orders):\n                        for driver_count, available_driver in enumerate(available_drivers):\n                            # only consider drivers whose manhattan distance is slower than 2\n                            if manhattan_distance(available_driver[1], active_order[1][:2]) &gt; MAX_MANHATTAN_DISTANCE:\n                                allowed_match[order_count, driver_count] = False\n                    # print(allowed_match)\n\n                    # computation of advantage function\n                    if method in ['mdp', 'myopic']:\n                        #Could also initialize with - infinity.\n                        advantage_function = np.zeros((len(active_orders), len(available_drivers))) \n                        for order_count, active_order in enumerate(active_orders):\n                            for driver_count, available_driver in enumerate(available_drivers):\n                                if(allowed_match[order_count, driver_count]):\n                                    # the pickup time\n                                    delta_t = 1 + manhattan_distance(available_driver[1], active_order[1][:2]) + manhattan_distance(active_order[1][:2], active_order[3])    \n                                    reward = BASE_REWARD_PER_TRIP + REWARD_FOR_DISTANCE_PARAMETER  * manhattan_distance(active_order[1][:2], active_order[3])  \n                                    #If the completion time is later than the last time step, we just stop set the future value to zero\n                                    future_value = 0.\n                                    if t + delta_t &lt; NUMBER_OF_TIME_STEPS: \n                                        discount = DISCOUNT_FACTOR\n                                        if method == 'myopic':\n                                            discount = 0.\n                                        future_value = np.power(discount, delta_t) * V[active_order[3][0],active_order[3][1], t + delta_t]\n                                    current_value = V[available_driver[1][0], available_driver[1][1], t]\n                                    modified_reward = reward\n                                    if method == 'mdp':\n                                        modified_reward = discounted_reward_mdp(DISCOUNT_FACTOR, delta_t, reward)\n                                    advantage_function[order_count, driver_count] = future_value - current_value + modified_reward\n                        \n                        # plot_sample_histogram(np.array([active_order[1] for active_order in active_orders]), \"Active Orders at time {}\".format(t))\n                        # plot_sample_histogram(np.array([available_driver[1] for available_driver in available_drivers]), \"Available Drivers at time {}\".format(t))\n                        \n                        # print(advantage_function)\n                        # print(advantage_function.shape)\n                        \n                        # Matchs orders to drivers. Note the important subtelty that the function linear_sum_assignment returns a full matching. But we are happy with a partial matching already.\n\n                    row_ind = [] \n                    col_ind = []\n\n                    # The initial independent runs should use the 'distance' policy to find the matching.\n                    # Later runs could either use 'mdp', 'myopic' or 'distance' policy\n                    if episode &gt;= NUM_INDEPENDENT_RUNS and method in ['mdp','myopic']:\n                        penalized_advantage_matrix = advantage_function\n                        for i in range(len(active_orders)):\n                            for j in range(len(available_drivers)):\n                                if not allowed_match[i,j]:\n                                    penalized_advantage_matrix[i,j] = - 100 * NUMBER_OF_ORDERS\n                        row_ind, col_ind = linear_sum_assignment(-penalized_advantage_matrix)\n\n                    else:\n                        #Use distance matrix to compute assignment\n                        distance_matrix = -np.ones((len(active_orders), len(available_drivers))) * 100 * NUMBER_OF_ORDERS\n                        for i in range(len(active_orders)):\n                            for j in range(len(available_drivers)):\n                                if allowed_match[i,j]:\n                                    distance_matrix[i,j] = -manhattan_distance(available_drivers[j][1], active_orders[i][1][:2]) \n                        row_ind, col_ind = linear_sum_assignment(-distance_matrix) \n\n                    matched_order_ind = []\n                    matched_driver_ind = []\n\n                    for i in range(len(row_ind)):\n                        if row_ind[i] &lt; len(active_orders) and col_ind[i] &lt; len(available_drivers) and allowed_match[row_ind[i],col_ind[i]]:\n                            matched_order_ind.append(row_ind[i])\n                            matched_driver_ind.append(col_ind[i])\n\n                    # print(f\"Matched orders in iteration {t}\")\n                    for i in range(len(matched_order_ind)):\n                        if allowed_match[matched_order_ind[i]][matched_driver_ind[i]]:\n                            matched_order = active_orders[matched_order_ind[i]]\n                            matched_driver = available_drivers[matched_driver_ind[i]]\n                            # if method == 'mdp':\n                                # print(f'Order {matched_order[-1]} at {matched_order[1][:2]} is matched to driver {matched_driver[-1]} at {matched_driver[1]}')\n                            matched_order[0] = True\n\n                            order_driver_distance = manhattan_distance(matched_driver[1], matched_order[1][:2])\n\n                            # continue to run the code only when the assertion is satisfied. Stop and return an error otherwise\n                            assert(order_driver_distance &lt;= 2)\n\n                            order_driver_distances.append(order_driver_distance)\n\n                            delta_t = 1 + manhattan_distance(matched_driver[1], matched_order[1][:2]) + manhattan_distance(matched_order[1][:2], matched_order[3])    \n                            matched_driver[0] = t + delta_t \n\n                            #Append to transition data.\n                            \n                            reward = BASE_REWARD_PER_TRIP + REWARD_FOR_DISTANCE_PARAMETER  * manhattan_distance(matched_order[1][:2], matched_order[3])  \n                            \n                            transition = [[matched_driver[1][0], matched_driver[1][1], t], [1, matched_order[1][:2], matched_order[3]], reward, [matched_order[3][0], matched_order[3][1], t + delta_t]]\n                            transition_data.append(transition.copy())\n\n                    # Set transition data for unmatched drivers \n                    for i, unmatched_driver in enumerate(available_drivers):\n                        if i not in matched_driver_ind:\n                            transition = [[unmatched_driver[1][0], unmatched_driver[1][1], t],[0],0,[unmatched_driver[1][0], unmatched_driver[1][1], t + 1]]\n                            transition_data.append(transition.copy())\n\n                if episode &gt;= NUM_INDEPENDENT_RUNS: \n                    number_of_served_orders = 0\n                    for i in range(len(orders)):\n                        number_of_served_orders += orders[i][0]\n                        \n                    # calculate the completion rate\n                    ratio_served = float(number_of_served_orders)/ NUMBER_OF_ORDERS\n                    benchmark_data[number_of_drivers_ind, method_ind, :,  episode - NUM_INDEPENDENT_RUNS] = [ratio_served, np.mean(np.array(order_driver_distances))]\n                    if method == 'mdp' and episode == NUM_EPISODES - 1:\n                        # Used for visualising value functions\n                        stored_mdp_V_functions.append(V.copy()) \n\n    print(benchmark_data) \n    plot_benchmarks(benchmark_data, number_of_drivers_list, method_list, measurement_keypoints) \n    plot_value_functions(stored_mdp_V_functions)\n\nHere are the plotting functions:\n\ndef plot_benchmarks(benchmark_data, number_of_drivers_list, method_list, measurement_keypoints):\n    \n    fig, axes = plt.subplots(1, benchmark_data.shape[2]) \n    benchmark_mean = np.mean(benchmark_data, axis = 3)\n    benchmark_std = np.std(benchmark_data, axis = 3)\n\n    barWidth = 0.25\n\n    barColors = ['tab:red', 'tab:green', 'tab:blue']\n\n    for idx, ax in enumerate(axes):\n        data_mean = benchmark_mean[:,:,idx] \n        data_std = benchmark_std[:,:,idx]\n        \n        # The x position of bars\n        x_pos = []\n        r1 = np.arange(len(number_of_drivers_list))\n        for i in range(len(method_list)): \n            x_pos.append([x + i * barWidth for x in r1]) \n\n        for method_idx, method in enumerate(method_list):\n            ax.bar(x_pos[method_idx], data_mean[:,method_idx], width = barWidth, color = barColors[method_idx], yerr=data_std[:,method_idx], capsize=7, label=method)\n        # general layout\n        ax.set_xticks([r + barWidth for r in range(len(number_of_drivers_list))])\n        ax.set_xticklabels(number_of_drivers_list)\n        ax.set_title(measurement_keypoints[idx]) \n\n    handles, labels = ax.get_legend_handles_labels()\n    fig.legend(handles, labels, loc='upper center', ncol=len(method_list)) \n    #Show graphic\n    plt.show() \n    \ndef plot_value_functions(value_functions):\n    time_step = 3\n    \n    fig = plt.figure(figsize=(12, 8))\n\n    for idx in range(len(value_functions)):\n        ax = fig.add_subplot(1,len(value_functions),idx+1, projection='3d')\n        plot_values = value_functions[idx][:,:,time_step]\n\n        # print(\"Value function is \", plot_values)\n\n        X = np.arange(NUMBER_OF_GRID_TILES_X)\n        Y = np.arange(NUMBER_OF_GRID_TILES_Y)\n        \n        X, Y = np.meshgrid(X,Y)\n\n        ax.plot_surface(X, Y, plot_values, cmap=plt.cm.coolwarm, linewidth=1, rstride=1, cstride=1)\n\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Y\")\n        ax.set_zlabel(\"Value\")\n    \n    plt.suptitle(\"Values at time {}\".format(time_step))\n    plt.show()    \n\nLet us run the algorithm. It takes more than 10 minutes to finish running the algorithm.\n\nreal_time_order_dispatch_algorithm()\n\n[[[[0.55       0.52       0.52       0.53       0.53       0.42\n    0.5        0.5        0.53       0.6        0.39       0.57\n    0.47       0.52       0.5        0.48       0.58       0.49\n    0.57       0.42       0.52       0.46       0.46       0.53\n    0.53       0.48       0.46       0.48       0.47       0.52\n    0.49       0.5        0.53       0.46       0.52       0.52\n    0.43       0.52       0.53       0.48       0.54       0.46\n    0.58       0.41       0.48       0.34       0.5        0.48\n    0.42       0.45      ]\n   [0.92727273 1.36538462 1.01923077 1.16981132 1.26415094 1.42857143\n    1.         1.24       1.37735849 1.23333333 1.28205128 1.33333333\n    1.06382979 1.21153846 1.08       1.14583333 1.12068966 1.18367347\n    1.22807018 1.11904762 1.03846154 1.26086957 1.2826087  1.33962264\n    1.22641509 1.02083333 1.43478261 1.22916667 1.19148936 1.28846154\n    1.18367347 1.3        1.11320755 1.2826087  1.21153846 1.11538462\n    1.23255814 1.17307692 1.18867925 1.25       1.03703704 1.2826087\n    1.31034483 1.12195122 1.22916667 1.35294118 1.16       1.10416667\n    1.21428571 1.06666667]]\n\n  [[0.44       0.52       0.54       0.41       0.44       0.49\n    0.49       0.51       0.54       0.5        0.49       0.37\n    0.45       0.46       0.49       0.53       0.54       0.55\n    0.49       0.39       0.51       0.49       0.5        0.43\n    0.52       0.5        0.49       0.47       0.42       0.36\n    0.43       0.44       0.48       0.46       0.47       0.47\n    0.48       0.45       0.43       0.46       0.48       0.47\n    0.47       0.43       0.39       0.49       0.45       0.47\n    0.51       0.48      ]\n   [1.59090909 1.53846154 1.55555556 1.58536585 1.59090909 1.73469388\n    1.51020408 1.60784314 1.48148148 1.58       1.65306122 1.62162162\n    1.57777778 1.58695652 1.55102041 1.64150943 1.64814815 1.63636364\n    1.69387755 1.66666667 1.62745098 1.57142857 1.76       1.55813953\n    1.59615385 1.68       1.75510204 1.68085106 1.57142857 1.55555556\n    1.48837209 1.65909091 1.625      1.67391304 1.55319149 1.68085106\n    1.5625     1.53333333 1.6744186  1.60869565 1.70833333 1.59574468\n    1.72340426 1.58139535 1.58974359 1.69387755 1.75555556 1.53191489\n    1.45098039 1.52083333]]\n\n  [[0.53       0.47       0.48       0.56       0.48       0.49\n    0.49       0.52       0.51       0.48       0.49       0.54\n    0.53       0.53       0.55       0.47       0.52       0.52\n    0.49       0.52       0.51       0.49       0.46       0.49\n    0.48       0.51       0.54       0.52       0.47       0.52\n    0.57       0.42       0.34       0.48       0.48       0.45\n    0.49       0.5        0.53       0.49       0.54       0.49\n    0.48       0.48       0.5        0.52       0.46       0.5\n    0.55       0.53      ]\n   [1.35849057 1.4893617  1.5625     1.30357143 1.5        1.34693878\n    1.48979592 1.38461538 1.52941176 1.45833333 1.59183673 1.51851852\n    1.43396226 1.47169811 1.43636364 1.46808511 1.55769231 1.55769231\n    1.40816327 1.34615385 1.43137255 1.65306122 1.45652174 1.6122449\n    1.5        1.60784314 1.37037037 1.57692308 1.5106383  1.53846154\n    1.66666667 1.54761905 1.58823529 1.58333333 1.4375     1.44444444\n    1.48979592 1.44       1.24528302 1.34693878 1.37037037 1.63265306\n    1.52083333 1.35416667 1.42       1.51923077 1.5        1.44\n    1.45454545 1.50943396]]]\n\n\n [[[0.71       0.76       0.73       0.69       0.67       0.71\n    0.77       0.72       0.78       0.65       0.78       0.75\n    0.7        0.82       0.72       0.78       0.79       0.72\n    0.76       0.86       0.69       0.82       0.67       0.61\n    0.79       0.82       0.77       0.79       0.82       0.83\n    0.84       0.72       0.71       0.76       0.76       0.68\n    0.77       0.83       0.72       0.83       0.8        0.8\n    0.78       0.74       0.76       0.67       0.8        0.8\n    0.68       0.75      ]\n   [1.11267606 1.11842105 1.09589041 1.33333333 0.92537313 1.04225352\n    1.         1.13888889 1.17948718 1.07692308 1.06410256 1.16\n    1.18571429 1.03658537 1.125      1.02564103 1.01265823 1.11111111\n    1.02631579 1.20930233 1.14492754 1.08536585 1.13432836 0.96721311\n    0.98734177 0.96341463 1.12987013 0.94936709 1.12195122 1.14457831\n    1.03571429 1.16666667 1.05633803 1.         1.03947368 1.11764706\n    0.97402597 1.09638554 1.19444444 1.02409639 1.025      1.05\n    0.98717949 1.06756757 1.10526316 1.31343284 1.0375     1.175\n    0.98529412 0.96      ]]\n\n  [[0.82       0.8        0.75       0.82       0.65       0.76\n    0.79       0.74       0.66       0.79       0.72       0.81\n    0.76       0.66       0.72       0.84       0.72       0.67\n    0.68       0.73       0.82       0.73       0.71       0.83\n    0.86       0.72       0.84       0.66       0.7        0.83\n    0.76       0.7        0.67       0.79       0.73       0.86\n    0.77       0.75       0.83       0.75       0.78       0.82\n    0.73       0.78       0.77       0.78       0.81       0.84\n    0.76       0.76      ]\n   [1.7804878  1.7625     1.68       1.59756098 1.6        1.61842105\n    1.7721519  1.59459459 1.68181818 1.70886076 1.68055556 1.66666667\n    1.68421053 1.86363636 1.70833333 1.54761905 1.63888889 1.65671642\n    1.64705882 1.71232877 1.63414634 1.68493151 1.6056338  1.72289157\n    1.72093023 1.65277778 1.57142857 1.68181818 1.71428571 1.6746988\n    1.67105263 1.58571429 1.76119403 1.72151899 1.75342466 1.61627907\n    1.68831169 1.66666667 1.74698795 1.70666667 1.70512821 1.6097561\n    1.65753425 1.53846154 1.74025974 1.6025641  1.60493827 1.5952381\n    1.68421053 1.65789474]]\n\n  [[0.69       0.82       0.74       0.81       0.7        0.73\n    0.71       0.74       0.72       0.72       0.8        0.69\n    0.81       0.81       0.63       0.79       0.73       0.8\n    0.83       0.7        0.77       0.76       0.77       0.78\n    0.75       0.73       0.79       0.84       0.84       0.75\n    0.82       0.79       0.79       0.84       0.7        0.89\n    0.76       0.83       0.81       0.81       0.76       0.71\n    0.82       0.77       0.78       0.75       0.64       0.83\n    0.84       0.71      ]\n   [1.47826087 1.46341463 1.43243243 1.51851852 1.4        1.57534247\n    1.26760563 1.52702703 1.48611111 1.41666667 1.5625     1.44927536\n    1.50617284 1.54320988 1.41269841 1.44303797 1.52054795 1.5875\n    1.43373494 1.55714286 1.4025974  1.57894737 1.46753247 1.51282051\n    1.34666667 1.52054795 1.39240506 1.32142857 1.38095238 1.44\n    1.42682927 1.48101266 1.46835443 1.47619048 1.61428571 1.48314607\n    1.57894737 1.4939759  1.41975309 1.5308642  1.39473684 1.56338028\n    1.35365854 1.37662338 1.6025641  1.38666667 1.59375    1.36144578\n    1.46428571 1.42253521]]]\n\n\n [[[0.91       0.86       0.9        0.77       0.92       0.98\n    0.84       0.98       0.83       1.         0.91       0.98\n    0.91       0.85       0.9        0.96       0.88       0.97\n    0.89       0.91       0.98       0.85       0.75       0.83\n    1.         0.89       0.86       0.93       0.98       0.85\n    0.97       0.93       0.81       0.87       0.88       0.96\n    0.87       0.95       0.97       0.97       0.9        0.91\n    0.79       0.92       0.96       0.89       0.83       0.86\n    0.84       0.89      ]\n   [0.94505495 1.20930233 0.84444444 0.84415584 0.81521739 0.70408163\n    0.97619048 0.86734694 0.75903614 0.76       0.91208791 1.\n    0.83516484 1.03529412 1.         0.95833333 0.93181818 0.79381443\n    1.03370787 0.83516484 0.89795918 0.81176471 0.90666667 0.93975904\n    0.86       0.91011236 0.89534884 0.89247312 0.91836735 0.96470588\n    0.79381443 0.76344086 0.87654321 0.87356322 0.70454545 0.71875\n    0.83908046 0.94736842 0.92783505 0.94845361 0.93333333 0.85714286\n    0.98734177 0.91304348 0.85416667 0.93258427 0.95180723 0.86046512\n    0.92857143 1.07865169]]\n\n  [[0.88       0.92       0.89       0.89       0.9        0.97\n    0.96       0.92       0.93       0.87       0.87       0.89\n    0.83       0.92       0.95       0.86       0.92       0.94\n    0.88       0.92       0.89       0.98       0.86       0.94\n    0.98       0.98       0.87       0.91       0.9        0.93\n    0.91       0.91       0.88       0.86       0.82       0.87\n    0.84       0.87       0.87       0.88       0.78       0.91\n    1.         0.91       0.95       0.87       0.99       0.81\n    0.94       0.9       ]\n   [1.78409091 1.75       1.87640449 1.70786517 1.8        1.77319588\n    1.8125     1.69565217 1.7311828  1.71264368 1.81609195 1.73033708\n    1.77108434 1.7173913  1.72631579 1.70930233 1.69565217 1.72340426\n    1.71590909 1.7173913  1.70786517 1.74489796 1.80232558 1.73404255\n    1.70408163 1.68367347 1.71264368 1.69230769 1.75555556 1.72043011\n    1.7032967  1.74725275 1.77272727 1.6744186  1.75609756 1.70114943\n    1.73809524 1.72413793 1.63218391 1.72727273 1.78205128 1.82417582\n    1.61       1.65934066 1.67368421 1.73563218 1.72727273 1.71604938\n    1.74468085 1.8       ]]\n\n  [[0.93       0.91       0.98       0.96       0.96       0.99\n    0.9        0.99       0.94       0.91       1.         0.98\n    0.88       0.97       0.83       0.96       0.9        0.97\n    0.92       1.         0.97       0.96       0.95       1.\n    0.99       0.93       0.98       0.79       1.         0.89\n    0.93       0.97       0.91       0.94       0.84       0.89\n    0.94       0.85       0.93       1.         0.95       0.97\n    0.93       0.95       0.99       0.91       0.86       0.92\n    0.91       0.9       ]\n   [1.53763441 1.63736264 1.43877551 1.55208333 1.47916667 1.4040404\n    1.42222222 1.51515152 1.4787234  1.67032967 1.4        1.41836735\n    1.43181818 1.54639175 1.56626506 1.51041667 1.5        1.58762887\n    1.57608696 1.29       1.5257732  1.51041667 1.61052632 1.51\n    1.33333333 1.53763441 1.47959184 1.56962025 1.27       1.53932584\n    1.5483871  1.34020619 1.59340659 1.54255319 1.47619048 1.64044944\n    1.58510638 1.44705882 1.43010753 1.45       1.31578947 1.35051546\n    1.55913978 1.53684211 1.44444444 1.61538462 1.48837209 1.54347826\n    1.38461538 1.58888889]]]]"
  },
  {
    "objectID": "Documents_files/ST455/Week7/Seminar7/HW7_solution.html",
    "href": "Documents_files/ST455/Week7/Seminar7/HW7_solution.html",
    "title": "In this homework assigment, you are required to apply the neural fitted Q-iteration algorithm to a pre-collected dataset for batch (offline) policy optimisation. Please follow the instructions detailed below.",
    "section": "",
    "text": "Step 1: Generate an offline dataset. Consider the CartPole example. We will use a sub-optimal policy for data generation. Specifically, consider the following deterministic policy \\(\\pi_b\\) that returns 0 (left) if the pole angle is negative and 1 otherwise. To allow exploration, we set the behavior policy to be a mixture of \\(\\pi_b\\) and a uniform random policy. Specifically, the agent will follow the uniform random policy or \\(\\pi_b\\) with equal probability. We simulate 100 episodes under this policy. This yields the offline dataset.\nStep 2: Fitted Q-iteration. We will apply the neural fitted Q-iteration (FQI) algorithm to this offline data to compute an optimal policy with three different choices of \\(\\gamma\\), corresponding to 0.95, 0.99 and 1. Please refer to Page 43 of Lecture 5 for the pseudocode of FQI in batch settings. We repeat the Q-iteration 20 times, e.g., apply supervised learning algorithms 20 times to learn the optimal Q-function. The initial Q-estimator can be set to a zero function. Each iteration yields a Q-estimator, based on which we can derive an estimated optimal policy. In total, we obtain 20 \\(\\times\\) 3 (3 choices of \\(\\gamma\\)) different policies.\n\nTo combine FQI with neural networks, we consider using the MLPregressor function. We can use the default neural network architecture (no need to specify no. of layers or no. of hidden nodes per layer). We may set the maximum number of iterations to 500.\nIn this example, we only have two actions (either pushing the cart to the left or to the right). As such, it would be better to use the second type of value function approximators on Page 11 of Lecture 5 (e.g., for each action, use a separate model for the value). The last type of approximators would be preferred in settings where we have a large action space.\nThe TD target depends on whether the current state is a terminal state or not. For a nonterminal state, the TD target is constructed as in the lecture slide. For a terminal state, the TD target is equal to the immediate reward.\n\nStep 3: Policy evaluation. For each of the computed 60 policies, we use the Monte Carlo method to evaluate the expected return under this policy, by generating 1000 episodes. Finally, plot all the returns in a single figure and comment on the results.\nFirst, let us review the three types of value function approximators\n\nSecond, let us review the fitted Q-teration algorithm for batch policy learning\n\nThe following code implements Step 1 of the assignment.\n\nimport gym  \nfrom sklearn.neural_network import MLPRegressor\nimport numpy as np\n\nenv = gym.make(\"CartPole-v1\")  \n\ndef action(angle, epsilon=0):\n    if np.random.binomial(1, epsilon) == 1:\n        return np.random.choice([0,1])\n    else:\n        if angle &lt; 0:\n            return 0\n        else:\n            return 1\n        \nepisodes = 100\nsar = []\nfor i in range(episodes):\n    obs = env.reset()\n    obs = obs[0]\n    done = False\n    while not done:\n        a = action(obs[2], epsilon=0.5)\n        obs, r, done, info, sth = env.step(a)\n        sar.append([obs, a, r, done])\n\nThe following code implements Step 3 of the assignment. Specifically, it implements the Monte Carlo method for policy evaluation.\n\ndef policy(obs, regr0, regr1):\n    q0 = regr0.predict(np.expand_dims(obs, 0))\n    q1 = regr1.predict(np.expand_dims(obs, 0))\n    if q0 &gt; q1:\n        return 0\n    else:\n        return 1\n\ndef reward(regr0, regr1, episodes=1000):\n    rewards = 0\n    for i in range(episodes):\n        obs = env.reset()\n        obs = obs[0]\n        done = False\n        while not done:\n            a = policy(obs, regr0, regr1)\n            obs, r, done, info, sth = env.step(a)\n            rewards += r\n    rewards = rewards / episodes\n    return rewards\n\nFinally, we implement the FQI algorithm. As given in the instruction, we will use MLPRegressor to solve the supervised learning problem. The input X and y correspond to the \\(n\\times p\\) feature matrix and \\(n\\)-dimensional response vector, respectively. The fit method is used to fit the model and the predict method is used to predict the mean outcome conditional on the features.\n\n## Intialize Q-function to be zero and construct target\nX = np.zeros((len(sar), len(obs)+1))\ny = np.zeros(len(sar))\nfor i in range(len(sar)):\n    X[i,-1] = sar[i][1]\n    X[i,0:len(obs)] = sar[i][0]\n#    y[i] = sar[i][2]\n\nrewardlist = [np.zeros(20), np.zeros(20), np.zeros(20)]\n\nfor j, gam in enumerate([0.95, 0.99, 1]):\n    for i in range(len(sar)):\n        y[i] = sar[i][2]\n    regr0 = MLPRegressor(random_state=1, max_iter=500).fit(X[X[:,-1]==0,0:-1], y[X[:,-1]==0])\n    regr1 = MLPRegressor(random_state=1, max_iter=500).fit(X[X[:,-1]==1,0:-1], y[X[:,-1]==1])\n    for k in range(20):\n        for i in range(len(sar)):\n            if sar[i][3]:\n                y[i] = sar[i][2]\n            else:\n                y[i] = sar[i][2] + gam * max(regr0.predict(np.expand_dims(sar[i+1][0], 0)), regr1.predict(np.expand_dims(sar[i+1][0], 0)))\n        regr0 = MLPRegressor(random_state=1, max_iter=500).fit(X[X[:,-1]==0,0:-1], y[X[:,-1]==0])\n        regr1 = MLPRegressor(random_state=1, max_iter=500).fit(X[X[:,-1]==1,0:-1], y[X[:,-1]==1])\n        rewardlist[j][k] = reward(regr0 = regr0, regr1 = regr1)\n    print(j)\n\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n\n\n0\n1\n2\n\n\nWe visualise the expected return\n\nimport matplotlib.pyplot as plt\n\ndef plot_return(rewards):\n    for a in rewards:\n        plt.plot(a, linewidth=3)\n    plt.xlabel('Iteration')\n    plt.ylim(0.0,500.0)\n    plt.ylabel('Regret')\n    legend_str = [0.95, 0.99, 1]\n    plt.legend(legend_str)\n    plt.show()\n\n\nplot_return(rewardlist)"
  },
  {
    "objectID": "Documents_files/ST455/Week9/Seminar9/HW9_solution.html",
    "href": "Documents_files/ST455/Week9/Seminar9/HW9_solution.html",
    "title": "HW9 Solution",
    "section": "",
    "text": "In this homework assignment, you will need to implement the Dyna-Q and Dyna-Q\\(^+\\) algorithms in the Blocking Maze example (Page 34 of Lecture 9) and the Shortcut Maze example (Page 33 of Lecture 9).\n\nIn the Blocking Maze example, obstacles will change after 1000 steps. You may set the step size \\(\\alpha\\) to 0.1, the number of planning steps \\(n\\) to 10, and the weight of bonus reward \\(\\kappa\\) (Page 32 of Lecture 9) to \\(10^{-4}\\). Please plot the cumulative rewards of Dyna-Q and Dyna-Q\\(^{+}\\), up to 3000 time steps, aggregated over 20 independent runs (see e.g., Page 34 in Lecture 9). That is, you run 20 independent trials. For each trial, you run Dyna-Q and Dyan-Q\\(^+\\) 3000 times steps, and average the cumulative reward over 20 trials.\nIn the Shortcut Maze example, obstacles will switch after 3000 steps. You may set the step size \\(\\alpha\\) to 0.1, the number of planning steps \\(n\\) to 50, and the weight of bonus reward \\(\\kappa\\) (Page 32 of Lecture 9) to \\(10^{-3}\\). Please plot the cumulative rewards of Dyna-Q and Dyna-Q\\(^+\\), up to 6000 time steps, aggregated over 5 independent runs (see e.g., Page 33 in Lecture 9).\n\n\n#######################################################################\n# Copyright (C)                                                       #\n# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport heapq\nfrom copy import deepcopy\n\nThe following code implements the standard maze and the Dyna-Q algorithm we discussed in the last seminar\n\nThe aim is to find a shortest path from S to G. Movements are allowed left, right, top and down but not on the greyed out obstacles.\n\nclass Maze:\n    def __init__(self):\n        # maze width\n        self.WORLD_WIDTH = 9\n\n        # maze height\n        self.WORLD_HEIGHT = 6\n\n        # all possible actions\n        self.ACTION_UP = 0\n        self.ACTION_DOWN = 1\n        self.ACTION_LEFT = 2\n        self.ACTION_RIGHT = 3\n        self.actions = [self.ACTION_UP, self.ACTION_DOWN, self.ACTION_LEFT, self.ACTION_RIGHT]\n\n        # start state\n        self.START_STATE = [2, 0]\n\n        # goal state\n        self.GOAL_STATES = [[0, 8]]\n\n        # all obstacles\n        self.obstacles = [[1, 2], [2, 2], [3, 2], [0, 7], [1, 7], [2, 7], [4, 5]]\n        self.old_obstacles = None\n        self.new_obstacles = None\n\n        # time to change obstacles\n        self.obstacle_switch_time = None\n\n        # initial state action pair values\n        # self.stateActionValues = np.zeros((self.WORLD_HEIGHT, self.WORLD_WIDTH, len(self.actions)))\n\n        # the size of q value\n        self.q_size = (self.WORLD_HEIGHT, self.WORLD_WIDTH, len(self.actions))\n\n        # max steps\n        self.max_steps = float('inf')\n\n    # take @action in @state\n    # @return: [new state, reward]\n    def step(self, state, action):\n        x, y = state\n        if action == self.ACTION_UP:\n            x = max(x - 1, 0)\n        elif action == self.ACTION_DOWN:\n            x = min(x + 1, self.WORLD_HEIGHT - 1)\n        elif action == self.ACTION_LEFT:\n            y = max(y - 1, 0)\n        elif action == self.ACTION_RIGHT:\n            y = min(y + 1, self.WORLD_WIDTH - 1)\n        if [x, y] in self.obstacles:\n            x, y = state\n        if [x, y] in self.GOAL_STATES:\n            reward = 1.0\n        else:\n            reward = 0.0\n        return [x, y], reward\n\n\n# a wrapper class for parameters of dyna algorithms\nclass DynaParams:\n    def __init__(self):\n        # discount\n        self.gamma = 0.95\n\n        # probability for exploration\n        self.epsilon = 0.1\n\n        # step size\n        self.alpha = 0.1\n\n        # weight for elapsed time\n        self.time_weight = 0\n\n        # n-step planning\n        self.planning_steps = 5\n\n        # average over several independent runs\n        self.runs = 10\n\n        # threshold for priority queue\n        self.theta = 0\n\n\n# choose an action based on epsilon-greedy algorithm\ndef choose_action(method, state, q_value, tau_value, maze, dyna_params):\n    if np.random.binomial(1, dyna_params.epsilon) == 1:\n        return np.random.choice(maze.actions)\n    else:\n        if method == 'Dyna-Q':\n            values = q_value[state[0], state[1], :]\n        else:\n            values = q_value[state[0], state[1], :] + dyna_params.time_weight*np.sqrt(tau_value[state[0], state[1], :])\n        return np.random.choice([action for action, value in enumerate(values) if value == np.max(values)])\n\n# Trivial model for planning in Dyna-Q\nclass TrivialModel:\n    # @rand: an instance of np.random.RandomState for sampling\n    def __init__(self, rand=np.random):\n        self.model = dict()\n        self.rand = rand\n\n    # feed the model with previous experience\n    def feed(self, state, action, next_state, reward):\n        state = deepcopy(state)\n        next_state = deepcopy(next_state)\n        if tuple(state) not in self.model.keys():\n            self.model[tuple(state)] = dict()\n        self.model[tuple(state)][action] = [list(next_state), reward]\n\n    # randomly sample from previous experience\n    def sample(self):\n        state_index = self.rand.choice(range(len(self.model.keys())))\n        state = list(self.model)[state_index]\n        action_index = self.rand.choice(range(len(self.model[state].keys())))\n        action = list(self.model[state])[action_index]\n        next_state, reward = self.model[state][action]\n        state = deepcopy(state)\n        next_state = deepcopy(next_state)\n        return list(state), action, list(next_state), reward\n\n\n# play for an episode for Dyna-Q algorithm\n# @method: Dyna-Q or Dyna-Q+\n# @q_value: state action pair values, will be updated\n# @model: model instance for planning\n# @maze: a maze instance containing all information about the environment\n# @dyna_params: several params for the algorithm\ndef dyna_q(method, q_value, tau_value, model, maze, dyna_params):\n    state = maze.START_STATE\n    steps = 0\n    while state not in maze.GOAL_STATES:\n        # track the steps\n        steps += 1\n\n        # get action\n        action = choose_action(method, state, q_value, tau_value, maze, dyna_params)\n\n        # take action\n        next_state, reward = maze.step(state, action)\n        \n        # tau-value update\n        tau_value = tau_value + 1\n        tau_value[state[0], state[1], action] = 0\n\n        # Q-Learning update\n        q_value[state[0], state[1], action] += \\\n            dyna_params.alpha * (reward + dyna_params.gamma * np.max(q_value[next_state[0], next_state[1], :]) -\n                                 q_value[state[0], state[1], action])\n\n        # feed the model with experience\n        model.feed(state, action, next_state, reward)\n\n        # sample experience from the model\n        for t in range(0, dyna_params.planning_steps):\n            state_, action_, next_state_, reward_ = model.sample()\n            q_value[state_[0], state_[1], action_] += \\\n                dyna_params.alpha * (reward_ + dyna_params.gamma * np.max(q_value[next_state_[0], next_state_[1], :]) -\n                                     q_value[state_[0], state_[1], action_])\n\n        state = next_state\n\n        # check whether it has exceeded the step limit\n        if steps &gt; maze.max_steps:\n            break\n\n    return steps\n\nNext, let us consider the blocking maze example\n\nBlocking Maze\nIn the Blocking Maze environment the enrivonment changes after a number of iterations. This is a natural assumption as in many real world scenarios the environment does change with time, also measuring states can often not be done without an error, which can be related to changing environments.\nHere, the model finds an optimal policy only for one of the tiles in the optimal policy to be blocked after the environment changes. Further exploration is needed to find the new optimal policy. While the Q-values of the optimal policy slowly decay, it is desirable to find the optimal policy faster.\nThis is exactly the benefit of Dyna-Q+, which favors exploration in the following way. When updating the Q-values based on previous experiences, a bonus is given to actions that haven’t been used in many timesteps. The reward \\(r\\) of an actions is increased by \\(\\kappa \\sqrt{\\tau}\\) where \\(\\kappa\\) is a small constant and \\(\\tau\\) the time that passed since the action in the current state was taken the last time.\n\n\n# wrapper function for changing maze\n# @maze: a maze instance\n# @dynaParams: several parameters for dyna algorithms\n\nmethods = ['Dyna-Q', 'Dyna-Q+']\n\ndef changing_maze(maze, dyna_params):\n    \n    # set up max steps\n    max_steps = maze.max_steps\n\n    # track the cumulative rewards\n    rewards = np.zeros((dyna_params.runs, 2, max_steps))\n\n    for run in tqdm(range(dyna_params.runs)):\n        # set up models\n        model = TrivialModel() #, TimeModel(maze, time_weight=dyna_params.time_weight)]\n\n        # initialize state action values\n        q_values = [np.zeros(maze.q_size), np.zeros(maze.q_size)]\n        # intialize tau values\n        tau_values = np.zeros(maze.q_size)\n\n        for i in range(len(methods)):\n\n            # set old obstacles for the maze\n            maze.obstacles = maze.old_obstacles\n\n            steps = 0\n            last_steps = steps\n            while steps &lt; max_steps:\n                # play for an episode\n                steps += dyna_q(methods[i], q_values[i], tau_values, model, maze, dyna_params)\n\n                # update cumulative rewards\n                rewards[run, i, last_steps: steps] = rewards[run, i, last_steps]\n                rewards[run, i, min(steps, max_steps - 1)] = rewards[run, i, last_steps] + 1\n                last_steps = steps\n\n                if steps &gt; maze.obstacle_switch_time:\n                    # change the obstacles\n                    maze.obstacles = maze.new_obstacles\n\n    # averaging over runs\n    rewards = rewards.mean(axis=0)\n\n    return rewards\n\n\n# set up a blocking maze instance\nblocking_maze = Maze()\nblocking_maze.START_STATE = [5, 3]\nblocking_maze.GOAL_STATES = [[0, 8]]\nblocking_maze.old_obstacles = [[3, i] for i in range(0, 8)]\n\n# new obstalces will block the optimal path\nblocking_maze.new_obstacles = [[3, i] for i in range(1, 9)]\n\n# step limit\nblocking_maze.max_steps = 3000\n\n# obstacles will change after 1000 steps\n# the exact step for changing will be different\n# However given that 1000 steps is long enough for both algorithms to converge,\n# the difference is guaranteed to be very small\nblocking_maze.obstacle_switch_time = 1000\n\n# set up parameters\ndyna_params = DynaParams()\ndyna_params.alpha = 1.0\ndyna_params.planning_steps = 10\ndyna_params.runs = 20\n\n# kappa must be small, as the reward for getting the goal is only 1\ndyna_params.time_weight = 1e-4\n\n# play\nrewards = changing_maze(blocking_maze, dyna_params)\n\nplt.figure(figsize=(8, 6), dpi=100)\n\nfor i in range(len(methods)):\n    plt.plot(rewards[i, :], label=methods[i])\n    \nplt.xlabel('time steps')\nplt.ylabel('cumulative reward')\nplt.legend()\nplt.show()\n\n100%|███████████████████████████████████████████| 20/20 [00:48&lt;00:00,  2.43s/it]\n\n\n\n\n\n\n\nShortcut Maze\nIn the Shortcut Maze we are in the opposite scenario to the Blocking Maze. After the environment changes a new shorter solutions does exist. The previously optimal policy is still valid and has the same reward as before, but is not optimal anymore.\nAgain, continued exploration is needed to find the new optimal policy.\n\n\n# set up a shortcut maze instance\nshortcut_maze = Maze()\nshortcut_maze.START_STATE = [5, 3]\nshortcut_maze.GOAL_STATES = [[0, 8]]\nshortcut_maze.old_obstacles = [[3, i] for i in range(1, 9)]\n\n# new obstacles will have a shorter path\nshortcut_maze.new_obstacles = [[3, i] for i in range(1, 8)]\n\n# step limit\nshortcut_maze.max_steps = 6000\n\n# obstacles will change after 3000 steps\n# the exact step for changing will be different\n# However given that 3000 steps is long enough for both algorithms to converge,\n# the difference is guaranteed to be very small\nshortcut_maze.obstacle_switch_time = 3000\n\n# set up parameters\ndyna_params = DynaParams()\n\n# 50-step planning\ndyna_params.planning_steps = 50\ndyna_params.runs = 5\ndyna_params.time_weight = 1e-3\ndyna_params.alpha = 1.0\n\n# play\nrewards = changing_maze(shortcut_maze, dyna_params)\n\nplt.figure(figsize=(8, 6), dpi=100)\n\nfor i in range(len(methods)):\n    plt.plot( rewards[i, :], label=methods[i])\nplt.xlabel('time steps')\nplt.ylabel('cumulative reward')\nplt.legend()\n\n100%|█████████████████████████████████████████████| 5/5 [01:36&lt;00:00, 19.31s/it]\n\n\n&lt;matplotlib.legend.Legend at 0x7fbec2dd3190&gt;"
  },
  {
    "objectID": "Documents_files/ST455/Week9/Seminar9/Seminar9-Part2.html",
    "href": "Documents_files/ST455/Week9/Seminar9/Seminar9-Part2.html",
    "title": "Setup",
    "section": "",
    "text": "Actor-Critic methods\nActor-Critic methods are policy gradient methods that represent the policy function independent of the value function.\nA policy function (or policy) returns a probability distribution over actions that the agent can take based on the given state. A value function determines the expected return for an agent starting at a given state and acting according to a particular policy forever after.\nIn the Actor-Critic method, the policy is referred to as the actor that proposes a set of possible actions given a state, and the estimated value function is referred to as the critic, which evaluates actions taken by the actor based on the given policy.\nIn this tutorial, both the Actor and Critic will be represented using neural networks.\nCartPole-v1\nIn the CartPole-v1 environment, a pole is attached to a cart moving along a frictionless track. The pole starts upright and the goal of the agent is to prevent it from falling over by applying a force of -1 or +1 to the cart. A reward of +1 is given for every time step the pole remains upright. An episode ends when (1) the pole is more than 15 degrees from vertical or (2) the cart moves more than 2.4 units from the center.\nThe problem is considered “solved” when the average total reward for the episode reaches 495 over 100 consecutive trials.\nThe code below is from this repository\nImport necessary packages and configure global settings.\nimport gym, os\nfrom itertools import count\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nenv = gym.make(\"CartPole-v1\").unwrapped\n\nstate_size = env.observation_space.shape[0]\naction_size = env.action_space.n\nlr = 0.0001"
  },
  {
    "objectID": "Documents_files/ST455/Week9/Seminar9/Seminar9-Part2.html#model",
    "href": "Documents_files/ST455/Week9/Seminar9/Seminar9-Part2.html#model",
    "title": "Setup",
    "section": "Model",
    "text": "Model\nThe Actor and Critic will be modeled using neural networks that generates the action probabilities and critic value respectively.\nDuring the forward pass, the model will take in the state as the input and will output both action probabilities and critic value \\(V\\), which models the state-dependent value function. The goal is to train a model that chooses actions based on a policy \\(\\pi\\) that maximizes expected return.\nFor Cartpole-v1, there are four values representing the state: cart position, cart-velocity, pole angle and pole velocity respectively. The agent can take two actions to push the cart left (0) and right (1) respectively.\n\nclass Actor(nn.Module):\n    def __init__(self, state_size, action_size):\n        super(Actor, self).__init__()\n        self.state_size = state_size\n        self.action_size = action_size\n        self.linear1 = nn.Linear(self.state_size, 128)\n        self.linear2 = nn.Linear(128, 256)\n        self.linear3 = nn.Linear(256, self.action_size)\n\n    def forward(self, state):\n        output = F.relu(self.linear1(state))\n        output = F.relu(self.linear2(output))\n        output = self.linear3(output)\n        distribution = Categorical(F.softmax(output, dim=-1))\n        return distribution\n\n\nclass Critic(nn.Module):\n    def __init__(self, state_size, action_size):\n        super(Critic, self).__init__()\n        self.state_size = state_size\n        self.action_size = action_size\n        self.linear1 = nn.Linear(self.state_size, 128)\n        self.linear2 = nn.Linear(128, 256)\n        self.linear3 = nn.Linear(256, 1)\n\n    def forward(self, state):\n        output = F.relu(self.linear1(state))\n        output = F.relu(self.linear2(output))\n        value = self.linear3(output)\n        return value\n\n\nComputing expected returns\nThe sequence of rewards for each timestep \\(t\\), \\(\\{r_{t}\\}^{T}_{t=1}\\) collected during one episode is converted into a sequence of expected returns \\(\\{G_{t}\\}^{T}_{t=1}\\) in which the sum of rewards is taken from the current timestep \\(t\\) to \\(T\\) and each reward is multiplied with an exponentially decaying discount factor \\(\\gamma\\):\n\\[G_{t} = \\sum^{T}_{t'=t} \\gamma^{t'-t}r_{t'}\\]\nSince \\(\\gamma\\in(0,1)\\), rewards further out from the current timestep are given less weight.\nIntuitively, expected return simply implies that rewards now are better than rewards later. In a mathematical sense, it is to ensure that the sum of the rewards converges.\n\ndef compute_returns(next_value, rewards, masks, gamma=0.99):\n    R = next_value\n    returns = []\n    for step in reversed(range(len(rewards))):\n        R = rewards[step] + gamma * R * masks[step]\n        returns.insert(0, R)\n    return returns\n\n\n\nThe actor-critic loss\n\nActor loss\nThe actor loss is based on policy gradients with the critic as a state dependent baseline and computed with single-sample (per-episode) estimates.\n\\[L_{actor} = -\\sum^{T}_{t=1} \\log\\pi_{\\theta}(a_{t} | s_{t})[G(s_{t}, a_{t})  - V^{\\pi}_{\\theta}(s_{t})]\\]\nwhere: - \\(T\\): the number of timesteps per episode, which can vary per episode - \\(s_{t}\\): the state at timestep \\(t\\) - \\(a_{t}\\): chosen action at timestep \\(t\\) given state \\(s\\) - \\(\\pi_{\\theta}\\): is the policy (actor) parameterized by \\(\\theta\\) - \\(V^{\\pi}_{\\theta}\\): is the value function (critic) also parameterized by \\(\\theta\\) - \\(G = G_{t}\\): the expected return for a given state, action pair at timestep \\(t\\)\nA negative term is added to the sum since the idea is to maximize the probabilities of actions yielding higher rewards by minimizing the combined loss.\n\n\nAdvantage\nThe \\(G - V\\) term in our \\(L_{actor}\\) formulation is called the advantage, which indicates how much better an action is given a particular state over a random action selected according to the policy \\(\\pi\\) for that state.\nWhile it’s possible to exclude a baseline, this may result in high variance during training. And the nice thing about choosing the critic \\(V\\) as a baseline is that it trained to be as close as possible to \\(G\\), leading to a lower variance.\nIn addition, without the critic, the algorithm would try to increase probabilities for actions taken on a particular state based on expected return, which may not make much of a difference if the relative probabilities between actions remain the same.\nFor instance, suppose that two actions for a given state would yield the same expected return. Without the critic, the algorithm would try to raise the probability of these actions based on the objective \\(J\\). With the critic, it may turn out that there’s no advantage (\\(G - V = 0\\)) and thus no benefit gained in increasing the actions’ probabilities and the algorithm would set the gradients to zero.\n\n\n\n\nCritic loss\nTraining \\(V\\) to be as close possible to \\(G\\) can be set up as a regression problem with the following loss function:\n\\[L_{critic} = L_{\\delta}(G, V^{\\pi}_{\\theta})\\]\nwhere \\(L_{\\delta}\\) is the squared-error loss.\n\n\n\nTraining\nTo train the agent, you will follow these steps:\n\nRun the agent on the environment to collect training data per episode.\nCompute expected return at each time step.\nCompute the loss for the actor and critic models.\nCompute gradients and update network parameters.\nRepeat 1-4 until either success criterion or max episodes has been reached.\n\nNote that this method differs algorithmically from the lecture and also from the Sutton-Barto Book. The weights are updated online after each step, while the implementation in this Notebook only updates parameters in an offline manner at the end of an episode.\n\ndef trainIters(actor, critic, n_iters):\n    optimizerA = optim.Adam(actor.parameters())\n    optimizerC = optim.Adam(critic.parameters())\n    for iter in range(n_iters):\n        state = env.reset()\n        state = state[0]\n        log_probs = []\n        values = []\n        rewards = []\n        masks = []\n        entropy = 0\n        env.reset()\n\n        for i in count():\n            env.render()\n            state = torch.FloatTensor(state).to(device)\n            dist, value = actor(state), critic(state)\n\n            action = dist.sample()\n            next_state, reward, done, _, _ = env.step(action.cpu().numpy())\n\n            log_prob = dist.log_prob(action).unsqueeze(0)\n            entropy += dist.entropy().mean()\n\n            log_probs.append(log_prob)\n            values.append(value)\n            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n\n            state = next_state\n\n            if done:\n                print('Iteration: {}, Score: {}'.format(iter, i))\n                break\n\n\n        next_state = torch.FloatTensor(next_state).to(device)\n        next_value = critic(next_state)\n        returns = compute_returns(next_value, rewards, masks)\n\n        log_probs = torch.cat(log_probs)\n        returns = torch.cat(returns).detach()\n        values = torch.cat(values)\n\n        advantage = returns - values\n\n        actor_loss = -(log_probs * advantage.detach()).mean()\n        critic_loss = advantage.pow(2).mean()\n\n        optimizerA.zero_grad()\n        optimizerC.zero_grad()\n        actor_loss.backward()\n        critic_loss.backward()\n        optimizerA.step()\n        optimizerC.step()\n    env.close()\n\n\nactor = Actor(state_size, action_size).to(device)\ncritic = Critic(state_size, action_size).to(device)\ntrainIters(actor, critic, n_iters=100)\n\n/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py:211: UserWarning: WARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v1\", render_mode=\"rgb_array\")\n  gym.logger.warn(\n\n\nIteration: 0, Score: 12\nIteration: 1, Score: 13\nIteration: 2, Score: 32\nIteration: 3, Score: 33\nIteration: 4, Score: 19\nIteration: 5, Score: 21\nIteration: 6, Score: 15\nIteration: 7, Score: 24\nIteration: 8, Score: 11\nIteration: 9, Score: 20\nIteration: 10, Score: 9\nIteration: 11, Score: 14\nIteration: 12, Score: 44\nIteration: 13, Score: 87\nIteration: 14, Score: 37\nIteration: 15, Score: 16\nIteration: 16, Score: 10\nIteration: 17, Score: 11\nIteration: 18, Score: 15\nIteration: 19, Score: 16\nIteration: 20, Score: 50\nIteration: 21, Score: 39\nIteration: 22, Score: 28\nIteration: 23, Score: 13\nIteration: 24, Score: 12\nIteration: 25, Score: 10\nIteration: 26, Score: 27\nIteration: 27, Score: 12\nIteration: 28, Score: 53\nIteration: 29, Score: 12\nIteration: 30, Score: 30\nIteration: 31, Score: 48\nIteration: 32, Score: 32\nIteration: 33, Score: 14\nIteration: 34, Score: 30\nIteration: 35, Score: 27\nIteration: 36, Score: 25\nIteration: 37, Score: 63\nIteration: 38, Score: 25\nIteration: 39, Score: 26\nIteration: 40, Score: 43\nIteration: 41, Score: 10\nIteration: 42, Score: 11\nIteration: 43, Score: 51\nIteration: 44, Score: 13\nIteration: 45, Score: 15\nIteration: 46, Score: 48\nIteration: 47, Score: 58\nIteration: 48, Score: 21\nIteration: 49, Score: 59\nIteration: 50, Score: 29\nIteration: 51, Score: 12\nIteration: 52, Score: 47\nIteration: 53, Score: 43\nIteration: 54, Score: 17\nIteration: 55, Score: 18\nIteration: 56, Score: 12\nIteration: 57, Score: 19\nIteration: 58, Score: 30\nIteration: 59, Score: 14\nIteration: 60, Score: 53\nIteration: 61, Score: 20\nIteration: 62, Score: 42\nIteration: 63, Score: 58\nIteration: 64, Score: 55\nIteration: 65, Score: 50\nIteration: 66, Score: 25\nIteration: 67, Score: 27\nIteration: 68, Score: 36\nIteration: 69, Score: 27\nIteration: 70, Score: 83\nIteration: 71, Score: 90\nIteration: 72, Score: 20\nIteration: 73, Score: 26\nIteration: 74, Score: 90\nIteration: 75, Score: 107\nIteration: 76, Score: 83\nIteration: 77, Score: 65\nIteration: 78, Score: 110\nIteration: 79, Score: 14\nIteration: 80, Score: 63\nIteration: 81, Score: 109\nIteration: 82, Score: 26\nIteration: 83, Score: 42\nIteration: 84, Score: 42\nIteration: 85, Score: 176\nIteration: 86, Score: 66\nIteration: 87, Score: 34\nIteration: 88, Score: 157\nIteration: 89, Score: 251\nIteration: 90, Score: 159\nIteration: 91, Score: 202\nIteration: 92, Score: 132\nIteration: 93, Score: 196\nIteration: 94, Score: 70\nIteration: 95, Score: 80\nIteration: 96, Score: 147\nIteration: 97, Score: 173\nIteration: 98, Score: 112\nIteration: 99, Score: 146"
  },
  {
    "objectID": "Documents_files/ST455/Week9/Seminar9/Seminar9-Part1.html",
    "href": "Documents_files/ST455/Week9/Seminar9/Seminar9-Part1.html",
    "title": "LSE 455 Seminar 9 : Reinforcement Learning - Dyna: Integrated Planning, Acting, and Learning",
    "section": "",
    "text": "The content of this notebook accompanies Chapter 8 in the Sutton & Barto book.\n\n\n\n#######################################################################\n# Copyright (C)                                                       #\n# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n# Permission given to modify the code as long as you keep this        #\n# declaration at the top                                              #\n#######################################################################\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm #progressbar\nfrom copy import deepcopy\n\nIn the following we implement following Maze\n\nThe aim is to find a shortest path from S to G. Movements are allowed left, right, top and down but not on the greyed out obstacles.\nStandard Random-sample one step tabular Q-planning methods suffer from only learning approximating the Q-value of tiles of distance \\(k\\) in \\(k\\) many iterations as depicted below.\n\nThe Dyna-Q Method on the right on the other hand, which revisits previously observed states (“Planning”) is able to offline at the end of an episode to update the Q-values accordingly.\n\nclass Maze:\n    def __init__(self):\n        # maze width\n        self.WORLD_WIDTH = 9\n\n        # maze height\n        self.WORLD_HEIGHT = 6\n\n        # all possible actions\n        self.ACTION_UP = 0\n        self.ACTION_DOWN = 1\n        self.ACTION_LEFT = 2\n        self.ACTION_RIGHT = 3\n        self.actions = [self.ACTION_UP, self.ACTION_DOWN, self.ACTION_LEFT, self.ACTION_RIGHT]\n\n        # start state\n        self.START_STATE = [2, 0]\n\n        # goal state\n        self.GOAL_STATES = [[0, 8]]\n\n        # all obstacles\n        self.obstacles = [[1, 2], [2, 2], [3, 2], [0, 7], [1, 7], [2, 7], [4, 5]]\n        \n        # the size of q value\n        self.q_size = (self.WORLD_HEIGHT, self.WORLD_WIDTH, len(self.actions))\n\n        # max steps\n        self.max_steps = float('inf')\n\n    # take @action in @state\n    # @return: [new state, reward]\n    def step(self, state, action):\n        x, y = state\n        if action == self.ACTION_UP:\n            x = max(x - 1, 0)\n        elif action == self.ACTION_DOWN:\n            x = min(x + 1, self.WORLD_HEIGHT - 1)\n        elif action == self.ACTION_LEFT:\n            y = max(y - 1, 0)\n        elif action == self.ACTION_RIGHT:\n            y = min(y + 1, self.WORLD_WIDTH - 1)\n        if [x, y] in self.obstacles:\n            x, y = state\n        if [x, y] in self.GOAL_STATES:\n            reward = 1.0\n        else:\n            reward = 0.0\n        return [x, y], reward\n\n# a wrapper class for parameters of dyna algorithms\nclass DynaParams:\n    def __init__(self):\n        # discount\n        self.gamma = 0.95\n\n        # probability for exploration\n        self.epsilon = 0.1\n\n        # step size\n        self.alpha = 0.1\n\n        # weight for elapsed time\n        self.time_weight = 0\n\n        # n-step planning\n        self.planning_steps = 5\n\n        # average over several independent runs\n        self.runs = 10\n\n        # threshold for priority queue\n        self.theta = 0\n\n\n# choose an action based on epsilon-greedy algorithm\ndef choose_action(state, q_value, maze, dyna_params):\n    if np.random.binomial(1, dyna_params.epsilon) == 1:\n        return np.random.choice(maze.actions)\n    else:\n        values = q_value[state[0], state[1], :]\n        return np.random.choice([action for action, value in enumerate(values) if value == np.max(values)])\n\n# Table lookup model for planning in Dyna-Q\nclass TrivialModel:\n    # @rand: an instance of np.random.RandomState for sampling\n    def __init__(self, rand=np.random):\n        self.model = dict()\n        self.rand = rand\n\n    # feed the model with previous experience\n    def feed(self, state, action, next_state, reward):\n        state = deepcopy(state)\n        next_state = deepcopy(next_state)\n        if tuple(state) not in self.model.keys(): # keys of dictionary shall be immutable, use tuble instead of list\n            self.model[tuple(state)] = dict()\n        self.model[tuple(state)][action] = [list(next_state), reward] \n    # In the last line, instead of appending the next_state-reward pair into a list, we replace it with the most\n    # recent observations. This is reasonable because under the current environment, the state transition and reward\n    # functions are deterministic. For each state-action pair, the associated next_state-reward pair can only take one\n    # value\n\n    # randomly sample from previous experience\n    def sample(self):\n        state_index = self.rand.choice(range(len(self.model.keys())))\n        state = list(self.model)[state_index]\n        action_index = self.rand.choice(range(len(self.model[state].keys())))\n        action = list(self.model[state])[action_index]\n        next_state, reward = self.model[state][action] \n        # For each state-action pair, there is only one unique next_state-reward pair\n        state = deepcopy(state) ## list is mutable, use deepcopy instead\n        next_state = deepcopy(next_state)\n        return list(state), action, list(next_state), reward\n\n\n# play for an episode for Dyna-Q algorithm\n# @q_value: state action pair values, will be updated\n# @model: model instance for planning\n# @maze: a maze instance containing all information about the environment\n# @dyna_params: several params for the algorithm\ndef dyna_q(q_value, model, maze, dyna_params):\n    state = maze.START_STATE\n    steps = 0\n    while state not in maze.GOAL_STATES:\n        # track the steps\n        steps += 1\n\n        # get action\n        action = choose_action(state, q_value, maze, dyna_params)\n\n        # take action\n        next_state, reward = maze.step(state, action)\n\n        # Q-Learning update\n        q_value[state[0], state[1], action] += \\\n            dyna_params.alpha * (reward + dyna_params.gamma * np.max(q_value[next_state[0], next_state[1], :]) -\n                                 q_value[state[0], state[1], action])\n\n        # feed the model with experience\n        model.feed(state, action, next_state, reward)\n\n        # sample experience from the model\n        for t in range(0, dyna_params.planning_steps):\n            state_, action_, next_state_, reward_ = model.sample()\n            q_value[state_[0], state_[1], action_] += \\\n                dyna_params.alpha * (reward_ + dyna_params.gamma * np.max(q_value[next_state_[0], next_state_[1], :]) -\n                                     q_value[state_[0], state_[1], action_])\n\n        state = next_state\n\n        # check whether it has exceeded the step limit\n        if steps &gt; maze.max_steps:\n            break\n\n    return steps\n\n\n# set up an instance for DynaMaze\ndyna_maze = Maze()\ndyna_params = DynaParams()\n\nruns = 10\nepisodes = 50\nplanning_steps = [0, 5, 50]\nsteps = np.zeros((len(planning_steps), episodes))\n\nfor run in tqdm(range(runs)):\n    for i, planning_step in enumerate(planning_steps):\n        dyna_params.planning_steps = planning_step\n        q_value = np.zeros(dyna_maze.q_size)\n\n        # generate an instance of Dyna-Q model\n        model = TrivialModel()\n        for ep in range(episodes):\n            # print('run:', run, 'planning step:', planning_step, 'episode:', ep)\n            steps[i, ep] += dyna_q(q_value, model, dyna_maze, dyna_params)\n\n# averaging over runs\nsteps /= runs\n\nplt.figure(figsize=(8, 6), dpi=80)\n\n\nfor i in range(len(planning_steps)):\n    plt.plot(steps[i, :], label='%d planning steps' % (planning_steps[i]), linewidth=3)\n\nplt.xlabel('episodes')\nplt.ylabel('steps per episode')\nplt.legend()\nplt.show()\n\n100%|███████████████████████████████████████████| 10/10 [00:32&lt;00:00,  3.28s/it]"
  },
  {
    "objectID": "Documents_files/ST455/Week2/Seminar2/Seminar2-Part1.html",
    "href": "Documents_files/ST455/Week2/Seminar2/Seminar2-Part1.html",
    "title": "Part 1: Implementation of the Tiger Problem",
    "section": "",
    "text": "Recall from the lecture the Tiger Problem:\n \nAs we have discussed in the lecture, the data generated from this example forms a POMDP. As such, the optimal policy is no longer stationary in this case. In this notebook, we will conduct a simulation study to show the advantage of history-dependent policies over the stationary policy. We first derive the form of the optimal stationary policy in this example.\nGiven an initial state (the inferred location of the tiger, either L or R), the class of stationary policies can be listed as follows:\n\nL \\(\\rightarrow\\) Open R; R \\(\\rightarrow\\) Open L\nL \\(\\rightarrow\\) Listen; R \\(\\rightarrow\\) Open L\nL \\(\\rightarrow\\) Open R; R \\(\\rightarrow\\) Listen\nL \\(\\rightarrow\\) Listen; R \\(\\rightarrow\\) Listen\nL \\(\\rightarrow\\) Open R; R \\(\\rightarrow\\) Open R\nL \\(\\rightarrow\\) Listen; R \\(\\rightarrow\\) Open R\nL \\(\\rightarrow\\) Open L; R \\(\\rightarrow\\) Listen\nL \\(\\rightarrow\\) Open L; R \\(\\rightarrow\\) Open L\nL \\(\\rightarrow\\) Open L; R \\(\\rightarrow\\) Open R\n\nThe last five policies involve actions to open L (or R) if we hear tiger on the L (or R). As such, these policies are not good. They will yield low rewards. The fourth policy chooses to listen at each time step. Since listenning gives us a reward of -1, it will yield a low reward as well. Among the first three policies, the first policy will give us the largest reward, since the second and third involve listening which gives us a slight penalty.\nTo summarize, according to the optimal stationary policy, we will open L (or R) if we hear tiger on the R (or L). In the following, we compare this policy with several history-dependent policies that first chooses to listen \\(k\\) (3,5,7,9,…) many times and then open L (or R) if we hear tiger on the R (or L) most of the times. When \\(k=1\\), it reduces to the optimal stationary policy.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nnp.random.seed(42)\n\nlisten = 0\nopen_left = 1\nopen_right = 2\n\nprobability_hear_correctly = 0.85\nreward_wrong_opening = -100\nreward_correct_opening = 10\nreward_listening = -1\n\nnum_trials = 1000\n\n\ndef step(action, tiger_is_left):\n    if action is listen:\n        hear_tiger_left = np.random.binomial(1, probability_hear_correctly if tiger_is_left else 1 - probability_hear_correctly)\n        return reward_listening, hear_tiger_left\n    if action is open_left:\n        if tiger_is_left:\n            return reward_wrong_opening, None\n        else:\n            return reward_correct_opening, None\n    # action is open_right     \n    if tiger_is_left:\n        return reward_correct_opening, None\n    else:\n        return reward_wrong_opening, None \n\ndef run_tiger_problem(num_trials, num_listen):\n    total_rewards = np.zeros(num_trials)\n    for trial in range(num_trials):\n        tiger_is_left = np.random.binomial(1,.5)\n        total_reward = 0\n        count_hear_left = 0\n        count_hear_right = 0\n        for _ in range(num_listen): \n            reward, hear_tiger_left = step(listen, tiger_is_left)\n            total_reward += reward\n            if hear_tiger_left:\n                count_hear_left += 1\n            else:\n                count_hear_right +=1 \n        reward, _ = step(open_left if count_hear_left &lt; count_hear_right else  (open_right if count_hear_right &lt; count_hear_left else (open_left if np.random.binomial(1, 0.5) else open_right)), tiger_is_left)\n        total_reward += reward\n        total_rewards[trial] = total_reward\n    return np.mean(total_rewards)\n\n\nmax_count_listen = np.abs(reward_correct_opening / reward_listening).astype(int)\n# list_of_listen_counts = list(range(max_count_listen))\nlist_of_listen_counts = list(range(1,max_count_listen,2))\nreward_data = np.zeros((len(list_of_listen_counts),2))\nfor j,i in enumerate(list_of_listen_counts):\n    average_reward = run_tiger_problem(num_trials, i)\n    reward_data[j] = [i, average_reward]\n    # print(\"Average return for {} tries is {}\".format(i, average_reward))\nplt.xlabel('number of times listening')\nplt.ylabel('average reward')\nplt.ylim(-20,10)\nfor i in range(max_count_listen):\n    plt.plot(reward_data[:,0], reward_data[:,1])\nplt.show()"
  },
  {
    "objectID": "Documents_files/ST455/Week2/Seminar2/Seminar2-Part2.html#sketch-of-the-proof",
    "href": "Documents_files/ST455/Week2/Seminar2/Seminar2-Part2.html#sketch-of-the-proof",
    "title": "Part 2: Proof of the Existence of the Optimal Stationary Policy",
    "section": "Sketch of the Proof",
    "text": "Sketch of the Proof\n\\(\\newcommand{\\P}{\\mathbb{P}}\\) \\(\\newcommand{\\E}{\\mathbb{E}}\\) \\(\\newcommand{\\S}{\\mathcal{S}}\\) \\(\\newcommand{\\A}{\\mathcal{A}}\\) \\(\\newcommand{\\R}{\\mathbb{R}}\\) \\(\\newcommand{\\L}{\\mathcal{L}}\\) \\(\\newcommand{\\Vpi}{V^{\\pi}}\\)\nWe aim to show under the MDP assumption (Markovianity & time-homogeneity), there exists an optimal stationary deterministic policy whose value is no worse than any history-dependent policy. Formally speaking, we aim to show the existence of \\(\\pi^*\\in\\) SD such that\n\\[V^{\\pi^*}(s)=\\sup_{\\pi \\in \\mathrm{HR}} \\Vpi(s)\\]\nfor any state \\(s\\), where HR is the class of history-dependent (random) policies and SD is the class of stationary deterministic policies. We also use MR to denote the class of Markov (random) policies.\nHere the function \\(\\Vpi(s)\\) is the value function, that is, the expected discounted reward in an infinite horizon with policy \\(\\pi\\) and initial state \\(s\\): \\[\n\\Vpi(s) = \\sum_{t = 0}^{\\infty} \\gamma^t E^{\\pi}(R_t \\mid S_0 = s)\\]\nTo simplify the proof, we focus on the setting where \\(R_t\\) is a deterministic function of \\((A_t,S_t)\\), i.e., \\(R_t=r(S_t, A_t)\\) for some reward function \\(r\\).\nRecall that the Markov Assumption meant memorylessness in the sense that for any state \\(s\\)\n\\[\\P(S_{t+1} = s \\mid A_t, S_t, \\ldots, A_1,S_1,A_0,S_0) = \\P(S_{t+1} = s \\mid A_t, S_t)\\]\nIn addition, neither \\(r\\) nor \\(\\P(S_{t+1} = s \\mid A_t, S_t)\\) depend on \\(t\\), under the time-homogeneity assumption.\nWe will prove this statement in three steps. We will follow the proof of Shi et al (2020).\n\nFirst, we show that Markov Policies are as good as history-dependent policies: \\(\\sup_{\\pi \\in \\mathrm{HR}} \\Vpi(s) = \\sup_{\\pi \\in \\mathrm{MR}} \\Vpi(s)\\) for any \\(s\\). 2.For any function \\(\\nu\\) that satisfies the Bellman optimality equation we have \\(\\nu(s) \\ge \\sup_{\\pi \\in \\mathrm{MR}} \\Vpi(s)\\).\nExistence of \\(\\pi^* \\in \\mathrm{SD}\\) such that \\(V^{\\pi^*}\\) satisfies the Bellman optimality equation.\n\nCombining 2. together with 3. yields \\(V^{\\pi^*}(s) \\ge \\sup_{\\pi \\in \\mathrm{MR}} \\Vpi(s)\\). Together with 1., we obtain \\(V^{\\pi^*}(s) \\ge \\sup_{\\pi \\in \\mathrm{HR}} \\Vpi(s)\\). The proof is completed.\n\nProof of 1.\nWe prove the following equation. For any policy \\(\\pi \\in \\mathrm{HR}\\), there exists a policy \\(\\bar \\pi \\in \\mathrm{MR}\\) such that for all start states \\(s_0\\) and any state \\(s\\) and action \\(a\\) we have\n\\[ \\P^\\pi(A_t = a, S_t = s | S_0 = s_0) = \\P^{\\bar \\pi}(A_t = a, S_t = s | S_0 = s_0) \\]\nby induction. Here, \\(\\bar \\pi\\) is allowed to depend on \\(s_0\\). But it does not depend on \\(s\\) and \\(a\\). This equation implies that for any \\(s_0\\) and \\(\\pi\\), there exists a Markov policy \\(\\bar \\pi\\) under which the generated state-action pair at each time has the same conditional distribution as that under \\(\\pi\\) given \\(S_0=s_0\\). Since the reward is a deterministic function of the state-action pair, the expected values of rewards given \\(S_0=s_0\\) under \\(\\pi\\) and \\(\\bar \\pi\\) are the same. The proof is thus completed.\nFor \\(t = 0\\) the statement is trivial. Now assume that the statement holds for some time \\(t\\). We will prove that then it also holds for \\(t + 1\\).\nBy conditional probability we have for all \\(s\\) that\n\\[\n\\begin{align}\n\\P^\\pi(S_{t+1} = s | S_0 = s_0) &= \\E^\\pi(\\P(S_{t+1} = s | A_t, S_t, S_0 = s_0) | S_0 = s_0) \\\\\n&= \\E^\\pi(\\P(S_{t+1} = s | A_t, S_t) | S_0 = s_0) \\qquad \\qquad \\text{by Markov Assumption} \\\\\n&= \\E^{\\bar \\pi}(\\P(S_{t+1} = s | A_t, S_t) | S_0 = s_0) \\qquad \\qquad \\text{by induction hypothesis}  \\\\\n&= \\P^{\\bar \\pi}(S_{t+1} = s | S_0 = s_0).\n\\end{align}\n\\]\nHence we have that\n\\[\n\\begin{align}\n\\P^{\\pi}(A_{t+1} = a, S_{t+1} = s \\mid S_0 = s_0) &= \\P^\\pi(A_{t+1} = a | S_{t+1} = s, S_0 = s_0) \\P^{\\pi}(S_{t+1} = s | S_0 = s_0) \\\\\n&= \\P^{\\bar \\pi}(A_{t+1} = a | S_{t+1} = s, S_0 = s_0) \\P^{\\bar \\pi}(S_{t+1} = s | S_0 = s_0) \\\\\n&= \\P^{\\bar \\pi}(A_{t+1} = a, S_{t+1} = s \\mid s_0),\n\\end{align}\n\\]\nwhere the action probability of \\(\\bar \\pi\\) at time step \\(t +1\\) is defined to be equal to the action probability of \\(\\pi\\) given \\(S_0=s_0\\).\n\n\nProof of 2.\nRecall that a function \\(\\nu: s \\to \\mathbb{R}\\) fulfills the Bellman optimality equations if\n\\[\n\\nu(s) = \\max_{a} \\left[r(s, a) + \\gamma \\sum_{s'} \\nu(s') \\P(S_{t+1}=s'|A_t=a,S_t=s)\\right]\n\\]\n\nI want to stress the point that at this point we have not proven that such a \\(\\nu\\) exists. Nonetheless we can make statements that hold for all \\(\\nu\\) that fulfill these equations. We now prove Step 2, that is \\(\\nu(s) \\ge \\sup_{\\pi \\in \\mathrm{MR}} \\Vpi(s)\\). \nLet \\(\\pi=\\{\\pi_k\\}_k \\in \\mathrm{MR}\\) be arbitrary where \\(\\pi_k\\) denotes the mapping the agent implements at time \\(k\\). We aim to show \\(\\nu(s)\\ge \\Vpi(s)\\). As \\(\\nu\\) fulfills the Bellman equations we in particular have that \\[\n\\nu(s) \\ge r(s, a) + \\gamma \\sum_{s'} \\nu(s') \\P(S_{t+1}=s'|A_t=a,S_t=s),\n\\] for any \\(a\\). As such, \\[\\begin{eqnarray*}\n\\nu(s) &\\ge& \\sum_a \\pi_0(a|s) \\left[r(s, a) + \\gamma \\sum_{s'} \\nu(s') \\P(S_{t+1}=s'|A_t=a,S_t=s)\\right]\\\\\n&=&\\sum_a \\pi_0(a|s) r(s, a) + \\gamma \\sum_{a,s'} \\pi_0(a|s) \\nu(s') \\P(S_{t+1}=s'|A_t=a,S_t=s)\n\\end{eqnarray*}\\]\nThe first term on the right-hand-side corresponds to the immediate reward under \\(\\pi_0\\). Iteratively applying the above and choose \\(\\pi_k\\) as the weight function at each iteration we get\n\\[\n\\nu(s) \\ge \\sum_{a,s}\\pi_0(a|s)r(s, a) + \\sum_{k=1}^K \\gamma^k \\E^{\\pi}(r(S_k, A_k) \\mid S_0 = s) + \\gamma^{K+1} \\E^{\\pi}(\\nu(S_{K+1}) \\mid s_0 = s)\n\\]\nAs \\(\\nu\\) is bounded, the last term converges to zero as \\(K \\to \\infty\\). This leads to \\[\\nu(s) \\ge \\Vpi(s)\\]\n\n\n\n\n\nProof of 3.\nWe will use Banach’s fixed point theorem to show the existence of a funciton \\(\\nu_0\\) that satisfies the optimal bellman equation. Suppose the existence has been proven. It follows from Bellman’s equation that \\[\n\\nu_0(s) = \\max_{a} \\left[r(s, a) + \\gamma \\sum_{s'} \\nu_0(s') \\P(S_{t+1}=s'|A_t=a,S_t=s)\\right]\n\\]\nFor each state \\(s\\), there exists an action that maximizes the right-hand-side. This allows us to define a stationary deterministic policy \\(\\pi^*\\) that takes \\(s\\) as input and outputs the argmax. The Bellman optimality equation can be rewritten as \\[\n\\nu_0(s) = \\left[r(s, \\pi^*(s)) + \\gamma \\sum_{s'} \\nu_0(s') \\P(S_{t+1}=s'|A_t=\\pi^*(s),S_t=s)\\right].\n\\] Using similar arguments in Proof of 2., we have by iteratively applying the above equation that \\[\n\\nu_0(s) = \\sum_{s}r(s, \\pi^*(s)) + \\sum_{k=1}^K \\gamma^k \\E^{\\pi^*}(r(S_k, A_k) \\mid S_0 = s) + \\gamma^{K+1} \\E^{\\pi^*}(\\nu_0(S_{K+1}) \\mid s_0 = s)\n\\] Letting \\(K\\to \\infty\\) leads to \\(\\nu_0(s)=V^{\\pi^*}(s)\\) for any \\(s\\).\nIt remains to show the existence. Consider the following function \\(F\\) that maps a function \\(\\nu\\) to another function \\(F_{\\nu}\\) defined on the state space such that\n\\[F_{\\nu}(s) = \\max_{a} \\left[r(s,a) + \\gamma \\sum_{s'} \\nu(s') \\P(S_{t+1}=s'|A_t=a,S_t=s)\\right].\\]\nThen for two different functions \\(\\nu_1, \\nu_2\\) we have\n\\[\n\\begin{aligned}\n\\max_s |F_{\\nu_1}(s) - F_{\\nu_2}(s)| &= \\max_s \\left|\\max_{a} \\left[r(s,a) + \\gamma \\sum_{s'} \\nu_1(s') \\P(S_{t+1}=s'|A_t=a,S_t=s)\\right]-\\max_{a} \\left[r(s,a) + \\gamma \\sum_{s'} \\nu_2(s') \\P(S_{t+1}=s'|A_t=a,S_t=s)\\right]\\right| \\\\\n& \\le \\max_s \\max_a \\left|\\left[r(s,a) + \\gamma \\sum_{s'} \\nu_1(s') \\P(S_{t+1}=s'|A_t=a,S_t=s)\\right]-\\left[r(s,a) + \\gamma \\sum_{s'} \\nu_2(s') \\P(S_{t+1}=s'|A_t=a,S_t=s)\\right]\\right|\\\\\n& \\le \\gamma \\max_{a,s} \\left|\\sum_{s'} [\\nu_1(s')-\\nu_2(s')]\\P(S_{t+1}=s'|A_t=a,S_t=s) \\right| \\\\\n&\\le \\gamma \\max_{a,s,s'} |\\nu_1(s')-\\nu_2(s')| \\left[\\sum_{s'} |\\P(S_{t+1}=s'|A_t=a,S_t=s)|\\right]\\\\\n& = \\gamma \\max_{a,s} |\\nu_1(s)-\\nu_2(s)|\n\\end{aligned}\n\\]\nHence by Banach’s fixed point theorem we have a unique \\(\\nu^*\\) such that \\(F(\\nu^*) = \\nu^*\\). The proof is hence completed."
  },
  {
    "objectID": "Documents_files/ST455/Week2/Seminar2/HW2_solution.html",
    "href": "Documents_files/ST455/Week2/Seminar2/HW2_solution.html",
    "title": "HW2",
    "section": "",
    "text": "Consider a \\(k\\)th-order Markov decision process model that satisfies the \\(k\\)th-order Markov assumption and the time-homogeneity assumption, for some integer \\(k\\ge 1\\). Specifically, the \\(k\\)th-order Markov assumption requires the conditional distribution of \\((S_{t+k},R_{t+k-1})\\) to be conditionally independent of the data history given \\((A_{t+k-1},S_{t+k-1})\\) and observations collected between time \\(t\\) and \\(t+k-2\\). When \\(k=1\\), it is reduced to the standard Markov assumption. The time homogeneity requires this conditional distribution to be stationary over time. Please discuss the form of the optimal policy under this model and justify your answer."
  },
  {
    "objectID": "Documents_files/ST455/Week2/Seminar2/HW2_solution.html#your-answer-here",
    "href": "Documents_files/ST455/Week2/Seminar2/HW2_solution.html#your-answer-here",
    "title": "HW2",
    "section": "your answer here",
    "text": "your answer here\nFor any \\(t\\), we construct a new state vector \\(\\overline{S}_{t}^{(k)}\\) by concatenating measures over the past \\(k\\) time steps. In particular, let \\[\\begin{eqnarray*}\n    \\overline{S}_{t}^{(k)}=(S_t,R_{t-1},A_{t-1},S_{t-1},\\cdots,R_{t-k+1},A_{t-k+1},S_{t-k+1}).\n\\end{eqnarray*}\\] When \\(k=1\\), \\(\\overline{S}_{t}^{(k)}\\) is reduced to \\(S_t\\).\nIn the following, we will show that the transformed data \\(\\{(\\overline{S}_{t}^{(k)},A_t,R_t)\\}_t\\) satisfy the MDP assumption (e.g., Markovanity & time-homogeneity). According to the existence of the optimal stationary policy theorem we discussed in Lecture 2, the optimal policy at time \\(t\\) depends on the history only through \\(\\overline{S}_{t}^{(k)}\\). In addition, these decision rules are stationary over time.\nBy definition, the Markov assumption requirs that\n\\[\\begin{eqnarray*}\n    \\mathbb{P}(\\overline{S}_{t+1}^{(k)},R_t|A_t,S_t,R_{t-1},A_{t-1},S_{t-1},\\cdots,R_0,A_0,S_0)=\\mathbb{P}(\\overline{S}_{t+1}^{(k)},R_t|A_t,\\overline{S}_t^{(k)}).\n\\end{eqnarray*}\\]\nRecall that \\(\\overline{S}_{t+1}^{(k)}\\) is a union of \\(S_{t+1}\\) and observations collected between time \\(t\\) and \\(t+k-2\\). Given \\(A_t\\) and \\(\\overline{S}_t^{(k)}\\), observations collected between time \\(t\\) and \\(t+k-2\\) are fixed. It suffices to show\n\\[\\begin{eqnarray*}\n    \\mathbb{P}(S_{t+1},R_t|A_t,S_t,R_{t-1},A_{t-1},S_{t-1},\\cdots,R_0,A_0,S_0)=\\mathbb{P}(S_{t+1},R_t|A_t,\\overline{S}_t^{(k)}).\n\\end{eqnarray*}\\]\nThis is automatically satisfied under the \\(k\\)th-oder Markov assumption.\nUsing similar arguments, to prove the time-homogeneity assumption, it suffices to show the probability \\(\\mathbb{P}(S_{t+1},R_t|A_t=a,\\overline{S}_t^{(k)}=\\bar{s})\\) to be a stationary function of \\(t\\), for any \\(a\\) and \\(\\bar{s}\\). This is automatically satisfied under the time-homogeneity assumption of the \\(k\\)th-order Markov decision process model."
  },
  {
    "objectID": "Documents_files/ST455/Week2/Seminar2/HW2.html",
    "href": "Documents_files/ST455/Week2/Seminar2/HW2.html",
    "title": "HW2",
    "section": "",
    "text": "Consider a \\(k\\)th-order Markov decision process model that satisfies the \\(k\\)th-order Markov assumption and the time-homogeneity assumption, for some integer \\(k\\ge 1\\). Specifically, the \\(k\\)th-order Markov assumption requires the conditional distribution of \\((S_{t+k},R_{t+k-1})\\) to be conditionally independent of the data history given \\((A_{t+k-1},S_{t+k-1})\\) and observations collected between time \\(t\\) and \\(t+k-2\\). When \\(k=1\\), it is reduced to the standard Markov assumption. The time homogeneity requires this conditional distribution to be stationary over time. Please discuss the form of the optimal policy under this model and justify your answer."
  },
  {
    "objectID": "Documents_files/ST455/Week2/Seminar2/HW2.html#your-answer-here",
    "href": "Documents_files/ST455/Week2/Seminar2/HW2.html#your-answer-here",
    "title": "HW2",
    "section": "your answer here",
    "text": "your answer here"
  },
  {
    "objectID": "Documents_files/ST455/Week5/Seminar5/Seminar5.html",
    "href": "Documents_files/ST455/Week5/Seminar5/Seminar5.html",
    "title": "Seminar 5: Generalization and Function Approximation",
    "section": "",
    "text": "In this seminar, we will practice solving reinforcement problems by using function approximation. Specifically, we will consider the following exercise: * The Mountain Car example: linear function approximator\nAssumed that you have already installed Numpy and Scipy, install sklearn using pip\nimport gym\nimport sklearn.pipeline\nfrom sklearn.kernel_approximation import RBFSampler\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nBefore starting looking into our exercises, we provie a summary of some basic concepts."
  },
  {
    "objectID": "Documents_files/ST455/Week5/Seminar5/Seminar5.html#a-quick-summary-of-some-basic-concepts",
    "href": "Documents_files/ST455/Week5/Seminar5/Seminar5.html#a-quick-summary-of-some-basic-concepts",
    "title": "Seminar 5: Generalization and Function Approximation",
    "section": "A quick summary of some basic concepts",
    "text": "A quick summary of some basic concepts\n\nOn-policy vs off-policy methods\nOn-policy methods aim to estimate the value of the policy that is used for control. Examples of on-policy methods are:\n- On-policy MC control - On-policy TD control: SARSA\nThe updates of action estimates according to SARSA are of the following form:\n\\[\nQ(s_t,a_t){\\leftarrow}Q(s_t,a_t)+\\alpha[r_{t+1}+{\\gamma}Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]\n\\]\nAn on-policy method updates action value estimates using the values of next state-action pairs where the next actions are generated by the current policy.\nIn off-policy methods, the behaviour policy that generates actions may be unrelated to the action that is being used in the target. Examples of off-policy methods include:\n\nOff-policy TD control: Q-learning\n\nThe updates of action estimates according to Q-learning are of the following form:\n\\[\nQ(s_t,a_t){\\leftarrow}Q(s_t,a_t)+\\alpha[r_{t+1}+{\\gamma}\\max_{a}Q(s_{t+1},a)-Q(s_t,a_t)]\n\\]\nQ-learning updates action value estimates using the values of next state-action pairs where the next actions are generated by a greedy policy. That is to say, the action-state values are estimated under the assumption of a greedy policy despite the fact that it is not following a greedy policy.\n\n\nOnline vs offline methods\nIn general, online learning is learning as new data becomes available. An update of state/state-action value is made after each action is taken and a new observation is obtained. Many online learning methods can also be made offline. The following classifications are based on the version of algorithms taught in this course: - TD(0) - Sarsa - Q-learning\nOff-line learning means learning from a static dataset. In our case, learning is done by the end of an entire episode using data collected during the episode. Examples are: - MC methods - n-Step TD - TD(\\(\\lambda\\))\n\n\nRecap on value function approximation\nWhat you have learned in the lecture:"
  },
  {
    "objectID": "Documents_files/ST455/Week5/Seminar5/Seminar5.html#the-mountain-car-problem",
    "href": "Documents_files/ST455/Week5/Seminar5/Seminar5.html#the-mountain-car-problem",
    "title": "Seminar 5: Generalization and Function Approximation",
    "section": "The Mountain Car Problem",
    "text": "The Mountain Car Problem\nThe Mountain Car problem is described in Example 10.1 (2nd Edition) in Sutton & Barto. The basic idea is driving the car to reach the goal position by building up momentum. We will use the existing environment in OpenAI Gym (ID: MountainCar-v0).\n\nActions = [push left (0), no push (1), push right (2)]\nReward is -1 on all time steps until the car reaches its goal\nThe state space is continuous\nThe states have two dimensions: state = [position, velocity]\nPosition: [-1.2, 0.6]\nThe goal is to reach the flag position(0.5)\nVelocity: [-0.7, 0.7]\n\n\nfrom IPython.display import Video\nVideo('./graphs/mountain_car.mp4')\n\n\n      Your browser does not support the video element.\n    \n\n\n\nFeature engineering\nThere are many ways to create the feature vector of a state. We can either use handcrafted or learned features. For instance, AlphaGo uses handcrafted features:\n\nIn reinforcement learning, the commonly used methods to extract features include tile-coding, kernel functions, or neural networks. In this exercise, we will create the feature vectors using kernel methods.\nThe RBF kernel is defined as \\[\nK(x,x')=\\exp(-\\gamma\\|x-x'\\|^2)\n\\]\nFor any positive definite kernel \\(K(x,x')\\) in recursive Kernel Hilbert space (RKHS), we have \\(K(x,x')=\\phi(x)^{\\top}{\\phi(x')}\\), where \\(\\phi\\) is a feature map.\n\nNormalize data\nAs the RBF kernel assumes all features are centred around 0 and have variance in the same order, we will normalize the data as the first step. For the task of normalization and feature engineering, we will use the scikit-learn library.\nAs this method falls into the category of online learning, we will define a normalizer and feature extractor using some pre-training data as follows:\nWe randomly sample 10000 observations as pre-training data for normalization.\n\nopenai_env = 'MountainCar-v0'# env = gym.envs.make('CartPole-v1')\nenv = gym.make(openai_env, render_mode=\"rgb_array\")\nn_actions = env.action_space.n\nactions = np.arange(n_actions)\nsample_size = 10000\n\n#Note: Alternatively create samples via env.observation_space.sample()\n\nobs_samples = []\nnum_samples = 0\nwhile num_samples &lt; sample_size:\n    env.reset()\n    done = False\n    while not done:\n        observation, _, done, _, _, = env.step(env.action_space.sample())\n        obs_samples.append(observation)\n        num_samples += 1\n# obs_samples = np.array([env.observation_space.sample() for i in range(sample_size)])\n# print(obs_samples)\n\nThe following code creates an instance of the sklearn StandardScaler class which has a method .fit()that computes the mean and standard deviation to be used for later scaling.\n\n# scaler = sklearn.preprocessing.StandardScaler().fit(obs_samples)\n\nWhen a new observation comes in, we can simply call\n\n# scaled_state = scaler.transform([state])\n\nto normalize the latest observed state.\n\n\nCreate features\nThe sklearn.pipeline.FeatureUnion class concatenates results of multiple transformer objects. This class allows you to combine multiple feature extraction mechanisms into a single transformer. We will choose 4 RBF kernels with different gamma values and an output dimensionality of 100. Each RBF kernel gives a 100-dimensional random feature vector that can be used to approximate the feature map function. The larger the dimension, the more accurate it approximates the true feature vector. Alternatively, we can select gamma using the median heuristic (see e.g., https://arxiv.org/abs/1707.07269)\nfeature_dim = 100\n\nfeaturiser = sklearn.pipeline.FeatureUnion(\n[(\"rbf1\", RBFSampler(gamma=5.0, n_components=feature_dim)),\n(\"rbf2\", RBFSampler(gamma=2.0, n_components=feature_dim)),\n(\"rbf3\", RBFSampler(gamma=1.0, n_components=feature_dim)),\n(\"rbf4\", RBFSampler(gamma=0.5, n_components=feature_dim))])\nWe now fit all the transformers using the scaled samples by the method .fit() under this class,\nfeaturiser.fit(scaler.transform(obs_samples))\nThe .transform() method of the scaler returns the centred and scaled samples.\nfeatures = featuriser.transform(scaled)\nNow the scaler and featuriser are ready for later use. When we obtain a new state, we can transform it to feature vectors easily.\n\n\n\nControl with function approximation\nWe approximate the action-value function by a linear combination of features, i.e.,\n\\[\\hat{Q}(s,a,\\mathbf{w})=\\phi(s,a)^{\\top}\\mathbf{w}\\]\nThe objective function is the MSE between the true action value \\(Q(s,a)\\) and \\(\\hat{Q}(s,a,\\mathbf{w})\\):\n\\[\n\\mathcal{L(\\mathbf{w})}=\\frac{1}{2}\\mathbb{E}_{s,a,r,s'}[(Q(s,a)-\\hat{Q}(s,a,\\mathbf{w}))^2]\n\\]\nDifferentiating the objective w.r.t. the parameter \\(\\mathbf{w}\\), we obtain the parameter update\n\\[\n\\mathbf{w}_{t+1}=\\mathbf{w}_t+{\\eta}(Q(s_t,a_t)-\\hat{Q}(s_t,a_t,\\mathbf{w}_t))\\phi(s_t,a_t)\n\\]\nUnfortunately, we don’t have access to the true \\(Q(s,a)\\) in practice. We therefore substitute a target for it.\nWe can substituting with a SARSA target\n\\[Q(s_t,a_t)=r_{t}+\\gamma\\hat{Q}(s_{t+1},a_{t+1},\\mathbf{w}_t)\\]\nor a Q-learning target,\n\\[\nQ(s_t,a_t)=r_{t}+\\gamma\\max_{a'}\\hat{Q}(s_{t+1},a',\\mathbf{w}_t)\n\\]\nIn the following implementation, we choose the SARSA target as a substitution to the true state-action values. The update for parameter \\(\\mathbf{w}\\) is therefore\n\\[\n\\mathbf{w}_{t+1}=\\mathbf{w}_t+{\\eta}(r_{t}+\\gamma\\hat{Q}(s_{t+1},a_{t+1},\\mathbf{w}_t)-\\hat{Q}(s_t,a_t,\\mathbf{w}_t))\\phi(s_t,a_t)\n\\]\n\nImplementation of the approximation function\n\nclass Approximator:\n    def __init__(self, obs, dim, n_a, alpha, discount):\n        self.__discount = discount\n        self.__alpha = alpha\n        self.__n_actions = n_a\n        self.__obs_samples = obs\n        self.__feature_dim = dim\n        self.__w_size = 4 * self.__feature_dim\n        self.__w = np.zeros((self.__n_actions, self.__w_size))\n        self.__scaler = None\n        self.__featuriser = None\n        self.__initialised = False\n\n    def initialise_scaler_featuriser(self):\n        self.__scaler = sklearn.preprocessing.StandardScaler().fit(self.__obs_samples)\n        self.__featuriser = sklearn.pipeline.FeatureUnion(\n                [(\"rbf1\", RBFSampler(gamma=5.0, n_components=self.__feature_dim)),\n                 (\"rbf2\", RBFSampler(gamma=2.0, n_components=self.__feature_dim)),\n                 (\"rbf3\", RBFSampler(gamma=1.0, n_components=self.__feature_dim)),\n                 (\"rbf4\", RBFSampler(gamma=0.5, n_components=self.__feature_dim))])\n        self.__featuriser.fit(self.__scaler.transform(self.__obs_samples))\n        self.__initialised = True\n\n    @property\n    def get_w(self):\n        return self.__w\n\n    def feature_transformation(self, state):\n        if not self.__initialised:\n            self.initialise_scaler_featuriser()\n\n        scaled = self.__scaler.transform([state])\n        features = self.__featuriser.transform(scaled)\n        return features\n\n    # linear_features\n    def action_value_estimator(self, features, a):\n        return np.inner(features, self.__w[a])\n\n    # minimising MSE between q(replaced by td target) and q_hat\n    def update_w(self, r, q, next_q, features, a):\n        target = r + self.__discount * next_q\n        td_error = target - q\n        w_gradient = self.__alpha * td_error * features\n        self.__w[a] = self.__w[a] + w_gradient\n        \n    def set_w(self, a, new_w):\n        self.__w[a] = new_w\n\n    def cost_to_go(self, state):\n        features = self.feature_transformation(state)\n        v_s = []\n        for i in range(self.__n_actions):\n            v_s.append(self.action_value_estimator(features, i))\n        return - np.max(v_s)\n\nThe cost to go function is defined as \\(f=-\\max_{a}Q_t(s,a)\\)\n\ndef plot_v(estimator, i=None):\n    if openai_env == 'MountainCar-v0':\n        fig = plt.figure(figsize=(12, 8))\n        ax = fig.add_subplot(projection='3d')\n\n        x = np.linspace(-1.2, 0.6, 30)\n        y = np.linspace(-0.07, 0.07, 30)\n\n        X, Y = np.meshgrid(x, y)\n        states = np.dstack((X, Y))\n\n        values = np.apply_along_axis(estimator.cost_to_go, 2, states)\n\n        ax.plot_surface(X, Y, values, cmap=plt.cm.coolwarm, linewidth=1, rstride=1, cstride=1)\n\n        extra_title = \"\"\n        if i:\n            extra_title = \" after {} iterations\".format(i)\n        ax.set_title(\"Cost to go function\" + extra_title)\n        ax.set_xlabel(\"Position\")\n        ax.set_ylabel(\"Velocity\")\n        ax.set_zlabel(\"Value\")\n        plt.show()\n\n\n\nSolve the problem\nBased on the randomly-sampled 10000 observations, create an instance of the state-action function approximator and initialize the normaliser and feature extractor:\n\nalpha = 0.1\ngamma = 1.0\nepsilon = 0.1\ndim = 100\n\n# create an instance of approximator and initialize the normalizer and feature extractor \nestimator = Approximator(obs_samples, dim, n_actions, alpha, gamma)  \nestimator.initialise_scaler_featuriser() \n\nPlay the game and do online updates for parameter \\(\\mathbf{w}\\),\n\ndef epsilon_greedy_policy(epsilon, actions, values):\n    if np.random.binomial(1, epsilon) == 1:\n        return np.random.choice(actions)\n    else:\n        return np.random.choice([action_ for action_, value_ in enumerate(values) if value_ == np.max(values)])\n\n\nepisodes = 50\nfor i in range(1, episodes + 1):\n    state = env.reset()[0]\n    a = env.action_space.sample()\n    step_count = 0\n    while True:\n        step_count +=1 \n        next_state, r, done, _, _, = env.step(a)  # check the openAI github repo\n\n        if done:\n            break\n\n        # compute q_sa\n        features = estimator.feature_transformation(state)\n        q_sa = estimator.action_value_estimator(features, a)\n\n        # compute all actions in the next state for optimal policy\n        next_feature = estimator.feature_transformation(next_state)\n        q_values = []\n        for j in actions:\n            q_values.append(estimator.action_value_estimator(next_feature, j))\n\n        next_a = epsilon_greedy_policy(epsilon, actions, q_values)\n        next_q_sa = estimator.action_value_estimator(next_feature, next_a)\n\n        # update weights for current action\n        estimator.update_w(r, q_sa, next_q_sa, features, a)\n\n        a = next_a\n        state = next_state\n    if i % 10 == 0:\n        plot_v(estimator, i)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResults\n\nplot_v(estimator)\n\n\n\n\n\nimport matplotlib.animation\nimport matplotlib\nfrom IPython.display import HTML\n\nrewards = 0\nenv.reset()\nframes = []\naction = env.action_space.sample()\nwhile True:\n    s, r, done, _, _, = env.step(action)\n    frames.append(env.render())\n    rewards = rewards + r\n    if done:\n        break\n\n    feature = estimator.feature_transformation(s)\n    q_values = []\n    for j in actions:\n        q_values.append(estimator.action_value_estimator(feature, j))\n\n    action = epsilon_greedy_policy(0, actions, q_values)\nprint(rewards)\n\nplt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\npatch = plt.imshow(frames[0])\nplt.axis('off')\nanimate = lambda i: patch.set_data(frames[i])\nani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\nHTML(ani.to_jshtml())\n\n-156.0\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "Documents_files/ST455/Week5/Seminar5/Seminar5.html#precomputed-graphs",
    "href": "Documents_files/ST455/Week5/Seminar5/Seminar5.html#precomputed-graphs",
    "title": "Seminar 5: Generalization and Function Approximation",
    "section": "Precomputed graphs",
    "text": "Precomputed graphs\nThe first plot is after 4 episodes, whilst the second one is after 100 episodes\n \nAs we will see, the optimal policy after 4 episodes of training is not good enough to solve the task.\n\nVideo('./graphs/4episodes.mp4')\n\n\n      Your browser does not support the video element.\n    \n\n\nPolicy after 100 episodes\n\nVideo('./graphs/100episodes.mp4')\n\n\n      Your browser does not support the video element.\n    \n\n\nTo replicate the results in the Sutton & Barto book, we observe that the action-values indeed display a spiral shape.\nThe left plot is after 1000 episodes, whilst the right one is after 9000 episodes,"
  },
  {
    "objectID": "Documents_files/ST455/Week4/Seminar4/Seminar4.html",
    "href": "Documents_files/ST455/Week4/Seminar4/Seminar4.html",
    "title": "Seminar 4: Temporal-Difference (TD) Learning",
    "section": "",
    "text": "In this seminar, we will implement and evaluate elementary solution methods for reinforcement learning, including methods based on Monte Carlo (MC) estimation and time-differencing (TD) methods. Specifically, we will consider examples in which we use the following solution methods:\nThe learning methods considered are based on backups which for dynamic programming, Monte Carlo, and temporal-difference learning can be represented as follows:"
  },
  {
    "objectID": "Documents_files/ST455/Week4/Seminar4/Seminar4.html#b.-the-windy-gridworld-problem",
    "href": "Documents_files/ST455/Week4/Seminar4/Seminar4.html#b.-the-windy-gridworld-problem",
    "title": "Seminar 4: Temporal-Difference (TD) Learning",
    "section": "B. The Windy GridWorld problem",
    "text": "B. The Windy GridWorld problem\nThe goal of this exercise is implement and evaluate SARSA algorithm for the Windy GridWorld problem which is defined as follows.\nThe Windy GridWorld problem is described in Example 6.5 in Sutton & Barto. \nThe Windy Gridworld problem is a standard gridworld with a crosswind upward through the middle of the grid. The strength of the wind is indicated by the number below each column. For example, if you are in the cell to the right of the goal, the action left will take you to the cell above the goal. To be precise, the next cell you will end up in depends on the action taken and the wind in the current cell. - An undiscounted episodic task - Rewards of -1 until the goal is reached\nSimilarly, define the variables and parameters as defined in the book,\n\nworld_height = 7\nworld_width = 10\nwind_strength = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n\n# actions\nup = 0\nleft = 1\nright = 2\ndown = 3\nactions = [up, left, right, down]\n\n\nThe environment\nA step function that returns next state, reward and is_done. Note that the next state is the current state moved towards the current action direction + the wind direction of the current state.\n\nclass WindyGridworld:\n    def __init__(self, init_position, goal_position):\n        self.initial_state = init_position\n        self.goal_state = goal_position\n        self.state = self.initial_state\n        self.reward = 0.0\n        self.is_terminal = False\n\n    # return (next state, reward, is_done)\n    def step(self, action):\n        i, j = self.state\n\n        if self.state == self.goal_state:\n            self.reward = 0.0\n            self.is_terminal = True\n        else:\n            if action == up:\n                self.state = [max(i - 1 - wind_strength[j], 0), j]\n            elif action == left:\n                # the next state (j-1) is the action + the wind in the previous state (j)\n                self.state = [max(i - wind_strength[j], 0), max(j - 1, 0)]\n            elif action == right:\n                self.state = [max(i - wind_strength[j], 0), min(j + 1, world_width - 1)]\n            elif action == down:\n                self.state = [max(min(i + 1 - wind_strength[j], world_height - 1), 0), j]\n            else:\n                assert False, \"Actions should be in the range of (0, 4).\"\n            self.reward = -1.0\n            self.is_terminal = False\n\n        return self.state, self.reward, self.is_terminal\n\n    def reset(self):\n        self.state = self.initial_state\n        self.reward = 0.0\n        self.is_terminal = False\n        return self.state\n\n\n\n\nThe policy\nImplement an \\(\\varepsilon\\)-greedy policy\n\ndef epsilon_greedy_policy(q_values, epsilon=0.1):\n    if np.random.binomial(1, epsilon) == 1:\n        return np.random.choice(actions)\n    else:\n        return np.random.choice([action_ for action_, value_ in enumerate(q_values) if value_ == np.max(q_values)])\n\n\n\nThe algorithm:\n\n\n\ndef sarsa(qsa, next_qsa, r, alpha=0.1, gamma=1.0):  \n    return qsa + alpha * (r + gamma * next_qsa - qsa)\n\nSolve the Windy GridWorld problem:\n\nq_sa = np.zeros((world_height, world_width, len(actions)))  \nepisodes = 200\ntimesteps = []  \n  \nstart_position = [3, 0]  \nterminal_position = [3, 7]  \n  \nenv = WindyGridworld(start_position, terminal_position)  \n  \nfor i in range(1, episodes + 1):  \n    state = env.reset() \n    is_done = False\n    row, col = state  \n    # initialise a  \n    a = epsilon_greedy_policy(q_sa[row, col, :])  \n    timesteps_per_epi = 1  \n  \n    while not is_done:  \n        next_state, r, is_done = env.step(a)  \n        row, col = state  \n        n_row, n_col = next_state  \n        \n        next_a = epsilon_greedy_policy(q_sa[n_row, n_col, :])  \n        q_sa[row, col, a] = sarsa(q_sa[row, col, a], q_sa[n_row, n_col, next_a], r)  \n  \n        state = next_state  \n        a = next_a  \n        timesteps_per_epi += 1  \n    timesteps.append(timesteps_per_epi)\n\n\ndef plot_episode_steps(steps, episodes):\n    plt.plot(np.arange(1, episodes + 1), steps)\n    plt.title(\"Episodes vs Time Steps\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Time Steps\")\n    plt.show()\n\nplot_episode_steps(timesteps, episodes)\n\n\n\n\nIn this problem, the only way to terminate an episode is to reach the goal state. The curve shows that the number of time steps requires drastically decreases.\nNote that the policy is unable to converge to the optimal policy after 170 episodes. Let the number of episodes be 1000. By the end of 1000 episodes, we choose a greedy policy\n\ndef print_optimal_policy(policy, terminal_position):\n    policy_display = np.empty_like(policy, dtype=str)\n    wind = np.empty_like(wind_strength, dtype=str)\n    for i in range(0, world_height):\n        for j in range(0, world_width):\n            wind[j] = str(wind_strength[j])\n            if [i, j] == terminal_position:\n                policy_display[i, j] = 'G'\n                continue\n            a = policy[i, j]\n            if a == up:\n                policy_display[i, j] = 'U'\n            elif a == left:\n                policy_display[i, j] = 'L'\n            elif a == right:\n                policy_display[i, j] = 'R'\n            elif a == down:\n                policy_display[i, j] = 'D'\n    print('Optimal policy is:')\n    for row in policy_display:\n        print(row)\n    print(wind)\n    \nepisodes = 1000\nfor i in range(1, episodes + 1):  \n    state = env.reset() \n    is_done = False\n    row, col = state  \n    # initialise a  \n    a = epsilon_greedy_policy(q_sa[row, col, :])  \n  \n    while not is_done:  \n        next_state, r, is_done = env.step(a)  \n        row, col = state  \n        n_row, n_col = next_state  \n        \n        next_a = epsilon_greedy_policy(q_sa[n_row, n_col, :])  \n        q_sa[row, col, a] = sarsa(q_sa[row, col, a], q_sa[n_row, n_col, next_a], r)  \n  \n        state = next_state  \n        a = next_a  \n\noptimal_policy = np.argmax(q_sa, axis=2)\nprint_optimal_policy(optimal_policy, terminal_position)\n\nOptimal policy is:\n['D' 'R' 'U' 'R' 'U' 'R' 'R' 'R' 'R' 'D']\n['D' 'U' 'R' 'U' 'R' 'R' 'R' 'R' 'U' 'D']\n['R' 'R' 'R' 'R' 'R' 'R' 'R' 'D' 'R' 'D']\n['R' 'R' 'R' 'R' 'R' 'R' 'R' 'G' 'R' 'D']\n['R' 'R' 'R' 'R' 'R' 'R' 'U' 'D' 'L' 'L']\n['D' 'R' 'R' 'D' 'R' 'U' 'U' 'R' 'L' 'D']\n['R' 'R' 'R' 'U' 'U' 'U' 'U' 'U' 'L' 'U']\n['0' '0' '0' '1' '1' '1' '2' '2' '1' '0']"
  },
  {
    "objectID": "Documents_files/ST455/Week4/Seminar4/HW4_solution.html",
    "href": "Documents_files/ST455/Week4/Seminar4/HW4_solution.html",
    "title": "HW4 Solution",
    "section": "",
    "text": "Consider a larger version of the Random Walk problem. Now we have 19 states instead of 5 and the left terminal state gives a reward of -1. The starting state is still the centre state.\nTask: - Implement the off-line \\(\\lambda\\)-return / TD(\\(\\lambda\\)) algorithm: - Compare the RMS(root mean square) error averaged over the 19 states between the predictions at the end of the episode for the 19 states and their true values, over the first 10 episodes, and over 10 runs (10 different sequence of walks). In other words, for each run, you generate 10 episodes and use TD(\\(\\lambda\\)) to learn the value. Then you calculate the difference between the learned value and the true value, square the difference, average it over 10 runs and take the square root.\n\nPlot the averaged RMS error vs \\(\\alpha\\) values for the following \\(\\lambda\\)-values:\n\nparameter settings:\nlambdas = [0.0, 0.4, 0.8, 0.9, 0.95, 0.975, 0.99, 1]\nalphas = [np.arange(0, 1.1, 0.1),\n          np.arange(0, 1.1, 0.1),\n          np.arange(0, 1.1, 0.1),\n          np.arange(0, 1.1, 0.1),\n          np.arange(0, 1.1, 0.1),\n          np.arange(0, 0.55, 0.05),\n          np.arange(0, 0.22, 0.02),\n          np.arange(0, 0.11, 0.01)]"
  },
  {
    "objectID": "Documents_files/ST455/Week4/Seminar4/HW4_solution.html#q1-the-tdlambda-algorithm-5",
    "href": "Documents_files/ST455/Week4/Seminar4/HW4_solution.html#q1-the-tdlambda-algorithm-5",
    "title": "HW4 Solution",
    "section": "",
    "text": "Consider a larger version of the Random Walk problem. Now we have 19 states instead of 5 and the left terminal state gives a reward of -1. The starting state is still the centre state.\nTask: - Implement the off-line \\(\\lambda\\)-return / TD(\\(\\lambda\\)) algorithm: - Compare the RMS(root mean square) error averaged over the 19 states between the predictions at the end of the episode for the 19 states and their true values, over the first 10 episodes, and over 10 runs (10 different sequence of walks). In other words, for each run, you generate 10 episodes and use TD(\\(\\lambda\\)) to learn the value. Then you calculate the difference between the learned value and the true value, square the difference, average it over 10 runs and take the square root.\n\nPlot the averaged RMS error vs \\(\\alpha\\) values for the following \\(\\lambda\\)-values:\n\nparameter settings:\nlambdas = [0.0, 0.4, 0.8, 0.9, 0.95, 0.975, 0.99, 1]\nalphas = [np.arange(0, 1.1, 0.1),\n          np.arange(0, 1.1, 0.1),\n          np.arange(0, 1.1, 0.1),\n          np.arange(0, 1.1, 0.1),\n          np.arange(0, 1.1, 0.1),\n          np.arange(0, 0.55, 0.05),\n          np.arange(0, 0.22, 0.02),\n          np.arange(0, 0.11, 0.01)]"
  },
  {
    "objectID": "Documents_files/ST455/Week4/Seminar4/HW4_solution.html#first-let-us-review-the-tdlambda-algorithm",
    "href": "Documents_files/ST455/Week4/Seminar4/HW4_solution.html#first-let-us-review-the-tdlambda-algorithm",
    "title": "HW4 Solution",
    "section": "First, let us review the TD(\\(\\lambda\\)) algorithm",
    "text": "First, let us review the TD(\\(\\lambda\\)) algorithm\n \n\n# Iterative policy evaluation\nimport numpy as np\ntrue_values = np.zeros(21)\nfor k in np.arange(1000):\n    true_values[1] = -0.5 + 0.5 * true_values[2]\n    for j in np.arange(2, 19):\n        true_values[j] = 0.5 * (true_values[j-1] + true_values[j+1])\n    true_values[19] = 0.5 + 0.5 * true_values[18]\n\nprint(true_values)\n\n[ 0.00000000e+00 -9.00000000e-01 -8.00000000e-01 -7.00000000e-01\n -6.00000000e-01 -5.00000000e-01 -4.00000000e-01 -3.00000000e-01\n -2.00000000e-01 -1.00000000e-01 -2.48141785e-12  1.00000000e-01\n  2.00000000e-01  3.00000000e-01  4.00000000e-01  5.00000000e-01\n  6.00000000e-01  7.00000000e-01  8.00000000e-01  9.00000000e-01\n  0.00000000e+00]\n\n\n\n# Pass the reward trajectory from time step t up to n'th step\nimport numpy as np\nimport matplotlib.pyplot as plt\nleft = 0\nright = 1\ndef random_policy():\n    return np.random.binomial(1, 0.5)\n\nclass RandomWalkWide:\n    def __init__(self, initial_state):\n        self.initial_state = initial_state\n        self.state = self.initial_state\n        self.reward = 0.0\n        self.is_terminal = False\n\n    # write step function that returns obs(next state), reward, is_done\n    def step(self, action):\n        if self.state == 19 and action == right:\n            self.state += 1\n            self.is_terminal = True\n            self.reward = 1.0\n        elif self.state == 1 and action == left:\n            self.state -= 1\n            self.is_terminal = True\n            self.reward = -1.0\n        else:\n            if action == left:\n                self.state -= 1\n                self.is_terminal = False\n                self.reward = 0.0\n\n            else:\n                self.state += 1\n                self.is_terminal = False\n                self.reward = 0.0\n\n        return self.state, self.reward, self.is_terminal\n\n    def reset(self):\n        self.state = self.initial_state\n        self.reward = 0.0\n        self.is_terminal = False\n        return self.state\n    \ndef n_step_reward(r, values, current_t, total_time, state_history, gam):\n    n_steps = len(r)\n    gamma_power = 1.0\n    r_n = 0.0\n\n    for n in range(n_steps):\n        r_n += gamma_power * r[n]\n        gamma_power *= gam\n\n    end_time = min(current_t + n_steps, total_time)\n    state = state_history[end_time]\n    r_n += gamma_power * values[state]\n\n    return r_n\n\ndef temporal_difference_lambda(values, state_history, r_trajectory, current_t, alpha=0.1, lam=1.0):\n    lambda_r = 0.0\n    lambda_power = 1.0\n    total_time = len(state_history)\n    total_n = total_time - current_t\n\n    for n in range(1, max(total_n, 1)):\n        lambda_r += lambda_power * n_step_reward(r_trajectory[current_t:current_t+n], values, current_t,\n                                                 total_time, state_history, 1.0)\n        lambda_power *= lam\n\n    lambda_r = (1.0 - lam) * lambda_r + lambda_power * np.sum(r_trajectory[current_t:total_time])\n    current_state = state_history[current_t]\n    return values[current_state] + alpha * (lambda_r - values[current_state])\n\n\ndef root_mean_square_error(values, true_values):\n    diff = values - true_values\n    mse = np.sum(diff ** 2)\n    return np.sqrt(mse)\n\n\ndef plot_rms_error_temporal_lambda(errors, alphas, lambdas):\n    for i in range(len(lambdas)):\n        plt.plot(alphas[i], errors[i, :], label=r'$\\lambda$ = ' + str(lambdas[i]))\n    plt.ylabel('RMS error averaged over states')\n    plt.xlabel(r'$\\alpha$')\n    plt.title(r'Off-line $\\lambda$-return')\n#    plt.ylim(0.25, 0.65)\n    plt.legend(bbox_to_anchor=(1, 0.5))\n    plt.show()\n\n\ntrue_values = np.arange(-20, 22, 2) / 20.0\ntrue_values[0] = true_values[20] = 0.0\n\ninitial_state = 10\nenv = RandomWalkWide(initial_state)\n\nepisodes = 10\nn_runs = 10\n\nlambdas = [0.0, 0.4, 0.8, 0.9, 0.95, 0.975, 0.99, 1]\nalphas = [np.arange(0, 1.1, 0.1),\n          np.arange(0, 1.1, 0.1),\n          np.arange(0, 1.1, 0.1),\n          np.arange(0, 1.1, 0.1),\n          np.arange(0, 1.1, 0.1),\n          np.arange(0, 0.55, 0.05),\n          np.arange(0, 0.22, 0.02),\n          np.arange(0, 0.11, 0.01)]\n\n# arrays to store rms for different values of alphas\nrms = np.empty((n_runs, len(lambdas), len(alphas[0])))\n\nfor n in range(n_runs):\n    print(\"Number of current run:\", n + 1)\n    for lambda_idx, lam in zip(range(len(lambdas)), lambdas):\n        for alpha_idx, alpha in zip(range(len(alphas[lambda_idx])), alphas[lambda_idx]):\n            values = np.zeros(21)\n            error = 0.0\n            for i in range(1, episodes + 1):\n                state = env.reset()\n                state_history = [state]\n                done = False\n                reward_trajectory = []\n                \n                while not done:\n                    action = random_policy()\n                    next_state, r, done = env.step(action)\n                    reward_trajectory.append(r)\n                    state_history.append(next_state)\n\n                for t, state in zip(range(len(state_history)), state_history):\n                    values[state] = temporal_difference_lambda(values, state_history, reward_trajectory, t,\n                                                               alpha=alpha, lam=lam)\n\n                error += np.square(root_mean_square_error(values[1:20], true_values[1:20]))\n\n            rms[n, lambda_idx, alpha_idx] = error / episodes\n\nrms = np.sqrt(np.mean(rms, axis=0))\n\n\nNumber of current run: 1\nNumber of current run: 2\nNumber of current run: 3\nNumber of current run: 4\nNumber of current run: 5\nNumber of current run: 6\nNumber of current run: 7\nNumber of current run: 8\nNumber of current run: 9\nNumber of current run: 10\n\n\n\nplot_rms_error_temporal_lambda(rms, alphas, lambdas)"
  },
  {
    "objectID": "Documents_files/ST455/Week4/Seminar4/HW4_solution.html#q2-implement-the-double-q-learning-algorithm-on-the-cliff-walking-example-5",
    "href": "Documents_files/ST455/Week4/Seminar4/HW4_solution.html#q2-implement-the-double-q-learning-algorithm-on-the-cliff-walking-example-5",
    "title": "HW4 Solution",
    "section": "Q2: Implement the double Q-learning algorithm on the Cliff Walking example (5%)",
    "text": "Q2: Implement the double Q-learning algorithm on the Cliff Walking example (5%)\n\nTask:\n\nSet the number of episodes to 1000\nOutput the estimated optimal policy\nPlot the average reward during episodes (see the plot in Seminar 4)"
  },
  {
    "objectID": "Documents_files/ST455/Week4/Seminar4/HW4_solution.html#first-let-us-review-the-double-q-learning-algorithm",
    "href": "Documents_files/ST455/Week4/Seminar4/HW4_solution.html#first-let-us-review-the-double-q-learning-algorithm",
    "title": "HW4 Solution",
    "section": "First, let us review the double Q-learning algorithm",
    "text": "First, let us review the double Q-learning algorithm"
  },
  {
    "objectID": "Documents_files/ST455/Week4/Seminar4/HW4_solution.html#second-let-us-implement-the-cliff-walking-environment",
    "href": "Documents_files/ST455/Week4/Seminar4/HW4_solution.html#second-let-us-implement-the-cliff-walking-environment",
    "title": "HW4 Solution",
    "section": "Second, let us implement the Cliff Walking environment",
    "text": "Second, let us implement the Cliff Walking environment\n\nworld_width = 12\nworld_height = 4 \n\n# actions\nup = 0\nleft = 1\nright = 2\ndown = 3\nactions = [up, left, right, down]\n\nclass CliffWalking:\n    def __init__(self, initial_state, goal_state):\n        self.initial_state = initial_state\n        self.goal_state = goal_state\n        self.state = self.initial_state\n        self.reward = 0.0\n        self.is_terminal = False\n\n    def is_cliff(self):\n        cliff = np.zeros((world_height, world_width), dtype=bool)\n        cliff[3, 1: -1] = True\n        return cliff[tuple(self.state)]\n\n    # return next_state, reward, done\n    def step(self, action):\n        i, j = self.state\n\n        if action == up:\n            self.state = [max(i - 1, 0), j]\n        elif action == left:\n            self.state = [i, max(j - 1, 0)]\n        elif action == right:\n            self.state = [i, min(j + 1, world_width - 1)]\n        elif action == down:\n            self.state = [min(i + 1, world_height - 1), j]\n        else:\n            assert False, \"Actions should be in the range of (0, 4)\"\n\n        if self.is_cliff():\n            self.state = self.initial_state\n            self.reward = -100.0\n            self.is_terminal = True\n        elif self.state == self.goal_state:\n            self.state = self.state\n            self.reward = 0.0\n            self.is_terminal = True\n        else:\n            self.reward = -1.0\n            self.is_terminal = False\n        return self.state, self.reward, self.is_terminal\n\n    def reset(self):\n        self.state = self.initial_state\n        self.reward = 0.0\n        self.is_terminal = False\n        return self.state"
  },
  {
    "objectID": "Documents_files/ST455/Week4/Seminar4/HW4_solution.html#finally-let-us-implement-the-double-q-learning-algorithm",
    "href": "Documents_files/ST455/Week4/Seminar4/HW4_solution.html#finally-let-us-implement-the-double-q-learning-algorithm",
    "title": "HW4 Solution",
    "section": "Finally, let us implement the double Q-learning algorithm",
    "text": "Finally, let us implement the double Q-learning algorithm\n\nstart_position = [3, 0]  \ngoal = [3, 11]\n\ndef eps_greedy_policy(qsa, epsilon=0.1):\n    if np.random.binomial(1, epsilon) == 1:\n        return np.random.choice(actions)\n    else:\n        return np.random.choice([action_ for action_, value_ in enumerate(qsa) if value_ == np.max(qsa)])\n\n\ndef double_q_learning(q1, q2, state, action, reward, next_state, alpha=0.1, gamma=1.0):\n    state_row, state_col = state\n    next_state_row, next_state_col = next_state\n    update_q1 = np.random.choice([True, False])\n    if update_q1:\n        a1 = eps_greedy_policy(q1[next_state_row, next_state_col,:], epsilon=0)\n        q1[state_row, state_col, action] += alpha * (reward + gamma * q2[next_state_row, next_state_col,a1] - q1[state_row, state_col, action])\n    else:\n        a2 = eps_greedy_policy(q2[next_state_row, next_state_col,:], epsilon=0)\n        q2[state_row, state_col, action] += alpha * (reward + gamma * q1[next_state_row, next_state_col,a2] - q2[state_row, state_col, action])\n    return q1,q2\n\n\nruns = 10\nepisodes = 1000\nrewards_double_qlearning = np.zeros(episodes)\nenv_double_q_learning = CliffWalking(start_position, goal)\n\nfor r in range(runs):\n    print(\"Number of current run:\", r + 1)\n    q1_double_qlearning = np.zeros((world_height, world_width, len(actions)))\n    q2_double_qlearning = np.zeros((world_height, world_width, len(actions)))\n\n    for i in range(episodes):\n        state_double_q = env_double_q_learning.reset()\n        done_double_q = False\n        g_double_q = 0.0\n        \n        while not done_double_q:\n            row_double_q, col_double_q = state_double_q\n            a_double_q = eps_greedy_policy(q1_double_qlearning[row_double_q, col_double_q, :] + q2_double_qlearning[row_double_q, col_double_q, :])\n            next_state_double_q, r_double_q, done_double_q = env_double_q_learning.step(a_double_q)\n            g_double_q += r_double_q\n\n            # Double-Q-learning updates\n            q1_double_qlearning, q2_double_qlearning = double_q_learning(q1_double_qlearning, q2_double_qlearning, state_double_q, a_double_q, r_double_q, next_state_double_q)\n            state_double_q = next_state_double_q\n\n        rewards_double_qlearning[i] += g_double_q\n\nNumber of current run: 1\nNumber of current run: 2\nNumber of current run: 3\nNumber of current run: 4\nNumber of current run: 5\nNumber of current run: 6\nNumber of current run: 7\nNumber of current run: 8\nNumber of current run: 9\nNumber of current run: 10\n\n\n\ndef plot_rewards(plots):\n    plt.figure()\n    for plot in plots:\n        method, method_title = plot\n        plt.plot(method, label=method_title)\n    # plt.plot(r_qlearning, label='Q-learning')\n    plt.xlabel('Episodes')\n    plt.ylabel('Sum of rewards during episodes')\n    plt.ylim([-100, 0])\n    plt.legend()\n    plt.show()\n\nrewards_double_qlearning /= runs \n\nplot_rewards([[rewards_double_qlearning, 'Double-Q-learning']])\n\n\n\n\n\ndef print_optimal_policy(policy, method):\n    policy_display = np.empty_like(policy, dtype=str)\n    for i in range(0, world_height):\n        for j in range(0, world_width):\n            if [i, j] == [3, 11]:\n                policy_display[i, j] = 'G'\n                continue\n            if i == 3 and j in np.arange(1,11): \n                policy_display[i, j] = 'C'\n                continue\n            a = policy[i, j]\n            if a == up:\n                policy_display[i, j] = 'U'\n            elif a == left:\n                policy_display[i, j] = 'L'\n            elif a == right:\n                policy_display[i, j] = 'R'\n            elif a == down:\n                policy_display[i, j] = 'D'\n    print(method + ' Optimal policy is:')\n    for row in policy_display:\n        print(row)\n        \noptimal_policy = np.argmax(q1_double_qlearning + q2_double_qlearning, axis=2)\nprint_optimal_policy(optimal_policy, 'Double Q-Learning')\n\nDouble Q-Learning Optimal policy is:\n['R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'D']\n['U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'R' 'R' 'D']\n['U' 'U' 'U' 'U' 'R' 'R' 'U' 'U' 'R' 'R' 'R' 'D']\n['U' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'G']"
  },
  {
    "objectID": "Documents_files/ST455/Week3/Seminar3/Seminar3.html",
    "href": "Documents_files/ST455/Week3/Seminar3/Seminar3.html",
    "title": "Seminar 3: Dynamic Programming and Monte Carlo methods",
    "section": "",
    "text": "The problem is described in the lecture and in Example 4.1 in Sutton & Barto.\n\n\n\nalt text\n\n\n\nThis is a undiscounted episodic task\nAction set for state s: A(s) = {up, down, left, right}\nActions that would take the agent off the grid leave the state unchanged\nReward of value -1 for each transition\n\nIterative update rule: Iterative update for the value function whose limit is the solution of the Bellman equation for the value function under a given policy \\(\\pi\\) is given by\n\\[\n\\begin{align*}V_{k+1}(s)&=\\mathbb{E}_{\\pi}[r_{t+1}+\\gamma{V_k(s_{t+1})|s_t=s]}\\\\&=\\sum_a\\pi(s,a)\\sum_{s'}P_{s,s'}^a[R_{s,s'}^a+\\gamma{V_k(s')}]\\end{align*}\n\\] for \\(s \\in S\\).\n\n\n\nAssume that the initial policy is uniform, we then implement the iterative policy evaluation algorithm using this “look-up table”:\n\nAs we know, algorithms that fall into the category of Dynamic Programming assume that the agent has a prefect knowledge of the environment, i.e., the transition probabilities and expected rewards are known. In order to compute the state values, we first need to learn the environment.\n\n\n\nFirst, enumerate the actions and define the number of states, actions, rows and columns.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nup = 0\nright = 1\nleft = 2\ndown = 3\n\nn_states = 16\nn_actions = 4\nmax_row = 4\nmax_col = 4\n\nThe following code is one of the ways to build the GridWorld environment. The output corresponds to the transition matrix. For more information about nditer, please refer to https://numpy.org/doc/stable/reference/arrays.nditer.html#arrays-nditer.\n\ndef grid_world():\n    p = {}\n    grid = np.arange(n_states).reshape([max_row, max_col])\n    it = np.nditer(grid, flags=['multi_index'])\n\n    with it:\n        while not it.finished:\n            s = it.iterindex\n            row, col = it.multi_index\n\n            p[s] = {a: [] for a in range(n_actions)}\n\n            is_done = lambda x: x == 0 or x == (n_states - 1)\n            reward = 0.0 if is_done(s) else -1.0\n\n            if is_done(s):\n                # 3 variables: next state, reward, done\n                p[s][up] = [(s, reward, True)]\n                p[s][right] = [(s, reward, True)]\n                p[s][left] = [(s, reward, True)]\n                p[s][down] = [(s, reward, True)]\n\n            else:\n                s_up = s if row == 0 else s - max_row\n                s_right = s if col == (max_col - 1) else s + 1\n                s_left = s if col == 0 else s - 1\n                s_down = s if row == (max_row - 1) else s + max_row\n\n                p[s][up] = [(s_up, reward, is_done(s_up))]\n                p[s][right] = [(s_right, reward, is_done(s_right))]\n                p[s][left] = [(s_left, reward, is_done(s_left))]\n                p[s][down] = [(s_down, reward, is_done(s_down))]\n\n            it.iternext()\n    return p\n\n\ngrid = np.arange(n_states).reshape([max_row, max_col])\nprint(grid) \nit = np.nditer(grid, flags=['multi_index'])\nfor x in it:\n    print(\"%d &lt;%s&gt;\" % (x, it.multi_index), end=',')\n\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]]\n0 &lt;(0, 0)&gt;,1 &lt;(0, 1)&gt;,2 &lt;(0, 2)&gt;,3 &lt;(0, 3)&gt;,4 &lt;(1, 0)&gt;,5 &lt;(1, 1)&gt;,6 &lt;(1, 2)&gt;,7 &lt;(1, 3)&gt;,8 &lt;(2, 0)&gt;,9 &lt;(2, 1)&gt;,10 &lt;(2, 2)&gt;,11 &lt;(2, 3)&gt;,12 &lt;(3, 0)&gt;,13 &lt;(3, 1)&gt;,14 &lt;(3, 2)&gt;,15 &lt;(3, 3)&gt;,\n\n\n\ngrid_world()\n\n{0: {0: [(0, 0.0, True)],\n  1: [(0, 0.0, True)],\n  2: [(0, 0.0, True)],\n  3: [(0, 0.0, True)]},\n 1: {0: [(1, -1.0, False)],\n  1: [(2, -1.0, False)],\n  2: [(0, -1.0, True)],\n  3: [(5, -1.0, False)]},\n 2: {0: [(2, -1.0, False)],\n  1: [(3, -1.0, False)],\n  2: [(1, -1.0, False)],\n  3: [(6, -1.0, False)]},\n 3: {0: [(3, -1.0, False)],\n  1: [(3, -1.0, False)],\n  2: [(2, -1.0, False)],\n  3: [(7, -1.0, False)]},\n 4: {0: [(0, -1.0, True)],\n  1: [(5, -1.0, False)],\n  2: [(4, -1.0, False)],\n  3: [(8, -1.0, False)]},\n 5: {0: [(1, -1.0, False)],\n  1: [(6, -1.0, False)],\n  2: [(4, -1.0, False)],\n  3: [(9, -1.0, False)]},\n 6: {0: [(2, -1.0, False)],\n  1: [(7, -1.0, False)],\n  2: [(5, -1.0, False)],\n  3: [(10, -1.0, False)]},\n 7: {0: [(3, -1.0, False)],\n  1: [(7, -1.0, False)],\n  2: [(6, -1.0, False)],\n  3: [(11, -1.0, False)]},\n 8: {0: [(4, -1.0, False)],\n  1: [(9, -1.0, False)],\n  2: [(8, -1.0, False)],\n  3: [(12, -1.0, False)]},\n 9: {0: [(5, -1.0, False)],\n  1: [(10, -1.0, False)],\n  2: [(8, -1.0, False)],\n  3: [(13, -1.0, False)]},\n 10: {0: [(6, -1.0, False)],\n  1: [(11, -1.0, False)],\n  2: [(9, -1.0, False)],\n  3: [(14, -1.0, False)]},\n 11: {0: [(7, -1.0, False)],\n  1: [(11, -1.0, False)],\n  2: [(10, -1.0, False)],\n  3: [(15, -1.0, True)]},\n 12: {0: [(8, -1.0, False)],\n  1: [(13, -1.0, False)],\n  2: [(12, -1.0, False)],\n  3: [(12, -1.0, False)]},\n 13: {0: [(9, -1.0, False)],\n  1: [(14, -1.0, False)],\n  2: [(12, -1.0, False)],\n  3: [(13, -1.0, False)]},\n 14: {0: [(10, -1.0, False)],\n  1: [(15, -1.0, True)],\n  2: [(13, -1.0, False)],\n  3: [(14, -1.0, False)]},\n 15: {0: [(15, 0.0, True)],\n  1: [(15, 0.0, True)],\n  2: [(15, 0.0, True)],\n  3: [(15, 0.0, True)]}}\n\n\nIn the end, the transition matrix is returned after the function is called. The transition matrix is essentially a look-up table:\n\n# Initial state values - 0s\nstate_values = np.zeros(16)\n# Assume a uniform policy\npi_a = 0.25\nlam = 1.0\ntheta = 1e-10\niteration_counter = 1\n\ntransition_probs = grid_world()\n\nwhile True:\n    v_old = np.copy(state_values)\n    delta = 0.0\n    for s in range(n_states):\n        v_s = 0.0\n\n        for a in range(n_actions):\n            current_entry = transition_probs[s][a][0]\n            next_s = current_entry[0]\n            reward = current_entry[1]\n            v_s += pi_a * (reward + lam * v_old[next_s])\n\n        state_values[s] = v_s\n        delta = np.maximum(delta, np.abs(state_values[s] - v_old[s]))\n    if iteration_counter % 40 == 0:\n        pass\n        #print(np.round(delta*100000,decimals=5))\n    iteration_counter += 1\n    if delta &lt; theta:\n        print(iteration_counter)\n        break\n        \n\n427"
  },
  {
    "objectID": "Documents_files/ST455/Week3/Seminar3/Seminar3.html#iterative-policy-evaluation",
    "href": "Documents_files/ST455/Week3/Seminar3/Seminar3.html#iterative-policy-evaluation",
    "title": "Seminar 3: Dynamic Programming and Monte Carlo methods",
    "section": "",
    "text": "The problem is described in the lecture and in Example 4.1 in Sutton & Barto.\n\n\n\nalt text\n\n\n\nThis is a undiscounted episodic task\nAction set for state s: A(s) = {up, down, left, right}\nActions that would take the agent off the grid leave the state unchanged\nReward of value -1 for each transition\n\nIterative update rule: Iterative update for the value function whose limit is the solution of the Bellman equation for the value function under a given policy \\(\\pi\\) is given by\n\\[\n\\begin{align*}V_{k+1}(s)&=\\mathbb{E}_{\\pi}[r_{t+1}+\\gamma{V_k(s_{t+1})|s_t=s]}\\\\&=\\sum_a\\pi(s,a)\\sum_{s'}P_{s,s'}^a[R_{s,s'}^a+\\gamma{V_k(s')}]\\end{align*}\n\\] for \\(s \\in S\\).\n\n\n\nAssume that the initial policy is uniform, we then implement the iterative policy evaluation algorithm using this “look-up table”:\n\nAs we know, algorithms that fall into the category of Dynamic Programming assume that the agent has a prefect knowledge of the environment, i.e., the transition probabilities and expected rewards are known. In order to compute the state values, we first need to learn the environment.\n\n\n\nFirst, enumerate the actions and define the number of states, actions, rows and columns.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nup = 0\nright = 1\nleft = 2\ndown = 3\n\nn_states = 16\nn_actions = 4\nmax_row = 4\nmax_col = 4\n\nThe following code is one of the ways to build the GridWorld environment. The output corresponds to the transition matrix. For more information about nditer, please refer to https://numpy.org/doc/stable/reference/arrays.nditer.html#arrays-nditer.\n\ndef grid_world():\n    p = {}\n    grid = np.arange(n_states).reshape([max_row, max_col])\n    it = np.nditer(grid, flags=['multi_index'])\n\n    with it:\n        while not it.finished:\n            s = it.iterindex\n            row, col = it.multi_index\n\n            p[s] = {a: [] for a in range(n_actions)}\n\n            is_done = lambda x: x == 0 or x == (n_states - 1)\n            reward = 0.0 if is_done(s) else -1.0\n\n            if is_done(s):\n                # 3 variables: next state, reward, done\n                p[s][up] = [(s, reward, True)]\n                p[s][right] = [(s, reward, True)]\n                p[s][left] = [(s, reward, True)]\n                p[s][down] = [(s, reward, True)]\n\n            else:\n                s_up = s if row == 0 else s - max_row\n                s_right = s if col == (max_col - 1) else s + 1\n                s_left = s if col == 0 else s - 1\n                s_down = s if row == (max_row - 1) else s + max_row\n\n                p[s][up] = [(s_up, reward, is_done(s_up))]\n                p[s][right] = [(s_right, reward, is_done(s_right))]\n                p[s][left] = [(s_left, reward, is_done(s_left))]\n                p[s][down] = [(s_down, reward, is_done(s_down))]\n\n            it.iternext()\n    return p\n\n\ngrid = np.arange(n_states).reshape([max_row, max_col])\nprint(grid) \nit = np.nditer(grid, flags=['multi_index'])\nfor x in it:\n    print(\"%d &lt;%s&gt;\" % (x, it.multi_index), end=',')\n\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]]\n0 &lt;(0, 0)&gt;,1 &lt;(0, 1)&gt;,2 &lt;(0, 2)&gt;,3 &lt;(0, 3)&gt;,4 &lt;(1, 0)&gt;,5 &lt;(1, 1)&gt;,6 &lt;(1, 2)&gt;,7 &lt;(1, 3)&gt;,8 &lt;(2, 0)&gt;,9 &lt;(2, 1)&gt;,10 &lt;(2, 2)&gt;,11 &lt;(2, 3)&gt;,12 &lt;(3, 0)&gt;,13 &lt;(3, 1)&gt;,14 &lt;(3, 2)&gt;,15 &lt;(3, 3)&gt;,\n\n\n\ngrid_world()\n\n{0: {0: [(0, 0.0, True)],\n  1: [(0, 0.0, True)],\n  2: [(0, 0.0, True)],\n  3: [(0, 0.0, True)]},\n 1: {0: [(1, -1.0, False)],\n  1: [(2, -1.0, False)],\n  2: [(0, -1.0, True)],\n  3: [(5, -1.0, False)]},\n 2: {0: [(2, -1.0, False)],\n  1: [(3, -1.0, False)],\n  2: [(1, -1.0, False)],\n  3: [(6, -1.0, False)]},\n 3: {0: [(3, -1.0, False)],\n  1: [(3, -1.0, False)],\n  2: [(2, -1.0, False)],\n  3: [(7, -1.0, False)]},\n 4: {0: [(0, -1.0, True)],\n  1: [(5, -1.0, False)],\n  2: [(4, -1.0, False)],\n  3: [(8, -1.0, False)]},\n 5: {0: [(1, -1.0, False)],\n  1: [(6, -1.0, False)],\n  2: [(4, -1.0, False)],\n  3: [(9, -1.0, False)]},\n 6: {0: [(2, -1.0, False)],\n  1: [(7, -1.0, False)],\n  2: [(5, -1.0, False)],\n  3: [(10, -1.0, False)]},\n 7: {0: [(3, -1.0, False)],\n  1: [(7, -1.0, False)],\n  2: [(6, -1.0, False)],\n  3: [(11, -1.0, False)]},\n 8: {0: [(4, -1.0, False)],\n  1: [(9, -1.0, False)],\n  2: [(8, -1.0, False)],\n  3: [(12, -1.0, False)]},\n 9: {0: [(5, -1.0, False)],\n  1: [(10, -1.0, False)],\n  2: [(8, -1.0, False)],\n  3: [(13, -1.0, False)]},\n 10: {0: [(6, -1.0, False)],\n  1: [(11, -1.0, False)],\n  2: [(9, -1.0, False)],\n  3: [(14, -1.0, False)]},\n 11: {0: [(7, -1.0, False)],\n  1: [(11, -1.0, False)],\n  2: [(10, -1.0, False)],\n  3: [(15, -1.0, True)]},\n 12: {0: [(8, -1.0, False)],\n  1: [(13, -1.0, False)],\n  2: [(12, -1.0, False)],\n  3: [(12, -1.0, False)]},\n 13: {0: [(9, -1.0, False)],\n  1: [(14, -1.0, False)],\n  2: [(12, -1.0, False)],\n  3: [(13, -1.0, False)]},\n 14: {0: [(10, -1.0, False)],\n  1: [(15, -1.0, True)],\n  2: [(13, -1.0, False)],\n  3: [(14, -1.0, False)]},\n 15: {0: [(15, 0.0, True)],\n  1: [(15, 0.0, True)],\n  2: [(15, 0.0, True)],\n  3: [(15, 0.0, True)]}}\n\n\nIn the end, the transition matrix is returned after the function is called. The transition matrix is essentially a look-up table:\n\n# Initial state values - 0s\nstate_values = np.zeros(16)\n# Assume a uniform policy\npi_a = 0.25\nlam = 1.0\ntheta = 1e-10\niteration_counter = 1\n\ntransition_probs = grid_world()\n\nwhile True:\n    v_old = np.copy(state_values)\n    delta = 0.0\n    for s in range(n_states):\n        v_s = 0.0\n\n        for a in range(n_actions):\n            current_entry = transition_probs[s][a][0]\n            next_s = current_entry[0]\n            reward = current_entry[1]\n            v_s += pi_a * (reward + lam * v_old[next_s])\n\n        state_values[s] = v_s\n        delta = np.maximum(delta, np.abs(state_values[s] - v_old[s]))\n    if iteration_counter % 40 == 0:\n        pass\n        #print(np.round(delta*100000,decimals=5))\n    iteration_counter += 1\n    if delta &lt; theta:\n        print(iteration_counter)\n        break\n        \n\n427"
  },
  {
    "objectID": "Documents_files/ST455/Week3/Seminar3/Seminar3.html#value-iteration",
    "href": "Documents_files/ST455/Week3/Seminar3/Seminar3.html#value-iteration",
    "title": "Seminar 3: Dynamic Programming and Monte Carlo methods",
    "section": "Value Iteration",
    "text": "Value Iteration\n\nGambler’s problem\nThis problem is described in the lecture and in Example 4.3 in Sutton & Barto. - A gambler makes bets on the outcomes of a sequence of coin flips - The gambler must decide for each coin flip what portion of his capital to stake - If outcome of the coin flip = heads then: - the gambler wins as much money as he has staked on this flip - else: - The gambler loses his stake - The game ends when the gambler reaches his goal of $100 or loses all the money - Formulated as an undiscounted, episodic, finite MDP problem - Pr[outcome of coin flip is heads] =p (known parameter)\nStates: There are 101 states, \\(s\\in\\{0,1,2,...,100\\}\\).\nActions: The actions are stakes, \\(a\\in\\{1,2,...,\\min(s,100-s)\\}\\). We also assume the probability of the coin coming up heads is 0.4.\nRewards: 1 when we achieves the goal and 0 otherwise\n\ngoal = 100\nstates = np.arange(goal + 1)\n\n# assume the head probability is 0.4\np_head = 0.4\n\nBelow are the functions to visualise the optimal state value function and the optimal policy\n\ndef plot_results(state_values, policy):\n    plt.figure(figsize=(10, 20))\n\n    plt.subplot(2, 1, 1)\n    plt.plot(state_values, linewidth=3)\n    plt.xlabel('Capital')\n    plt.ylabel('Value estimates')\n    plt.title('State values computed by iterative value evaluation')\n\n    plt.subplot(2, 1, 2)\n    plt.bar(states, policy, linewidth=3)\n    plt.xlabel('Capital')\n    plt.ylabel('Final policy (stake)')\n    plt.title('Final optimal policy')\n\n    plt.show()\n\nWe next implement the value iteration algorithm\n\n\n# initialise state values\nstate_values = np.zeros(goal + 1)\nstate_values[goal] = 1.0\npolicy = np.zeros(goal + 1)\ntheta = 1e-8\n\n# Value evaluation\nwhile True:\n    delta = 0.0\n\n    for s in states[1: goal]:\n        actions = np.arange(1, min(s, goal - s) + 1)\n\n        actions_returns = []\n\n        for a in actions:\n            actions_returns.append(p_head * state_values[s + a] + (1. - p_head) * state_values[s - a])\n\n        new_value = np.max(actions_returns)\n\n        delta = np.maximum(delta, np.abs(state_values[s] - new_value))\n\n        state_values[s] = new_value\n\n    if delta &lt; theta:\n        break\n\nOnce the state values have converged, the algorithm then chooses a greedy policy that takes the action offering maximum return in the policy improvement step.\n\n# Policy improvement\nfor s in states[1: goal]:\n    actions = np.arange(1, min(s, goal - s) + 1)\n\n    actions_returns = []\n\n    for a in actions:\n        actions_returns.append(p_head * state_values[s + a] + (1. - p_head) * state_values[s - a])\n\n    # round to resemble the figure in the book\n    policy[s] = actions[np.argmax(np.round(actions_returns[0: goal], 5))]\n\n\nplot_results(state_values, policy)"
  },
  {
    "objectID": "Documents_files/ST455/Week3/Seminar3/Seminar3.html#monte-carlo-methods",
    "href": "Documents_files/ST455/Week3/Seminar3/Seminar3.html#monte-carlo-methods",
    "title": "Seminar 3: Dynamic Programming and Monte Carlo methods",
    "section": "Monte Carlo methods",
    "text": "Monte Carlo methods\n\nMonte Carlo prediction\nIn Monte Carlo methods, the value function of a state is estimated by the average of the returns following each visit or each first visit to that state in a set of episodes.\n\nIncremental updates\nIncremental mean: \\[\\mu_k=\\frac{1}{k}\\sum_{j=1}^kx_j=\\mu_{k-1}+\\frac{1}{k}(x_k-\\mu_{k-1})\\]\nIncremental MC updates: \\[v^j(s_t)=v^{j-1}(s_t)+\\frac{1}{N(s_t)}(G^j_t-v^{j-1}(s_t))\\]\nwhere \\(G^j_t\\) is the total return at j’th episode and \\(N(s_t)\\) is the number of times state \\(s_t\\) was visited - the every-visit MC method.\n\n\n\nExample: Blackjack\nBlackjack is a card game. The goal is to obtain cards the sum of whose numerical values is as large as possible without exceeding 21. The face cards (JQK) count as 10 and the ace can count either as 1 or as 11. Consider the version in which each player competes independently against the dealer. The game begins with 2 cards dealt to both dealer and player. One of the dealer’s card is shown and the other is hidden. If the player has 21 immediately, it is called a natural. She then wins unless the dealer also has a natural. In that case, the game is a draw. If the player doesn’t start with a natural, she can choose to either ‘hit’ or ‘stick’. If the cards in hand exceeds 21, she loses. If she hits, she receives another card. If she sticks, it becomes the dealer’s turn.\nThis environment has been built in Gym, which is defined as following: - Each game is an episode - The cards are drawn from \\(\\mathrm{deck}\\in\\{1,2,3,4,5,6,7,8,9,10,10,10,10\\}\\) with replacement - The two actions are \\(a\\in\\{hit=1,stick=0\\}\\) - The rewards for win, draw and lose are \\(r\\in\\{+1,0,-1\\}\\) - The observation space has 3 variables: the players current sum, the dealer’s one showing card (1-10 where 1 is ace), and whether or not the player holds a usable ace (0 or 1).\nWe first consider a simple policy which chooses to hit when the sum of the player’s cards in hand is equal to or less than 20 otherwise stick and evaluate it using the every-visit MC method.\n\ndef policy(hand_sum):\n    if hand_sum &gt; 20:\n        return 0\n    else:\n        return 1\n    \ndef mc_policy_evaluation(state_count, r, value):\n    return value + (r - value) / state_count\n\nThe player makes decisions on the basis of the three variables in the observation space: current sum (12-21), the dealer’s one showing card (1-10) and whether or not she has a usable ace. Here, we focus on the setting where the current sum is at least 12. If it is smaller than 12, hit would be the optimal action. This makes for a total of 200 states. We therefore create two matrices to store the states for the two situations - with ace and without ace. The row in each matrix indicates the player’s cards and the column indicates the dealer’s card.\nImport the environment and create arrays to store the state values and the number of appearances of each states,\n\nimport gym\nfrom mpl_toolkits.mplot3d import Axes3D\nnp.random.seed(123)\n\nenv = gym.make('Blackjack-v1', natural=False, sab=False)\nvalues_usable_ace = np.zeros((10, 10))\nvalues_no_usable_ace = np.zeros_like(values_usable_ace)\nstate_count_ace = np.zeros_like(values_usable_ace)\nstate_count_no_ace = np.zeros_like(state_count_ace)\nepisodes = 10000\n\nPlay the game for a number of episodes, count the state visits and record the observations and rewards:\n\nfor e in range(episodes):\n        done = False\n        obs = env.reset()\n        state_history = []\n        g = []\n        \n        obs = obs[0]\n        if obs[0] &lt; 11:\n            done = True   \n\n        state_history.append(obs)\n        \n        while not done:\n            a = policy(obs[0])\n            obs, r, done, info, prob = env.step(a)\n            g.append(r)\n            if done:\n                break\n            state_history.append(obs)\n\n        final_reward = sum(g)\n\n        for player_idx, dealer_idx, ace in state_history:\n            player_idx -= 12\n            dealer_idx -= 1\n\n            if ace:\n                state_count_ace[player_idx, dealer_idx] += 1.0\n                values_usable_ace[player_idx, dealer_idx] = mc_policy_evaluation(state_count_ace[player_idx, dealer_idx],\n                                                                                 final_reward,\n                                                                                 values_usable_ace[player_idx, dealer_idx])\n            else:\n                state_count_no_ace[player_idx, dealer_idx] += 1.0\n                values_no_usable_ace[player_idx, dealer_idx] = mc_policy_evaluation(state_count_no_ace[player_idx, dealer_idx],\n                                                                                     final_reward,\n                                                                                     values_no_usable_ace[player_idx, dealer_idx])\n\n\ndef plot_v(values, ace=True):\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(projection='3d')\n\n    x = np.arange(12, 22)\n    y = np.arange(1, 11)\n\n    X, Y = np.meshgrid(y, x)\n\n    Z = values.reshape(X.shape)\n\n    ax.plot_surface(X, Y, Z, cmap=plt.cm.coolwarm, linewidth=1, rstride=1, cstride=1)\n    if ace:\n        ax.set_title(\"With Ace\")\n    else:\n        ax.set_title(\"Without Ace\")\n    ax.set_xlabel(\"Dealer Showing\")\n    ax.set_ylabel(\"Player Hand\")\n    ax.set_zlabel(\"State Value\")\n    plt.show()\n\nplot_v(values_usable_ace)\nplot_v(values_no_usable_ace, ace=False)\n\n\n\n\n\n\n\n\n\nHW: Implement Monte Carlo control in the Blackjack example"
  },
  {
    "objectID": "Documents_files/ST455/Week3/Seminar3/HW3_solution.html",
    "href": "Documents_files/ST455/Week3/Seminar3/HW3_solution.html",
    "title": "Implement the MC control algorithm with exploring starts in the Blackjack example and visualise the resulting optimal policy",
    "section": "",
    "text": "You may want to consider the following initial policy: stick if the sum of player’s cards &gt;= 18 (action = 0), else hit\nSet episodes to 500000\nYou can focus on settings where the current sum is at least 12, as in the example given in Seminar3.ipynb\nThe following plots are examples of optimal policies obtained based on MC control. It is likely that your plots might be slightly different, due to the use a different random seed. There is no need for you to get exactly the same plots.\n\n \n\nRecall the MC control algorithm\n\nFirstly, import the environment and create Numpy arrays to store the value and the number of appearance for every state-action pair,\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport gym\nenv = gym.make('Blackjack-v1')\nplayer = 10\ndealer = 10\nn_action = 2\nqsa_with_ace = np.zeros([player, dealer, n_action])\nqsa_without_ace = np.zeros_like(qsa_with_ace)\na_s_counts_ace = np.zeros_like(qsa_with_ace)\na_s_counts_no_ace = np.zeros_like(a_s_counts_ace)\n\nInitialise the policy: stick if the sum of player’s cards &gt;= 18 (action = 0), else hit\n\npolicy_with_ace = np.ones([player, dealer], dtype=int)\npolicy_with_ace[7:, :] = 0\npolicy_without_ace = np.ones([player, dealer], dtype=int)\npolicy_without_ace[7:, :] = 0\n\nepisodes = 500000\nepsilon = 0.1\n\nRun a certain number of episodes, evaluate and update the current policy after each episode,\n\ndef plot_policy(policy, ace=True):\n    # Get colors\n    cmap = plt.cm.get_cmap(\"Paired\")\n    colors = list([cmap(0.2), cmap(0.8)])\n    label = [\"Stick\", \"Hit\"]\n\n    # Plot results\n    plt.figure(figsize=(15, 6))\n\n    player_range = np.arange(11, 22)\n    dealer_range = np.arange(0, 11)\n\n    plt.pcolor(dealer_range, player_range, policy, label=label, cmap=mpl.colors.ListedColormap(colors))\n    plt.axis([dealer_range.min(), dealer_range.max(), player_range.min(), player_range.max()])\n    col_bar = plt.colorbar()\n    col_bar.set_ticks([0.25, 0.75])\n    col_bar.set_ticklabels(label)\n    plt.grid()\n    plt.xlabel(\"Dealer Showing\")\n    plt.ylabel(\"Player Score\")\n    if ace:\n        plt.title(\"Optimal Policy With a Usable Ace ($\\pi_*$)\")\n    else:\n        plt.title(\"Optimal Policy Without a Usable Ace ($\\pi_*$)\")\n    plt.show()\n\n\nnp.arange(0,10)\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\ndef policy_evaluation(counts, qsa, g):\n    return qsa + (g - qsa) / counts\n\nfor e in range(episodes + 1):\n    done = False\n    obs = env.reset()\n    state_action_history = []\n    g = []\n    # random first action\n    a = env.action_space.sample()\n\n    obs = obs[0]\n    if obs[0] &lt;= 11:\n        done = True\n    else:    \n        state_action_history.append([obs[0], obs[1], obs[2], a])\n    \n\n    while not done:\n        obs, r, done, info, prob = env.step(a)\n        g.append(r)\n        if done:\n            break\n        current_player_idx = obs[0] - 12\n        current_dealer_idx = obs[1] - 1\n        if obs[2]:\n            a = policy_with_ace[current_player_idx, current_dealer_idx]\n        else:\n            a = policy_without_ace[current_player_idx, current_dealer_idx]\n\n        state_action_history.append([obs[0], obs[1], obs[2], a])\n\n    final_reward = sum(g)\n    for player_idx, dealer_idx, ace, action in state_action_history:\n        player_idx -= 12\n        dealer_idx -= 1\n\n        if ace:\n            a_s_counts_ace[player_idx, dealer_idx, action] += 1.0\n            qsa_with_ace[player_idx, dealer_idx, action] = policy_evaluation(a_s_counts_ace[player_idx, dealer_idx, action],\n                                                                        qsa_with_ace[player_idx, dealer_idx, action],\n                                                                        final_reward)\n            # improve policy\n            policy_with_ace[player_idx, dealer_idx] = np.argmax(qsa_with_ace[player_idx, dealer_idx])\n\n        else:\n            a_s_counts_no_ace[player_idx, dealer_idx, action] += 1.0\n            qsa_without_ace[player_idx, dealer_idx, action] = policy_evaluation(a_s_counts_no_ace[player_idx, dealer_idx, action],\n                                                                           qsa_without_ace[player_idx, dealer_idx, action],\n                                                                           final_reward)\n            policy_without_ace[player_idx, dealer_idx] = np.argmax(qsa_without_ace[player_idx, dealer_idx])\n            \nplot_policy(policy_with_ace)\nplot_policy(policy_without_ace, ace=False)"
  },
  {
    "objectID": "Documents_files/ST455/Week10/Seminar/alpha_zero_gomoku/Seminar10.html",
    "href": "Documents_files/ST455/Week10/Seminar/alpha_zero_gomoku/Seminar10.html",
    "title": "LSE ST455 Reinforcement Learning - Alpha Zero for Gomoku",
    "section": "",
    "text": "In this week’s Seminar we look at an implementation of Alpha Zero. Technicall, we loook at its predecessor, a version specialised to Go called AlphaGo Zero.\nIn the lecture in Week 9 we spoke about AlphaGo. A review of AlphaGo pipeline:"
  },
  {
    "objectID": "Documents_files/ST455/Week10/Seminar/alpha_zero_gomoku/Seminar10.html#alphago-pipeline",
    "href": "Documents_files/ST455/Week10/Seminar/alpha_zero_gomoku/Seminar10.html#alphago-pipeline",
    "title": "LSE ST455 Reinforcement Learning - Alpha Zero for Gomoku",
    "section": "AlphaGo Pipeline",
    "text": "AlphaGo Pipeline\n\n\n\n\nPolicy and value networks.\n\n\n\n\n\n\n\nAn Illustration of MCTS."
  },
  {
    "objectID": "Documents_files/ST455/Week10/Seminar/alpha_zero_gomoku/Seminar10.html#alphago-zero",
    "href": "Documents_files/ST455/Week10/Seminar/alpha_zero_gomoku/Seminar10.html#alphago-zero",
    "title": "LSE ST455 Reinforcement Learning - Alpha Zero for Gomoku",
    "section": "AlphaGo Zero",
    "text": "AlphaGo Zero\n\nDifferences between AlphaGo and AlphaGo Zero\n\nAlphaGo uses supervised learning from human expert moves whereas AlphaGo Zero is solely based on reinforcement learning, without human data, guidance or domain knowledge beyond game rules.\nAlphaGo trains the policy and value networks separately whereas AlphaGo Zero uses a single neural network.\nAlphaGo Zero uses Monte-Carlo tree search as a policy improvement step to simulate data trajectories (according to UCB) and Monte Carlo evaluation as a policy evaluation step to update the policy and value network. It repeats this procedure to improve the policy and value network (similar to policy iteration).\n\n\n\nMonte-Carlo-Tree-Search\n\n\n\n\nTaken from DeepMind. Great depiction of MCTS.\n\n\n\nOnce the Tree Search is finished we collect tree search values \\(z\\) and tree search probabilities \\(\\pi\\) for all the nodes in the search tree.\n\n\nSelf-Play\n\n\n\n\nTaken from DeepMind\n\n\n\nThe model plays against itself which is depicted above. Given a certain game state the algorithm performs a MTCS to generate the tree search probabilities \\(\\pi\\). An action is then sampled from \\(\\pi\\) which brings us in the next state. Again, perform a MTCS and so on.. This goes on until the game is finished. The tree search reward \\(z\\) is collected.\n\n\nTraining\n\n\n\n\nTaken from DeepMind\n\n\n\n\\[L_{value} = (z - V)^2.\\]\nThis is the squared loss induced by wrong prediction of the value functions at the experienced states.\nThe loss corresponding to the actions is\n\\[L_{action} = - \\pi^\\top \\log p.\\]\nThe loss is large if the tree search probabilities \\(\\pi\\) are very different from the network probabilities \\(p\\) and is minimized if \\(\\pi = p\\). The function \\(\\pi\\) is generated by the MCTS where the initial update probabilities \\(p\\) are modified to account for experienced Monte-Carlo \\(Q\\) functions and stimulate exploration by using upper confidence bounds (recall our use of these in Seminar 1 when discussing \\(k\\)-bandit problems).\nThe regularization loss is defined as\n\\[L_{reg} = c \\|\\theta\\|^2\\]\nfor some constant \\(c\\) and the current network parameters \\(\\theta\\).\nThe overall loss is defined as\n\\[L = L_{value} + L_{action} + L_{reg}.\\]"
  },
  {
    "objectID": "Documents_files/ST455/Week10/Seminar/alpha_zero_gomoku/Seminar10.html#gomoku",
    "href": "Documents_files/ST455/Week10/Seminar/alpha_zero_gomoku/Seminar10.html#gomoku",
    "title": "LSE ST455 Reinforcement Learning - Alpha Zero for Gomoku",
    "section": "Gomoku",
    "text": "Gomoku\nThe implementation we use is applied to the game of Gomoku.\n\n\n\n\nTypical game state in Gomoku\n\n\n\nGomoku is a 2-player game where players alternatingly place stones on a square board. The aim of the game is to position \\(k\\) consecutive stones horizontally, vertically or diagonally.\nGomoku is a much easier game than Go and board sizes up to 10 x 10 are suitable for a standard CPU to develop a good model in a few days.\nLet us begin by playing a round on an 8x8 board where the target is to get 5 consecutive stones. The MCTS Model used was trained for a few days on a standard CPU.\n\nfrom play import run\n\npygame 2.1.0 (SDL 2.0.16, Python 3.9.7)\nHello from the pygame community. https://www.pygame.org/contribute.html\n\n\n\n%%capture\nrun(width=8, height=8, num_consecutive=5)\n\nThe current AI good but still far from optimal:\n\n\n\n\nImplemented Graphical User Interface: Winning against the current 8x8 and 5 consecutive stones.\n\n\n\n\nThe network architecture\n\nInput\nself.board_width = board_width\nself.board_height = board_height\n\n# Define the tensorflow neural network\n# 1. Input:\nself.input_states = tf.placeholder(\n        tf.float32, shape=[None, 4, board_width, board_height])\nself.input_state = tf.transpose(self.input_states, [0, 2, 3, 1])\n\n\nNetwork Layers\n# 2. Common Networks Layers\nself.conv1 = tf.layers.conv2d(inputs=self.input_state,\n                              filters=32, kernel_size=[3, 3],\n                              padding=\"same\", data_format=\"channels_last\",\n                              activation=tf.nn.relu)\nself.conv2 = tf.layers.conv2d(inputs=self.conv1, filters=64,\n                              kernel_size=[3, 3], padding=\"same\",\n                              data_format=\"channels_last\",\n                              activation=tf.nn.relu)\nself.conv3 = tf.layers.conv2d(inputs=self.conv2, filters=128,\n                              kernel_size=[3, 3], padding=\"same\",\n                              data_format=\"channels_last\",\n                              activation=tf.nn.relu)\n\n\nAction Networks\n# 3-1 Action Networks\nself.action_conv = tf.layers.conv2d(inputs=self.conv3, filters=4,\n                                    kernel_size=[1, 1], padding=\"same\",\n                                    data_format=\"channels_last\",\n                                    activation=tf.nn.relu)\n# Flatten the tensor\nself.action_conv_flat = tf.reshape(\n        self.action_conv, [-1, 4 * board_height * board_width])\n# 3-2 Full connected layer, the output is the log probability of moves\n# on each slot on the board\nself.action_fc = tf.layers.dense(inputs=self.action_conv_flat,\n                                 units=board_height * board_width,\n                                 activation=tf.nn.log_softmax)\n\n\nEvaluation Networks\nself.evaluation_conv = tf.layers.conv2d(inputs=self.conv3, filters=2,\n                                       kernel_size=[1, 1],\n                                       padding=\"same\",\n                                       data_format=\"channels_last\",\n                                       activation=tf.nn.relu)\nself.evaluation_conv_flat = tf.reshape(\n       self.evaluation_conv, [-1, 2 * board_height * board_width])\nself.evaluation_fc1 = tf.layers.dense(inputs=self.evaluation_conv_flat,\n                                     units=64, activation=tf.nn.relu)\n# output the score of evaluation on current state\nself.evaluation_fc2 = tf.layers.dense(inputs=self.evaluation_fc1,\n                                     units=1, activation=tf.nn.tanh)\n\n\nThe loss function\n# 1. Label: the array containing if the game wins or not for each state\nself.labels = tf.placeholder(tf.float32, shape=[None, 1])\n# 2. Predictions: the array containing the evaluation score of each state\n# which is self.evaluation_fc2\n# 3-1. Value Loss function\nself.value_loss = tf.losses.mean_squared_error(self.labels,\n                                               self.evaluation_fc2)\n# 3-2. Policy Loss function\nself.mcts_probs = tf.placeholder(\n        tf.float32, shape=[None, board_height * board_width])\nself.policy_loss = tf.negative(tf.reduce_mean(\n        tf.reduce_sum(tf.multiply(self.mcts_probs, self.action_fc), 1)))\n# 3-3. L2 penalty (regularization)\nl2_penalty_beta = 1e-4\nvars = tf.trainable_variables()\nl2_penalty = l2_penalty_beta * tf.add_n(\n    [tf.nn.l2_loss(v) for v in vars if 'bias' not in v.name.lower()])\n# 3-4 Add up to be the Loss function\nself.loss = self.value_loss + self.policy_loss + l2_penalty\n\n\nThe optimizer\n# Define the optimizer we use for training\nself.learning_rate = tf.placeholder(tf.float32)\nself.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n\n\n\nTraining a model\nWe can also train a new model based on new board dimensions. The best model encountered during training will be stored and can be used to play against.\n\nfrom train import TrainPipeline\n\ntraining_pipeline = TrainPipeline(width=12, height=12, n_in_row=2)\ntraining_pipeline.run()"
  },
  {
    "objectID": "Documents_files/ST455/Week10/Seminar/alpha_zero_gomoku/Seminar10.html#on-a-few-technical-details-in-the-gomoku-implementations",
    "href": "Documents_files/ST455/Week10/Seminar/alpha_zero_gomoku/Seminar10.html#on-a-few-technical-details-in-the-gomoku-implementations",
    "title": "LSE ST455 Reinforcement Learning - Alpha Zero for Gomoku",
    "section": "On a few technical details in the Gomoku Implementations",
    "text": "On a few technical details in the Gomoku Implementations\n\nData augmentation via rotation/flipping\nThe game state is invariant under rotating the board or flipping the game state at one of the axes of symmetry. Hence we can simulate data based on these transformations.\ndef get_equi_data(self, play_data):\n    \"\"\"augment the data set by rotation and flipping\n    play_data: [(state, mcts_prob, winner_z), ..., ...]\n    \"\"\"\n    extend_data = []\n    for state, mcts_porb, winner in play_data:\n        for i in [1, 2, 3, 4]:\n            # rotate counterclockwise\n            equi_state = np.array([np.rot90(s, i) for s in state])\n            equi_mcts_prob = np.rot90(np.flipud(\n                mcts_prob.reshape(self.board_height, self.board_width)), i)\n            extend_data.append((equi_state,\n                                np.flipud(equi_mcts_prob).flatten(),\n                                winner))\n            # flip horizontally\n            equi_state = np.array([np.fliplr(s) for s in equi_state])\n            equi_mcts_prob = np.fliplr(equi_mcts_prob)\n            extend_data.append((equi_state,\n                                np.flipud(equi_mcts_prob).flatten(),\n                                winner))\n    return extend_data"
  },
  {
    "objectID": "Documents_files/ST455/Week10/Seminar/HW10_solution.html",
    "href": "Documents_files/ST455/Week10/Seminar/HW10_solution.html",
    "title": "HW10",
    "section": "",
    "text": "In this assignment, our goal is to implement and evaluate the performance of the lower confidence bound algorithm for the four Bernoulli arms example covered in Lecture 1.\n\n\n\nFour arms\n\n\nYou will run 200 trials. For each trial, generate an offline dataset which consists of 500 rewards under the first arm, 500 rewards under the second arm, 1 reward under the third arm and 500 rewards under the last arm. You then apply the LCB algorithm with the constant \\(c\\) equal to \\(0, 0.1, 1\\). For each choice of \\(c\\), compute the regret (see the definition on Page 20 of Lecture 10) in each trial. Then aggregate the regret over 200 trials, for each \\(c\\). Print and compare these three regrets.\n\nFirst, we implement the environment\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib import rc\n\n# you might see an error that the module \"tkinter\" is not installed. If on Mac Os you can install it through the terminal via \"brew install python-tk@3.9\". General help can as always be found on stackoverflow: \"https://stackoverflow.com/questions/25905540/importerror-no-module-named-tkinter\" \n\nnp.random.seed(10)\n\nbandit_probabilities = [0.10, 0.40, 0.10, 0.10]\n\nnumber_of_bandits = len(bandit_probabilities) # = n_actions\n    \naction_space = np.arange(number_of_bandits) # =[0,1,2,3]\n\nnumber_of_trials = 200\n\ndef step(action):\n    rand = np.random.random()  # [0.0,1.0)\n    reward = 1.0 if (rand &lt; bandit_probabilities[action]) else 0.0\n    return reward\n\n\n\nSecond, we review the lower confidence bound algorithm and implement it\n\n\nlcb_constants = [0, 0.1, 1.0]\nsample_size = [500, 500, 1, 500]\n\n\ndef lower_confidence_bound_policy(c, actions, q_values, num_invocations):\n    lower_confidence_bounds = [q_values[action] - c * np.sqrt(np.log(sum(num_invocations)) / (num_invocations[action])) if num_invocations[action] &gt; 0 else np.inf for action in actions]\n    return np.random.choice([action_ for action_, value_ in enumerate(lower_confidence_bounds) if value_ == np.max(lower_confidence_bounds)])\n\n\nregret = np.zeros((len(lcb_constants), number_of_trials), dtype=float)\n\nfor lcb_constant_counter, lcb_constant in enumerate(lcb_constants):\n    for trial in range(number_of_trials):\n        n = np.zeros(number_of_bandits, dtype=int)\n        q = np.zeros(number_of_bandits, dtype=float)\n\n        for j in range(len(sample_size)):\n            for t in range(sample_size[j]):\n                action = j\n                r = step(action)\n\n                # updating action counter and expected reward Q\n                n[action] += 1\n                q[action] = q[action] + 1.0 / (n[action] + 1) * (r - q[action])\n\n        regret[lcb_constant_counter, trial] = np.max(bandit_probabilities) - bandit_probabilities[lower_confidence_bound_policy(lcb_constant, action_space, q, sample_size)]\n\nprint(np.mean(regret, axis=1))\n\n[0.027 0.    0.   ]"
  },
  {
    "objectID": "Documents_files/ST455/Week11/Seminar/Sem11-Part1.html",
    "href": "Documents_files/ST455/Week11/Seminar/Sem11-Part1.html",
    "title": "OPE in Contextual Bandits",
    "section": "",
    "text": "This seminar is concerned with OPE under a contextual bandit setting \\(\\{(S_t,A_t,R_t)\\}_{t\\ge 0}\\) where the state-action-reward triplets are independent over time. Let \\(b\\) denote the behavior policy that generates the data, e.g., \\(\\mbox{Pr}(A_t=a|S_t)=b(a|S_t)\\) for any \\(a\\) and \\(t\\), and \\(\\pi\\) denote the target policy that we would like to evaluate. Our objective is to estimate the mean outcome under the target policy, \\[\\begin{eqnarray*}\n    \\eta^{\\pi}=\\int_{s} \\sum_a \\pi(a|s)r(s,a) \\nu(s)ds,\n\\end{eqnarray*}\\] where \\(r(s,a)\\) corresponds to the reward function \\(r(s,a)=\\mathbb{E} (R|A=a,S=s)\\) and \\(\\nu\\) corresponds to the density function of the state\nWe implement three estimators here, corresponding to the direct estimator, importance sampling estimator and the doubly-robust estimator.\n\nFirst, we define the data generating process and use the online Monte Carlo method to simulate the target policy’s oracle value\nWe consider using the following example to demonstrate the performance of these three estimators: \\[\\begin{eqnarray*}\n&&S_t\\sim N(0,1),\\\\\n&&b(1|s)=1-b(0|s)=\\frac{\\exp(s)}{1+\\exp(s)},\\\\\n&&R_t=r(S_t,A_t)+N(0,1)\n\\end{eqnarray*}\\] where \\(r(a,s)=as\\). The target policy \\(\\pi\\) is given by \\[\\begin{eqnarray*}\n\\pi(1|s)=1-\\pi(0|s)=\\frac{1}{1+\\exp(s)}\n\\end{eqnarray*}\\]\n\nfrom ast import Import\nfrom random import sample\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\n\n# Task: Play around with dataset and sample sizes. \nDATASET_SIZE = 10**7\nsample_sizes = [200,400,600,800,1000] ## later to be used in trainning\n\nnp.random.seed(42)\n\n## Monte Carlo estimate on full dataset\n### Generating states, applying policies and generating rewards.\ndata_states = np.random.normal(size = DATASET_SIZE)\n\ndef sigmoid(x, theta = 1): # target and behavior policies\n    return 1 / (1 + np.exp(- theta * x))\n\ntarget_policy = sigmoid(data_states, theta=-1)\ndata_actions = np.random.binomial(1, target_policy)\nv0 = np.mean(data_states * data_actions)\n\n## define generate reward function, behavior and target policies to be used later\n\ndef generate_rewards(states, actions):\n    return states * actions + np.random.normal(size=states.shape[0])\ndata_rewards = generate_rewards(data_states, data_actions) \n\nclass Policy:\n    def apply(self, action, state):\n        pass\n\nclass LogRegPolicy(Policy): ## define the class of sigmoid policies, indexed by theta\n    def __init__(self, theta):\n        self._theta = theta\n    def apply(self, action, state):\n        return sigmoid(state, self._theta) if action == 1 else 1 - sigmoid(state, self._theta)\n\n\n## print the true value\nprint(v0)\n\n-0.20669656362057126\n\n\n\n\nSecond, we implement the direct estimator.\nThe direct estimator is given by \\[\\begin{eqnarray*}\n    \\frac{1}{T}\\sum_{t=0}^{T-1} \\sum_a \\pi(a|S_t)\\widehat{r}(S_t,a),\n\\end{eqnarray*}\\] where \\(\\widehat{r}\\) denotes some regression estimator for the reward function. That is, we use \\(\\widehat{r}\\) to estimate \\(r\\) and the empirical state distribution to estimate \\(\\nu\\), and directly plug-in these estimators in the definition of \\(\\eta^{\\pi}\\) for policy evaluation.\nTo estimate the reward function, we can consider using two models: a constant model under which \\[\\begin{eqnarray*}\n    \\widehat{r}(a,s)=\\left[\\sum_{t=0}^{T-1} \\mathbb{I}(A_t=a)\\right]^{-1}\\left[\\sum_{t=0}^{T-1} \\mathbb{I}(A_t=a)R_t\\right]\n\\end{eqnarray*}\\] and a linear regression model which applies a linear regression with responses \\(\\{R_t:\\mathbb{I}(A_t=a)\\}_{t\\ge 0}\\) and predictors \\(\\{S_t:\\mathbb{I}(A_t=a)\\}_{t\\ge 0}\\) to estimate \\(r(a,s)\\). Here, the constant model will misspecify the reward function whereas the linear regression model correctly specifies \\(r\\).\n\n# Direct Estimators\n\nclass Estimator:\n    def estimate_policy(self, policy, states, actions, rewards):\n        pass\n    def name(self):\n        return self._name\n\nclass DirectEstimator(Estimator):\n    def estimate(self, state, action):\n        pass\n    \n    def estimate_policy(self, states):\n        target_policy = sigmoid(states, theta=-1)\n        estimate = target_policy * self.estimate(states, 1)\n        estimate += (1 - target_policy) * self.estimate(states, 0)\n        return np.mean(estimate) \n\n## 1. direct estimator with constant reward \n\nclass ConstantDirectEstimator(DirectEstimator):\n    def __init__(self, states, actions, rewards):\n        self._name = \"const direct\"\n        self._constant_reward = np.zeros(2)\n        for action in range(2):\n            self._constant_reward[action] = np.sum(rewards[actions==action])/np.sum(actions == action)\n \n    def estimate(self, states, action):\n        return self._constant_reward[action] * np.ones(len(states))\n\n## 2. direct estimator with lin reg reward\n\nclass LinRegDirectEstimator(DirectEstimator):\n    def __init__(self, states, actions, rewards):\n        self._name = \"lin reg direct\"\n        self._lin_reg_reward = [None] * 2 \n        for action in range(2):\n            self._lin_reg_reward[action] = LinearRegression().fit(np.expand_dims(states[actions==action],axis=1), rewards[actions == action])\n    def estimate(self, states, action):\n        return np.squeeze(self._lin_reg_reward[action].predict(np.expand_dims(states,axis=1)))\n    \ndef create_direct_estimators(states, actions, rewards):\n    estimators = []\n\n    constant_estimator = ConstantDirectEstimator(states, actions, rewards)\n    estimators.append(constant_estimator)\n\n    lin_reg_direct_estimator = LinRegDirectEstimator(states, actions, rewards)\n    estimators.append(lin_reg_direct_estimator)    \n\n    return estimators\n    \nnumber_of_runs = 20\nestimates = np.zeros(shape=(len(sample_sizes), number_of_runs, 2),dtype=float)\nfor sample_size_idx, sample_size in enumerate(sample_sizes):\n#    print(f\"sample size {sample_size}\")\n\n    for run in range(number_of_runs):\n\n        states = np.random.normal(size = sample_size)\n        behavior_policy = sigmoid(states, theta=1)\n        actions = np.random.binomial(1, behavior_policy)\n        rewards = generate_rewards(states, actions)\n\n        estimators = create_direct_estimators(states, actions, rewards)\n\n        for estimator_idx, estimator in enumerate(estimators):\n            estimate = estimator.estimate_policy(states)\n#            print(estimator.name(), estimate)\n            estimates[sample_size_idx, run, estimator_idx] = estimate\n    \ndef plot_data(estimates):\n\n    mean_error = np.mean(estimates, axis=1) - v0\n    mse = np.square(mean_error) + np.square(np.std(estimates, axis=1))\n    plt.figure(figsize=(6, 4), dpi=100)\n    fig, axs = plt.subplots(2)\n    fig.suptitle(\"Off-Policy evaluation\")\n    \n    # Plotting both the curves simultaneously\n    for idx in range(mse.shape[1]):\n        axs[0].plot(sample_sizes, mse[:,idx], label=estimators[idx].name(), linewidth=3)\n        axs[1].plot(sample_sizes, mean_error[:,idx], label=estimators[idx].name(), linewidth=3)\n    \n    # Naming the x-axis, y-axis and the whole graph\n    axs[0].set_xlabel(\"Sample Size\")\n    axs[0].set_ylabel(\"Mean squared error\")\n    \n    # axs[0].plt.xlabel(\"Sample Size\")\n    axs[1].set_ylabel(\"Bias\")\n    \n    # Adding legend, which helps us recognize the curve according to it's color\n    plt.legend()\n    \n    # To load the display window\n    plt.show()\n\nplot_data(estimates)    \n\n&lt;Figure size 600x400 with 0 Axes&gt;\n\n\n\n\n\nAs we can see from the above plot, the bias and MSE of linear regression-based direct estimator are very close to zero. To the contrary, the constant model-based direct estimator is biased, due to the misspecification of the reward function\n\n\nThird, we implement the IS estimator.\nThe IS estimator is given by \\[\\begin{eqnarray*}\n    \\frac{1}{T}\\sum_{t=0}^{T-1}\\frac{\\pi(A_t|S_t)}{\\widehat{b}(A_t|S_t)}R_t,\n\\end{eqnarray*}\\] where \\(\\widehat{b}\\) denotes the estimated behavior policy. Using the change of measure theory, we can show that the above estimator is unbiased when \\(\\widehat{b}=b\\).\nTo estimate the behavior policy, we can similarly consider two models: a constant model under which \\[\\begin{eqnarray*}\n\\widehat{b}(a,s)=\\frac{1}{T}\\sum_{t=0}^{T-1} \\mathbb{I}(A_t=a)\n\\end{eqnarray*}\\] and a logistic regression model applies a logistic regression with responses \\(\\{R_t:\\mathbb{I}(A_t=a)\\}_{t\\ge 0}\\) and predictors \\(\\{S_t:\\mathbb{I}(A_t=a)\\}_{t\\ge 0}\\) to estimate \\(b(a,s)\\). Similarly, the constant model will misspecify the reward function whereas the logistic regression model correctly specifies \\(b\\).\n\nclass ImportanceEstimator(Estimator):\n    def estimate(self, states):\n        pass\n\n    def estimate_policy(self, states, actions, rewards):\n        target_policy = sigmoid(states, theta=-1)\n        estimate = np.zeros(len(states))\n        estimate[actions==1] = target_policy[actions==1] / self.estimate(states[actions==1]) * rewards[actions==1]\n        estimate[actions==0] = (1-target_policy[actions==0]) / (1-self.estimate(states[actions==0])) * rewards[actions==0]\n        return np.mean(estimate)\n\n## 3. importance sampling estimator with constant policy     \n    \nclass ConstantImportanceEstimator(ImportanceEstimator):\n    def __init__(self, states, actions):\n        self._name = \"const importance\"\n        self._constant_policy = np.mean(actions) \n    def estimate(self, states):\n        return self._constant_policy * np.ones(len(states))\n\n## 4. importance sampling estimator with log regression policy\n\nclass LogisticRegressionImportanceEstimator(ImportanceEstimator):\n    def __init__(self, states, actions):\n        self._name = \"log reg importance\"\n        self._logistic_reg_policy = LogisticRegression().fit(np.expand_dims(states,axis=1), actions)\n    def estimate(self, states):\n        pred = self._logistic_reg_policy.predict_proba(np.expand_dims(states, axis=1))\n        pred = np.squeeze(pred)\n        return pred[:,1]\n    \n## 5. importance sampling estimator with known behavior policy\n\nclass OracleImportanceEstimator(ImportanceEstimator):\n    def __init__(self, states, actions):\n        self._name = \"oracle importance\"\n    def estimate(self, states):\n        return sigmoid(states, theta=1)    \n    \ndef create_IS_estimators(states, actions, rewards):\n    estimators = []\n\n    constant_importance_estimator = ConstantImportanceEstimator(states, actions)\n    estimators.append(constant_importance_estimator)\n\n    logistic_reg_importance_estimator = LogisticRegressionImportanceEstimator(states, actions)\n    estimators.append(logistic_reg_importance_estimator)\n\n    oracle_importance_estimator = OracleImportanceEstimator(states, actions)\n    estimators.append(oracle_importance_estimator)\n\n    return estimators\n\nnumber_of_runs = 20\nestimates = np.zeros(shape=(len(sample_sizes), number_of_runs, 3),dtype=float)\nfor sample_size_idx, sample_size in enumerate(sample_sizes):\n#    print(f\"sample size {sample_size}\")\n\n    for run in range(number_of_runs):\n\n        states = np.random.normal(size = sample_size)\n        behavior_policy = sigmoid(states, theta=1)\n        actions = np.random.binomial(1, behavior_policy)\n        rewards = generate_rewards(states, actions)\n\n        estimators = create_IS_estimators(states, actions, rewards)\n\n        for estimator_idx, estimator in enumerate(estimators):\n            estimate = estimator.estimate_policy(states, actions, rewards)\n#            print(estimator.name(), estimate)\n            estimates[sample_size_idx, run, estimator_idx] = estimate\n            \nplot_data(estimates) \n\n&lt;Figure size 600x400 with 0 Axes&gt;\n\n\n\n\n\nAs we can see from the above plot, biases and MSEs of logistic regression-based IS and oracle IS estimators are very close to zero. To the contrary, the constant model-based IS estimator is biased, due to the misspecification of the behavior policy. In addition, comparing the logistic regression-based IS and the oracle IS estimator, it is easy to tell that the former has a smaller variance. This aligns with the following fact we discussed in the lecture\n\n\n\nFinally, we implement the doubly-robust estimator.\nThe doubly-robust estimator is given by \\[\\begin{eqnarray*}\n    \\frac{1}{T}\\sum_{t=0}^{T-1} \\sum_a \\pi(a|S_t)\\widehat{r}(S_t,a)+\\frac{1}{T}\\sum_{t=0}^{T-1}\\frac{\\pi(A_t|S_t)}{\\widehat{b}(A_t|S_t)}[R_t-\\widehat{r}(S_t,A_t)].\n\\end{eqnarray*}\\] The first term in the above equation is the direct estimator whereas the second term corresponds to an augmentation term. The purpose of adding the augmentation term is to offer additional protection against potential model misspecification of the reward function. It can be shown that the above estimator is doubly-robust, in the sense that it is consistent when either \\(\\widehat{r}\\) or \\(\\widehat{b}\\) is correctly specified.\nWe similarly consider two choices of reward functions and two choices of behavior policies. This yields a total of 4 doubly-robust estimators.\n\nclass DoublyRobustEstimator(Estimator):\n    def __init__(self, direct_estimator, importance_estimator):\n        self._name = \"doubly robust: \" + direct_estimator.name() + \" + \" + importance_estimator.name()\n        self._direct_estimator = direct_estimator\n        self._importance_estimator = importance_estimator\n    def estimate_policy(self, states, actions, rewards):\n        rb = np.zeros(len(states))\n        rb[actions==1] = self._direct_estimator.estimate(states[actions==1], 1)\n        rb[actions==0] = self._direct_estimator.estimate(states[actions==0], 0)\n        estimate = np.zeros(len(states))\n        target_policy = sigmoid(states, theta=-1)\n        estimate[actions==1] = target_policy[actions==1] / self._importance_estimator.estimate(states[actions==1]) * rb[actions==1]\n        estimate[actions==0] = (1-target_policy[actions==0]) / (1-self._importance_estimator.estimate(states[actions==0])) * rb[actions==0]\n        return self._direct_estimator.estimate_policy(states) + self._importance_estimator.estimate_policy(states,actions, rewards) - np.mean(estimate)\n    \ndef create_DR_estimators(states, actions, rewards):\n    estimators = []\n\n    constant_estimator = ConstantDirectEstimator(states, actions, rewards)\n\n    lin_reg_direct_estimator = LinRegDirectEstimator(states, actions, rewards)\n    \n    direct_estimators = [constant_estimator, lin_reg_direct_estimator]\n\n    constant_importance_estimator = ConstantImportanceEstimator(states, actions)\n\n    logistic_reg_importance_estimator = LogisticRegressionImportanceEstimator(states, actions)\n    \n    importance_estimators = [constant_importance_estimator, logistic_reg_importance_estimator]\n\n    doubly_robust_estimators = []\n    for dir_est in direct_estimators:\n        for imp_est in importance_estimators:\n            estimators.append(DoublyRobustEstimator(dir_est,imp_est))\n\n    return estimators\n\nnumber_of_runs = 20\nestimates = np.zeros(shape=(len(sample_sizes), number_of_runs, 4),dtype=float)\nfor sample_size_idx, sample_size in enumerate(sample_sizes):\n#    print(f\"sample size {sample_size}\")\n\n    for run in range(number_of_runs):\n\n        states = np.random.normal(size = sample_size)\n        behavior_policy = sigmoid(states, theta=1)\n        actions = np.random.binomial(1, behavior_policy)\n        rewards = generate_rewards(states, actions)\n\n        estimators = create_DR_estimators(states, actions, rewards)\n\n        for estimator_idx, estimator in enumerate(estimators):\n            estimate = estimator.estimate_policy(states, actions, rewards)\n#            print(estimator.name(), estimate)\n            estimates[sample_size_idx, run, estimator_idx] = estimate\n            \nplot_data(estimates) \n\n&lt;Figure size 600x400 with 0 Axes&gt;\n\n\n\n\n\nWe make a few observations. First, it can be seen that only one DR estimator that uses the constant model to parameterize both the behavior policy and the reward function is not consistent. All other three models are consistent. This verifies the doubly-robustness property, as we discussed in the lecture.\n\nSecond, it can be seen that among the three consistent DR estimators, the one that misspecifies the reward function (uses a constant model for the reward) has a larger MSE. This verifies Fact 2 that we covered in the lecture"
  },
  {
    "objectID": "Documents_files/ST455/Week11/Seminar/Sem11-Part2.html",
    "href": "Documents_files/ST455/Week11/Seminar/Sem11-Part2.html",
    "title": "OPE in Reinforcement Learning",
    "section": "",
    "text": "The second problem is concerned with OPE under the CartPole environment. We aim to estimate the value of a certain policy based on data generated by another policy.\nimport gym  \nfrom sklearn.neural_network import MLPRegressor\nimport sklearn.exceptions\nimport numpy as np\nimport warnings\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nwarnings.simplefilter(\"ignore\", category=sklearn.exceptions.ConvergenceWarning)\nenv = gym.make(\"CartPole-v1\")  \n\nepisodes_for_behavior_policy = 100\nnum_mc_episodes_for_target_policy = 10000\nregression_iterations = 60 \ngammas = np.array([0.95, 0.99, 1])\nlr = 0.5\ntol = 1e-4\nThe behavior policy is chosen to be the same as in HW7, that is we choose a parameter \\(\\epsilon\\) for which we randomly pick an action. With probability \\(1 - \\varepsilon\\) we pick our action based on the angle of the pole.\ndef behavior_policy(angle, epsilon=0):\n    if np.random.binomial(1, epsilon) == 1:\n        return np.random.choice([0,1])\n    else:\n        if angle &lt; 0:\n            return 0\n        else:\n            return 1\nAs for the target policy, we can consider a fixed policy that push the cart to the left when the angle is negative and push the cart to the right when the angle is positive. So it is the same as the behavior policy with \\(\\varepsilon = 0\\).\ndef target_policy(angle):\n    return behavior_policy(angle)\nWe first use the Monte Carlo method to evaluate the true value under this policy for \\(\\gamma = 0.95, 0.99, 1.00\\).\nrewards = np.zeros((gammas.shape[0], num_mc_episodes_for_target_policy))\n\nfor i in tqdm(range(num_mc_episodes_for_target_policy)):\n    obs = env.reset()\n    obs = obs[0]\n    done = False\n    reward = np.zeros_like(gammas)\n    gam_pow = np.ones_like(gammas)\n    while not done:\n        a = target_policy(obs[2])\n        obs, r, done, info, _ = env.step(a)\n        reward += gam_pow * r \n        gam_pow *= gammas\n    rewards[:,i] = reward\n\nmc_rewards_target_policy = np.mean(np.array(rewards), axis=1) \n\nprint(f\"Monte Carlo reward of target policy is {mc_rewards_target_policy}\")\n\n\n\n\nMonte Carlo reward of target policy is [17.44633696 34.21167484 42.0513    ]\nNext, we generate data with the behavior policy.\nsar = []\nsar_by_episode = []\ninitial_states = []\n\nfor i in tqdm(range(episodes_for_behavior_policy)):\n    obs = env.reset()\n    obs = obs[0]\n    done = False\n    initial = True\n    cur_sar = []\n    while not done:\n        a = behavior_policy(obs[2], epsilon=0.5)\n        obs, r, done, info, _ = env.step(a)\n        if initial:\n            initial_states.append(obs)\n            initial = False\n        cur_sar.append([obs, a, r, done])\n    sar_by_episode.append(cur_sar)\n\nsar = [item for cur_sar in sar_by_episode for item in cur_sar]"
  },
  {
    "objectID": "Documents_files/ST455/Week11/Seminar/Sem11-Part2.html#fitted-q-evaluation-with-mlpregressor",
    "href": "Documents_files/ST455/Week11/Seminar/Sem11-Part2.html#fitted-q-evaluation-with-mlpregressor",
    "title": "OPE in Reinforcement Learning",
    "section": "Fitted Q Evaluation with MLPRegressor",
    "text": "Fitted Q Evaluation with MLPRegressor\nNext, we employ the fitted-Q evaluation algorithm which iteratively updates the Q-function using the following formula \\[\\begin{eqnarray*}\n    \\widehat{Q}_k=\\mathrm{argmin}_{Q} \\sum_{t} [R_{i,t}+\\gamma \\sum_{a} \\pi(a|S_{i,t+1})\\widehat{Q}_{k-1}(a,S_{i,t+1}) -Q(S_{i,t},A_{i,t})]^2.\n\\end{eqnarray*}\\] Here, \\(i\\) indexes the episode and \\(t\\) indexes the decision time. In the CartPole example, similarly, when \\(S_{i,t}\\) is a termination state, then the target becomes \\(R_{i,t}\\). We use the MLPregressor to implement solve the regression problem. Given the Q-estimator, the value estimator is computed by the averaged value of \\(T^{-1}\\sum_i \\sum_a \\pi(a|S_{i,t})\\widehat{Q}_k(a,S_{i,0})\\).\n\n## Intialize Q-function to be zero and construct target\n\nX = np.zeros((len(sar), len(obs)+1))\ny = np.zeros(len(sar))\nfor i in range(len(sar)):\n    X[i,-1] = sar[i][1]\n    X[i,0:len(obs)] = sar[i][0]\n\n### Run Fitted-Q-Iteration\n\nstored_estimates = np.zeros((len(gammas), regression_iterations))\n\nfor gamma_idx in tqdm(range(len(gammas))):\n    gamma = gammas[gamma_idx]\n    for i in range(len(sar)):\n        y[i] = sar[i][2]\n    regr0 = MLPRegressor(random_state=1, max_iter=500, learning_rate_init=lr, tol=tol).fit(X[X[:,-1]==0,0:-1], y[X[:,-1]==0])\n    regr1 = MLPRegressor(random_state=1, max_iter=500, learning_rate_init=lr, tol=tol).fit(X[X[:,-1]==1,0:-1], y[X[:,-1]==1])\n    for k in tqdm(range(regression_iterations)):\n        for i in range(len(sar)):\n            if sar[i][3]:\n                y[i] = sar[i][2]\n            else:\n                action = target_policy(sar[i+1][0][2])\n                y[i] = sar[i][2] + gamma * ((1-action) * regr0.predict(np.expand_dims(sar[i+1][0], 0)) + action * regr1.predict(np.expand_dims(sar[i+1][0], 0)))\n        regr0 = MLPRegressor(random_state=1, max_iter=500, learning_rate_init=0.2).fit(X[X[:,-1]==0,0:-1], y[X[:,-1]==0])\n        regr1 = MLPRegressor(random_state=1, max_iter=500, learning_rate_init=0.2).fit(X[X[:,-1]==1,0:-1], y[X[:,-1]==1])\n\n        ### Run Estimator on initial states\n\n        regr = [regr0, regr1]\n\n        estimated_value = 0\n        for state in initial_states:\n            target_action = target_policy(state[2])\n            estimated_value += regr[target_action].predict(np.expand_dims(state, axis=0))\n        estimated_value /= len(initial_states)\n        # tqdm.write(f\"Estimated value after {k} iterations for gamma {gamma} is {estimated_value}\")\n        stored_estimates[gamma_idx, k] = estimated_value\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we plot the results\n\nplt.figure(figsize=(6, 4), dpi=100)\nplt.title(\"Off-Policy evaluation for CartPole\")\n\nfor idx, gamma in enumerate(gammas):\n    plt.plot(range(regression_iterations), stored_estimates[idx], label=f\"Q gam = {gamma}\",linewidth=3)\n\nfor idx, gamma in enumerate(gammas):\n    mc_val = np.full((regression_iterations), mc_rewards_target_policy[idx])\n    plt.plot(range(regression_iterations), mc_val, label = f\"MC gam = {gamma}\", linewidth=2)\n\nplt.xlabel(\"Fitted Q Iteration\")\nplt.ylabel(\"Estimated Values\")\n\nplt.legend()\n\nplt.show()"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_9.html",
    "href": "Documents_files/ME200/Solutions_Assignment_9.html",
    "title": "Solutions to Assignment 9",
    "section": "",
    "text": "ME200, Johannes Ruf and Luitgard Veraart\n$ $\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import comb\nrng = np.random.default_rng(12345)"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_9.html#exercise-37-pricing-a-given-european-option",
    "href": "Documents_files/ME200/Solutions_Assignment_9.html#exercise-37-pricing-a-given-european-option",
    "title": "Solutions to Assignment 9",
    "section": "Exercise 37: (Pricing a given European option)",
    "text": "Exercise 37: (Pricing a given European option)\n\nIn order to have no arbitrage we need \\(d &lt; 1+r &lt; u\\) which is equivalent to \\(-\\frac{2}{3} &lt; r &lt; 2\\).\nHere \\(\\tilde{p} = \\frac{1+r - d}{u-d} = \\frac{1+\\frac{2}{3} - \\frac{1}{3}}{3-\\frac{1}{3}}=\\frac{1}{2}\\) and the time-0 price is given by \\[\\begin{align*}\nV_0&=X_0^{\\varphi} = \\frac{1}{(1+r)^2} \\left( V_2(HH) \\tilde{p}^2 + V_2(HT) \\tilde{p}(1-\\tilde{p}) + V_2(TH) \\tilde{p}(1 - \\tilde{p}) + V_2(TT)(1-\\tilde{p})^2\\right)\\\\\n&=\\frac{1}{(1+\\frac{2}{3})^2} \\left( (1-x) \\frac{1}{4} + 2 x  \\frac{1}{4} + (x - \\frac{1}{2})  \\frac{1}{4}\\right) \\\\\n&= \\frac{9}{25} \\frac{1}{4} \\left(2x + \\frac{1}{2}\\right)\\\\\n&= \\frac{9}{25} \\frac{1}{2}  x + \\frac{9}{25} \\frac{1}{8}.\n\\end{align*}\\]"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_9.html#exercise-38-asian-option",
    "href": "Documents_files/ME200/Solutions_Assignment_9.html#exercise-38-asian-option",
    "title": "Solutions to Assignment 9",
    "section": "Exercise 38: (Asian option)",
    "text": "Exercise 38: (Asian option)\n\nSuppose at time \\(n\\) we have \\(S_n=s\\) and \\(Y_n=y\\). Then at time \\(n+1\\) we have:\n\n\nif the coin toss is H: \\(S_{n+1} = us\\), \\(Y_{n+1}=Y_n + S_{n+1} = Y_n + u S_n\\).\nif the coin toss is T: \\(S_{n+1} = ds\\), \\(Y_{n+1}=Y_n + S_{n+1} = Y_n + d S_n\\).\n\nTherefore, formula (8.13) and (8.14) take the form \\[\\begin{align*}\nv_n(s, y) = \\frac{1}{1+r} \\left( \\tilde{p} v_{n+1}(us, y+us) + (1-\\tilde{p}) v_{n+1}(ds, y+ds) \\right)\n\\end{align*}\\] and \\[\\begin{align*}\n\\Delta_n(s,y) = \\frac{v_{n+1}(us, y+us) - v_{n+1} (ds, y+ds)}{us -ds}.\n\\end{align*}\\]\n\nThen, we obtain \\[\\begin{align*}\nv_3(32, 60)&=(60/4-4)^+=11,\\\\\nv_3(8, 36)&=(36/4-4)^+=5, \\\\\nv_3(8, 24)&=(24/4-4)^+=2, \\\\\nv_3(8, 18)&=(18/4-4)^+=0.5, \\\\\nv_3(2, 18)&=(18/4-4)^+=0.5, \\\\\nv_3(2, 12)&=(12/4-4)^+=0, \\\\\nv_3(2, 9)&=(9/4-4)^+=0, \\\\\nv_3(0.5, 7.5)&=(7.5/4-4)^+=0.\n\\end{align*}\\] Next we use the recursion to compute \\(v_2\\): \\[\\begin{align*}\nv_2(16, 28)& =\\frac{4}{5} \\left( \\frac{1}{2} v_3(32, 60) + \\frac{1}{2} v_3(8, 36)\\right)=\\frac{2}{5}(11+5)=6.4, \\\\\nv_2(4, 16)& =\\frac{4}{5} \\left( \\frac{1}{2} v_3(8, 24) + \\frac{1}{2} v_3(2, 18)\\right)=\\frac{2}{5}(2+0.5)=1, \\\\\nv_2(4, 10)& =\\frac{4}{5} \\left( \\frac{1}{2} v_3(8, 18) + \\frac{1}{2} v_3(2, 12)\\right)=\\frac{2}{5}(0.5+0)=0.2, \\\\\nv_2(1, 7)& =\\frac{4}{5} \\left( \\frac{1}{2} v_3(2, 9) + \\frac{1}{2} v_3(0.5, 7.5)\\right)=\\frac{2}{5}(0+0)=0.\n\\end{align*}\\] Again we use the recursion to compute \\(v_1\\): \\[\\begin{align*}\nv_1(8, 12)&=\\frac{4}{5} \\left( \\frac{1}{2} v_2(16, 28) + \\frac{1}{2} v_2(4, 16)\\right)=\\frac{2}{5}(6.4+1)=2.96, \\\\\nv_1(2, 6)&=\\frac{4}{5} \\left( \\frac{1}{2} v_2(4, 10) + \\frac{1}{2} v_2(1, 7)\\right)=\\frac{2}{5}(0.2 + 0)=0.08.\n\\end{align*}\\] Finally, we get the time-0 price: \\[\\begin{align*}\nv_0(4, 4)&=\\frac{4}{5} \\left( \\frac{1}{2} v_1(8, 12) + \\frac{1}{2} v_2(2, 6)\\right)=\\frac{2}{5}(2.96+0.08)=1.216.\n\\end{align*}\\]\nThe formula for \\(\\Delta_n(s, y)\\) has been derived in part 1."
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_9.html#exercise-39-generating-samples-from-the-normal-distribution",
    "href": "Documents_files/ME200/Solutions_Assignment_9.html#exercise-39-generating-samples-from-the-normal-distribution",
    "title": "Solutions to Assignment 9",
    "section": "Exercise 39: (Generating samples from the normal distribution)",
    "text": "Exercise 39: (Generating samples from the normal distribution)\n\n\\[\\begin{align*}\n\\textrm{E}[Z] = \\textrm{E}[\\sum_{i=1}^{12} U_i] = \\sum_{i=1}^{12} \\textrm{E}[U_i] = \\sum_{i=1}^{12} \\frac{1}{2}\\left(\\frac{1}{2} - \\frac{1}{2} \\right)=0  \n\\end{align*}\\] and \\[\\begin{align*}\n\\textrm{Var}(Z)=\\textrm{Var}(\\sum_{i=1}^{12} U_i) = \\sum_{i=1}^{12} \\textrm{Var}(U_i) =\n\\sum_{i=1}^{12} \\frac{1}{12}\\left(\\frac{1}{2}-\\left(-\\frac{1}{2}\\right) \\right)^2=1,  \n\\end{align*}\\] where we use the fact that the \\(U_i\\) are independent when we wrote that the variance of the sum is equal to the sum of the variances.\nWe apply the Central Limit Theorem to the sample of the \\(U_i\\) with \\(U \\sim \\textrm{Unif}\\left(-\\frac{1}{2}, \\frac{1}{2}\\right)\\). Then,\n\\[\\begin{align*}\n\\frac{\\frac{1}{n} \\sum_{i=1}^n U_i - \\textrm{E}[U]}{\\sqrt{\\frac{\\textrm{Var}(U)}{n}}}\n=\\frac{\\frac{1}{n} \\sum_{i=1}^n U_i}{\\sqrt{\\frac{1/12}{n}}}\n=\\frac{\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n U_i}{\\sqrt{1/12}}\n\\end{align*}\\] follows a standard normal distribution for large \\(n\\). Now with \\(n=12\\) we find that \\[\\begin{align*}\n\\frac{\\frac{1}{\\sqrt{12}} \\sum_{i=1}^{12} U_i}{\\sqrt{1/12}} = Z\n\\end{align*}\\] and hence \\(Z\\) has approximately a standard normal distribution by the Central Limit Theorem.\n\n\nsamplesize = 100000\nZs = np.zeros(samplesize)\n\nfor i in range(samplesize): \n    u = rng.random(size=12)\n    nonstandardu = -0.5 + u\n    # Alternatively could use \n    # nonstandardu = rng.uniform(low=-0.5, high=0.5, size=12)\n    Zs[i] = nonstandardu.sum()\n    \nsample_numpy = rng.standard_normal(size = samplesize) \n    \n\n_, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 8))\naxs[0].hist(Zs, bins=1000, density=True);\naxs[0].set_title('Histogram of random numbers generated by antiquated generator')\naxs[1].hist(sample_numpy, bins=1000, density=True);\naxs[1].set_title('Histogram of random numbers generated by NumPy directly');\nfor ax in axs:\n    ax.set_ylim([0, 0.6]);\n    ax.set_xlim([-4, 4]);"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_8.html",
    "href": "Documents_files/ME200/Solutions_Assignment_8.html",
    "title": "Solutions to Assignment 8",
    "section": "",
    "text": "ME200, Johannes Ruf and Luitgard Veraart\n$ $"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_8.html#exercise-34-hedging-a-long-position",
    "href": "Documents_files/ME200/Solutions_Assignment_8.html#exercise-34-hedging-a-long-position",
    "title": "Solutions to Assignment 8",
    "section": "Exercise 34: (Hedging a long position)",
    "text": "Exercise 34: (Hedging a long position)\nThe investor should use the opposite of the replicating strategy derived in the lecturer for the seller of the option. This means that rather than holding \\(1/2\\) shares of stocks she should hold \\(-1/2\\) shares of stocks (i.e., she short sells the stock). This generates an income of \\(\\unicode{x00A3}1/2\\cdot 4=2\\). She should now invest this in the riskless asset.\nIf at time 1 the stock price goes up, she has an option worth \\(\\unicode{x00A3}3\\) and has \\(\\unicode{x00A3}2\\cdot \\frac{5}{4}= \\unicode{x00A3}2.5\\) in the riskless asset. She must pay \\(\\unicode{x00A3}0.5 \\cdot 8= \\unicode{x00A3}4\\) to cover the short position in the stock, which leaves her with \\(\\unicode{x00A3}1.5\\) as desired.\nIf at time 1 the stock price goes down, she has an option worth \\(\\unicode{x00A3}0\\) and has \\(\\unicode{x00A3}2\\cdot \\frac{5}{4}= \\unicode{x00A3}2.5\\) in the riskless asset. She must pay \\(\\unicode{x00A3}0.5 \\cdot 2= \\unicode{x00A3}1\\) to cover the short position in the stock, which leaves her with \\(\\unicode{x00A3}1.5\\) as desired."
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_8.html#exercise-35-monte-carlo-approximation-of-european-put-option",
    "href": "Documents_files/ME200/Solutions_Assignment_8.html#exercise-35-monte-carlo-approximation-of-european-put-option",
    "title": "Solutions to Assignment 8",
    "section": "Exercise 35: (Monte Carlo approximation of European put option)",
    "text": "Exercise 35: (Monte Carlo approximation of European put option)\nIn the following we will be using the method choice available in numpy.random to generate a sample from a discrete probability distribution. See here:\nhttps://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html\n\nimport numpy as np\nfrom scipy.special import comb\n\n\nrng = np.random.default_rng(12345)\n\n\ndef generateriskneutralstocks(S0, u, d, r, samplesize): \n    ptilde = (1 + r - d) / (u - d)\n    stocks =  rng.choice(np.array([S0 * u, S0 * d]), size = samplesize, replace = True, p=np.array([ptilde, 1-ptilde]))\n    return(stocks)\n\ndef putpriceMC(S0, u, d, r, samplesize, K):\n    mystocks = generateriskneutralstocks(S0, u, d, r, samplesize)\n    payoffs= np.maximum(K - mystocks, 0) / (1 + r)\n    return(payoffs.mean())\n\nprint('MC price of put in binomial model is: {:.4f}'.format(putpriceMC(S0=4, u=2, d=0.5, r=0.25, samplesize = 10000, K = 5)))\n\nMC price of put in binomial model is: 1.1822"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_8.html#exercise-36-multi-period-binomial-model",
    "href": "Documents_files/ME200/Solutions_Assignment_8.html#exercise-36-multi-period-binomial-model",
    "title": "Solutions to Assignment 8",
    "section": "Exercise 36: (Multi-period binomial model)",
    "text": "Exercise 36: (Multi-period binomial model)\n\ndef europeanput_binomial(S0, K, r, N, u, d):\n    ptilde = (1 + r - d) / (u - d)\n    myprice = 0\n    for k in range(0, N + 1):\n        stockprice = S0 * (u ** k) * (d ** (N-k))\n        myprice = myprice + \\\n            comb(N, k) * (ptilde ** k) * ((1-ptilde) ** (N-k)) *\\\n                np.maximum(K - stockprice, 0)\n    result = myprice / ((1+r) ** N)\n    return result\n\nprint('Analytical price of put in binomial model is: {:.4f}'.format( \n      europeanput_binomial(S0=4, u=2, d=0.5, r=0.25, N=1, K=5)))\n\nAnalytical price of put in binomial model is: 1.2000\n\n\n\ndef europeancall_binomial(S0, K, r, N, u, d):\n    ptilde = (1 + r - d) / (u - d)\n    myprice = 0\n    for k in range(0, N + 1):\n        stockprice = S0 * (u ** k) * (d ** (N-k))\n        myprice = myprice + \\\n            comb(N, k) * (ptilde ** k) * ((1-ptilde) ** (N-k)) *\\\n                np.maximum(stockprice - K, 0)\n    result = myprice / ((1+r) ** N)\n    return result\n\nprint('Analytical price of call in binomial model is: {:.4f}'.format( \n      europeancall_binomial(S0=4, u=2, d=0.5, r=0.25, N=1, K=5)))\n\nAnalytical price of call in binomial model is: 1.2000\n\n\nThe put-call parity in the binomial model is given by \\(C_0 - P_0 = S_0 - K/(1+r)^N\\) where \\(C_0, P_0\\) denote the price of the European call and put option, respectively. Let us use this to check our implementation:\n\nS0 = 100\nK = 100\nu = 3\nd = 1 / 3\nr = 2 / 3\nN = 5\n\ntest = S0 - K / (1 + r) ** N\n\ncall = europeancall_binomial(S0=100, u=3, d=1/3, r=2/3, N=5, K=100) \nput = europeanput_binomial(S0=100, u=3, d=1/3, r=2/3, N=5, K=100) \n\nprint('call - put = {:.4f}'.format(call - put))\nprint('S0 - K/(1+r)^N= {:.4f}'.format(test))\n\ncall - put = 92.2240\nS0 - K/(1+r)^N= 92.2240"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_6.html",
    "href": "Documents_files/ME200/Solutions_Assignment_6.html",
    "title": "Solutions to Assignment 6",
    "section": "",
    "text": "ME200, Johannes Ruf and Luitgard Veraart"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_6.html#exercise-28-dice",
    "href": "Documents_files/ME200/Solutions_Assignment_6.html#exercise-28-dice",
    "title": "Solutions to Assignment 6",
    "section": "Exercise 28 – dice",
    "text": "Exercise 28 – dice\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nsample_size = 500\n\nrng = np.random.default_rng(seed=223344)\nx = rng.choice(np.arange(1, 7), size=sample_size)\n\n\nxbar = np.cumsum(x) / np.arange(1, sample_size + 1)     # rolling average\n\n\nfig, ax = plt.subplots()\nax.plot(np.arange(1, sample_size + 1), xbar)\nax.set_ylim([2, 5])\nax.hlines(3.5, xmin=1, xmax=sample_size, color='red')\nax.set_title('Plot of xbar against the number of samples');\n\n\n\n\nThe sample average tends to expected value 3.5 as \\(k\\) increases."
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_6.html#exercise-29",
    "href": "Documents_files/ME200/Solutions_Assignment_6.html#exercise-29",
    "title": "Solutions to Assignment 6",
    "section": "Exercise 29",
    "text": "Exercise 29\n\\[\n{\\rm Var} \\left( \\bar{X}_n \\right) = \\frac{1}{n^2} \\sum _{i=1}^n {\\rm Var}\n(X_i) =  \\frac{1}{n} {\\rm Var}  (X) .\n\\]\nI.i.d. stands for ``independent and identically distributed’’."
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_6.html#exercise-30",
    "href": "Documents_files/ME200/Solutions_Assignment_6.html#exercise-30",
    "title": "Solutions to Assignment 6",
    "section": "Exercise 30",
    "text": "Exercise 30\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef h(x):\n    return np.cos(np.exp(3 * x))\n\n\nsample_size = 5_000_000\n\nrng = np.random.default_rng(seed=334411)\nx = h(rng.random(size=sample_size))\n\nprint('The Monte Carlo approximation of the integral is {:.4f}.'.format(np.mean(x)))\n\nThe Monte Carlo approximation of the integral is -0.0968.\n\n\n\nburn_in_samples = 1000\n\nfig, ax = plt.subplots()\nrunning_average = np.cumsum(x) / np.arange(1, sample_size + 1)\nax.plot(np.arange(burn_in_samples, sample_size + 1), running_average[burn_in_samples - 1: ])\nax.hlines(-0.097137, xmin=1, xmax=sample_size, color='red')\nax.set_title('Plot of xbar against the number of samples');\n\n\n\n\n\nsample_size = 100_000_000\ntotal = 0\nfor i in range(sample_size):\n    x = np.random.uniform(low=-1, high=1, size=1)\n    y = np.random.uniform(low=-1, high=1, size=1)\n    total += x**2+y**2&lt;=1\nprint(4*total/sample_size)\n\n[3.14138448]"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_4.html",
    "href": "Documents_files/ME200/Solutions_Assignment_4.html",
    "title": "Solutions to Assignment 4",
    "section": "",
    "text": "ME200, Johannes Ruf and Luitgard Veraart"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_4.html#exercise-18",
    "href": "Documents_files/ME200/Solutions_Assignment_4.html#exercise-18",
    "title": "Solutions to Assignment 4",
    "section": "Exercise 18",
    "text": "Exercise 18\nPart 1: The output of this code will print 1 and -1.\nPart 2: The output of this code will print 1."
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_4.html#exercise-19",
    "href": "Documents_files/ME200/Solutions_Assignment_4.html#exercise-19",
    "title": "Solutions to Assignment 4",
    "section": "Exercise 19",
    "text": "Exercise 19\nLet \\(F\\) denote the CDF of the exponential distribution with parameter \\(\\mu\\). From the lecture, we have\n\\[\nF^{-1} (u) = - \\frac{\\ln(1-u)} {\\mu} .\n\\]\nAdapting the code from the lecture, say for \\(\\mu = \\frac{1}{10}\\), we have:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nmu = 1/10\nsample_size = 100_000\n\n\n#rng = np.random.default_rng(seed=12345) \n#u = rng.random(size=sample_size)\n\nnp.random.seed(12345) \nu = np.random.random(size=sample_size)\nsamples = -np.log(u) / mu\n\n\n#samples_numpy = rng.exponential(size=sample_size, scale=1/mu)\nsamples_numpy = np.random.exponential(size=sample_size, scale=1/mu)\n\n\n_, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 8))\naxs[0].hist(samples, bins=1000, density=True);\naxs[0].set_title('Histogram of random numbers generated by inverse transform method')\naxs[1].hist(samples_numpy, bins=1000, density=True);\naxs[1].set_title('Histogram of random numbers generated by NumPy directly');\nfor ax in axs:\n    ax.set_ylim([0, 0.12]);\n    ax.set_xlim([0, 100]);"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_4.html#exercise-20",
    "href": "Documents_files/ME200/Solutions_Assignment_4.html#exercise-20",
    "title": "Solutions to Assignment 4",
    "section": "Exercise 20",
    "text": "Exercise 20\nTo determine \\(c\\), let us observe that\n\\[\n  1 =  \\sum_{i = 1}^4  P[X = i] = c \\sum_{i = 1}^4 i = 10 c.\n\\]\nHence, \\(c = 0.1\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nsample_size = 1000\nc = 0.1\nsamples = np.array([])   # empty array\n\n\n#rng = np.random.default_rng(seed=443322)\nnp.random.seed(443322)\n\nfor i in range(sample_size):\n    #u = rng.random()\n    u = np.random.random()\n    if u &lt; c:\n        tmp = 1\n    elif u &lt; 3 * c:\n        tmp = 2\n    elif u &lt; 6 * c:\n        tmp = 3\n    else:\n        tmp = 4\n        \n    samples = np.append(samples, tmp)\n\n\nfig, ax = plt.subplots()\nax.hist(samples, bins=100);"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_4.html#exercise-21",
    "href": "Documents_files/ME200/Solutions_Assignment_4.html#exercise-21",
    "title": "Solutions to Assignment 4",
    "section": "Exercise 21",
    "text": "Exercise 21\nFirst, we need to determine the CDF of the logistic distribution. Using substitution, we get\n\\[\n  F(x) = \\int_{-\\infty}^x f(t) dt = - \\int_{\\infty}^{\\mathrm{e}^{-x}} \\frac{1}{(1+u)^2} du\n    = \\frac{1}{1 + \\mathrm{e}^{-x}}.\n\\]\nComputing the inverse \\(F^{-1}\\), we get\n\\[\nF^{-1}(u) = \\log\\left(\\frac{u}{1-u}\\right).\n\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nsample_size = 10000\n\n\n#rng = np.random.default_rng(seed=1111)\n#u = rng.random(size=sample_size)\n\nnp.random.seed(1111)\nu = np.random.random(size=sample_size)\n\nsamples = np.log(u / (1-u))\n\n\nfig, ax = plt.subplots()\nax.hist(samples, bins=100, density=True);"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_4.html#exercise-22",
    "href": "Documents_files/ME200/Solutions_Assignment_4.html#exercise-22",
    "title": "Solutions to Assignment 4",
    "section": "Exercise 22",
    "text": "Exercise 22\n\nimport numpy as np\n\n\ndef fib(n):\n    if n == 1:\n        return np.array([1])\n    if n == 2:\n        return np.array([1, 1])\n    \n    numbers = np.array([1, 1])\n    \n    for i in range(3, n+1):\n        numbers = np.append(numbers, numbers[-1] + numbers[-2])\n    return numbers\n\n\nfib(15)\n\narray([  1,   1,   2,   3,   5,   8,  13,  21,  34,  55,  89, 144, 233,\n       377, 610])"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_11.html",
    "href": "Documents_files/ME200/Solutions_Assignment_11.html",
    "title": "Solutions to Assignment 11",
    "section": "",
    "text": "ME200, Johannes Ruf and Luitgard Veraart"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_11.html#exercise-42-black-scholes-option-pricing-formula---european-put-option",
    "href": "Documents_files/ME200/Solutions_Assignment_11.html#exercise-42-black-scholes-option-pricing-formula---european-put-option",
    "title": "Solutions to Assignment 11",
    "section": "Exercise 42: (Black-Scholes option pricing formula - European put option)",
    "text": "Exercise 42: (Black-Scholes option pricing formula - European put option)\nLet \\(X \\sim \\textrm{N}(0, 1)\\). Then,\n\\[\\begin{align*}\n\\textrm{E}\\left[ e^{-rT} (K-S_T)^+ \\right] &=\n\\textrm{E}\\left[ e^{-rT}(K-S_0 \\exp \\left( (r - \\frac{\\sigma^2}{2})T + \\sigma \\sqrt{T} X \\right))^+\\right]\\\\\n&= \\int_{-\\infty}^{+\\infty} e^{-rT} (K - S_0 \\exp\\left( (r - \\frac{\\sigma^2}{2})T + \\sigma \\sqrt{T} x\\right))^+ \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx=(*).\n\\end{align*}\\] Now observe that \\[\\begin{align*}\nK \\geq S_0 \\exp\\left(   (r - \\frac{\\sigma^2}{2})T + \\sigma \\sqrt{T} x\\right)\n\\Longleftrightarrow\nx \\leq - \\frac{\\log \\left(\\frac{S_0}{K} \\right) + \\left(r - \\frac{\\sigma^2}{2}\\right)T}{\\sigma \\sqrt{T}}=-(D_1 - \\sigma \\sqrt{T})=-D_1 + \\sigma \\sqrt{T}.\n\\end{align*}\\] Hence, \\[\\begin{align*}\n(*)&= \\int_{-\\infty}^{-D_1+ \\sigma \\sqrt{T}} e^{-rT} (K-S_0 \\exp\\left( (r - \\frac{\\sigma^2}{2})T + \\sigma \\sqrt{T} x\\right)) \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx\\\\\n&=\ne^{-rT} K \\int_{-\\infty}^{-D_1+ \\sigma \\sqrt{T}}  \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx -\ne^{-rT} S_0 \\int_{-\\infty}^{-D_1+ \\sigma \\sqrt{T}}  \\exp\\left( (r - \\frac{\\sigma^2}{2})T + \\sigma \\sqrt{T} x\\right) \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx.\n\\end{align*}\\]\nWe now look at the two integrals separately. We start with the first one: \\[\\begin{align*}\n\\int_{-\\infty}^{-D_1+ \\sigma \\sqrt{T}} \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx\n=\\Phi(-D_1 + \\sigma \\sqrt{T}).\n\\end{align*}\\]\nNext we evaluate the second integral:\n\\[\\begin{align*}\n&\\int_{-\\infty}^{-D_1+ \\sigma \\sqrt{T}}  \\exp\\left( (r - \\frac{\\sigma^2}{2})T + \\sigma \\sqrt{T} x\\right) \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx\\\\\n&= \\exp\\left( (r - \\frac{\\sigma^2}{2})T \\right)\n\\int_{-\\infty}^{-D_1+ \\sigma \\sqrt{T}}  \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left( -\\frac{1}{2} \\left( x^2 -2\\sigma \\sqrt{T} x + \\sigma^2 T - \\sigma^2 T \\right) \\right) dx\\\\\n&=\\exp\\left( (r - \\frac{\\sigma^2}{2})T  + \\frac{\\sigma^2T}{2}\\right)\n\\int_{-\\infty}^{-D_1+ \\sigma \\sqrt{T}} \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left( -\\frac{1}{2} \\left( x -\\sigma \\sqrt{T}) \\right)^2 \\right)dx\\\\\n&=e^{rT} \\Phi(-D_1+\\sigma \\sqrt{T}-\\sigma \\sqrt{T})\\\\\n&=e^{rT} \\Phi(-D_1).\n\\end{align*}\\]\nCombining these results, gives \\[\\begin{align*}\n\\textrm{E}\\left[ e^{-rT} (K-S_T)^+ \\right]\n=  K e^{-rT} \\Phi(-D_1 +\\sigma \\sqrt{T}) - S_0 \\Phi(-D_1).\n\\end{align*}\\]"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_11.html#exercise-43-variance-reduction-techniques-for-option-pricing",
    "href": "Documents_files/ME200/Solutions_Assignment_11.html#exercise-43-variance-reduction-techniques-for-option-pricing",
    "title": "Solutions to Assignment 11",
    "section": "Exercise 43: (Variance reduction techniques for option pricing)",
    "text": "Exercise 43: (Variance reduction techniques for option pricing)\n\nA Monte Carlo estimator for the European put option with maturity \\(T\\) and strike \\(K\\) is given by\n\n\\[\\begin{align*}\nV^{\\textrm{MC}}_0(n) = \\frac{1}{n} \\sum_{i=1}^n e^{-rT} \\left(K - S_0 \\exp\\left( (r - \\frac{\\sigma^2}{2}) T + \\sigma \\sqrt{T} X_i \\right) \\right)^+,\n\\end{align*}\\] where \\(X_1, \\ldots, X_n\\) are i.i.d. from the \\(\\mathcal{N}(0, 1)\\) distribution.\n\nSince for \\(X \\sim \\mathcal{N}(0, 1)\\), \\((X, -X)\\) is an antithetic pair, an antithetic variates estimator for the time-0 price of a European put option is given by \\[\\begin{align*}\nV^{\\textrm{MC}}_0(n) &= \\frac{1}{2n} \\sum_{i=1}^n e^{-rT} \\left(K - S_0 \\exp\\left( (r - \\frac{\\sigma^2}{2}) T + \\sigma \\sqrt{T} X_i \\right) \\right)^+\\\\\n& \\quad\n+\\frac{1}{2n} \\sum_{i=1}^n e^{-rT} \\left(K - S_0 \\exp\\left( (r - \\frac{\\sigma^2}{2}) T + \\sigma \\sqrt{T} (-X_i) \\right) \\right)^+\n,\n\\end{align*}\\] where \\(X_1, \\ldots, X_n\\) are i.i.d. from the \\(\\mathcal{N}(0, 1)\\) distribution.\nLet \\(X_i\\) be i.i.d. random variables from the standard normal distribution and let \\(S_i(T)=S_0 \\exp\\left( (r - \\frac{\\sigma^2}{2}) T + \\sigma \\sqrt{T} X_i \\right)\\) and \\(Y_i = e^{-rT} (K - S_i)^+\\). Then a control variate estimator for the time-0 price of a European put option is given by \\[\\begin{align*}\n\\overline{Y}_n(b) = \\frac{1}{n} \\sum_{i=1}^n (Y_i - b(e^{-r T}S_i(T) - S(0))),  \n\\end{align*}\\] where \\(b\\) would often be chosen to be \\(\\hat{b}_n^*\\) as defined in the lecture notes.\n\nWe will implement the different estimators in Python next.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nrng = np.random.default_rng(2468)\n\n\nAnalytical formula and standard Monte Carlo estimator for the European put price\nFirst, we implement the analytical formula for the time-\\(0\\) price of the European put in the Black-Scholes model and a classical Monte Carlo estimator.\n\ndef black_scholes_put(S0, K, r, T, sigma):\n    d1 = (np.log(S0 / K) + (r + 0.5 * sigma**2) * T)/(sigma * np.sqrt(T))\n    d2 = d1 - sigma * np.sqrt(T)\n    tmp1 = S0 * norm.cdf(-d1, loc=0, scale=1)\n    tmp2 = K * np.exp(-r * T) * norm.cdf(-d2, loc=0, scale=1)\n    price = tmp2 - tmp1\n    return price\n\ndef terminal_stockprice(rng, S0, T, r, sigma, samplesize):\n    mystandardnormalsample = rng.standard_normal(size=samplesize)\n    tmp1 = (r - 0.5*(sigma ** 2)) * T\n    tmp2 = sigma * np.sqrt(T) * mystandardnormalsample\n    stockprice = S0 * np.exp(tmp1 + tmp2)\n    return stockprice\n\ndef bs_put_mc(rng, S0, K, T, r, sigma, samplesize, myepsilon):\n    # Generate terminal stock prices.\n    mystockprice = terminal_stockprice(rng, S0, T, r, sigma, samplesize)\n    # Compute payoffs.\n    payoffs = np.maximum(K - mystockprice, 0)\n    # Discount payoffs\n    discountedpayoffs = np.exp(- r * T)*payoffs\n    # Compute MC price\n    price = np.mean(discountedpayoffs)\n    # Compute confidence interval next\n    standarddev_rv = np.std(discountedpayoffs, ddof=1)\n    standarddev_mcest = standarddev_rv / np.sqrt(samplesize)\n    aepsilon = norm.ppf(1.0 - myepsilon * 0.5)\n    # Left boundary of CI\n    ci_left = price - aepsilon * standarddev_mcest\n    # Right boundary of CI\n    ci_right = price + aepsilon * standarddev_mcest\n    return price, standarddev_mcest, ci_left, ci_right\n\n\n# Defining some model parameters\nS0 = 50.0\nK = 50.0\nT = 0.25\nr = 0.05\nsigma = 0.3\nhalfsamplesize = 100000\nsamplesize = 2 * halfsamplesize\nmyepsilon = 0.05\n\n\nprint('----------------')\nprint('The analytical option price of the put is {:.4f}'.format(black_scholes_put(S0, K, r, T, sigma)))\nprint('-----------------')\nMCresults = bs_put_mc(rng, S0, K, T, r, sigma, samplesize, myepsilon)\nprint('MC put price: {:.4f} and stdev of MC est: {:.4f}'.format(MCresults[0],MCresults[1]))\nprint('CI based on MC is (\"{:.4f}, {:.4f})'.format(MCresults[2], MCresults[3]))\n\n----------------\nThe analytical option price of the put is 2.6704\n-----------------\nMC put price: 2.6776 and stdev of MC est: 0.0085\nCI based on MC is (\"2.6610, 2.6943)\n\n\n\n\nAntithetic variates estimator for the European put price\nSecond, we implement an antithetic variates estimator for the time-\\(0\\) price of the European put.\n\ndef terminal_stockprice_av(rng, S0, T, r, sigma, halfsamplesize):\n    \"\"\"Function computes terminal stock prices based on antithetic pairs. \"\"\"\n    mynormals1 = rng.standard_normal(halfsamplesize)\n    mynormals2 = - mynormals1\n    tmp1 = (r - 0.5*sigma ** 2) * T\n    tmp2 = sigma * np.sqrt(T) * mynormals1\n    tmp3 = sigma * np.sqrt(T) * mynormals2\n    stockprice1 = S0 * np.exp(tmp1 + tmp2)\n    stockprice2 = S0 * np.exp(tmp1 + tmp3)\n    allstockprices = np.concatenate((stockprice1, stockprice2))\n    return stockprice1, stockprice2, allstockprices\n\n\ndef bs_put_av(rng, S0, K, T, r, sigma, halfsamplesize, myepsilon):\n    \"\"\" Antithethic variate estimation for European call price in BS model.\"\"\"\n    # Note that 2*halfsamplesize random variables will be used in the AV estimator.\n    # Generate terminal stock prices.\n    mystockprices = terminal_stockprice_av(rng, S0, T, r, sigma, halfsamplesize)\n    # Compute payoffs.\n    payoffs1 = np.maximum(K - mystockprices[0], 0)\n    payoffs2 = np.maximum(K - mystockprices[1], 0)\n    # Discount payoffs\n    discpayoffs1 = np.exp(- r * T)*payoffs1\n    discpayoffs2 = np.exp(- r * T)*payoffs2\n    thecov = np.cov(discpayoffs1, discpayoffs2, ddof=1)[0, 1]\n    possiblereduction = thecov / (2 * halfsamplesize)\n    discpayoffs = np.concatenate((discpayoffs1, discpayoffs2))\n    price = np.mean(discpayoffs)\n    standarddev_rv = np.std(discpayoffs, ddof=1)\n    standarddev_avest = standarddev_rv / np.sqrt(2 * halfsamplesize)\n    aepsilon = norm.ppf(1.0 - myepsilon * 0.5)\n    ci_left = price - aepsilon * standarddev_avest\n    ci_right = price + aepsilon * standarddev_avest\n    return price, standarddev_avest, ci_left, ci_right, possiblereduction\n\nprint('----------------')\nAVresults = bs_put_av(rng, S0, K, T, r, sigma, halfsamplesize, myepsilon)\nprint('AV price: {:.4f} and stdev of AV est: {:.4f}'.format(AVresults[0], AVresults[1]))\nprint('CI based on AV is ({:.4f}, {:.4f})'.format(AVresults[2], AVresults[3]))\nprint('Note that sample covariance/(2n) is {:.8f}'.format(AVresults[4]))\n\n----------------\nAV price: 2.6594 and stdev of AV est: 0.0085\nCI based on AV is (2.6428, 2.6760)\nNote that sample covariance/(2n) is -0.00003536\n\n\n\n\nControl variates estimator for the European put price\nThird, we implement a control variate estimator for the time-\\(0\\) price of the European put in the Black-Scholes model.\n\ndef bs_put_cv(rng, S0, K, T, r, sigma, samplesize, myepsilon):\n    \"\"\" Control variate estimation for European put price in BS model.\"\"\"\n    # Generate terminal stock prices.\n    mystockprice = terminal_stockprice(rng, S0, T, r, sigma, samplesize)\n    # Compute payoffs.\n    payoffs = np.maximum(K - mystockprice, 0)\n    # Discount payoffs\n    discountedpayoffs = np.exp(- r * T)*payoffs\n    # Use discounted stock as control\n    xs = np.exp(- r * T) * mystockprice\n    # Compute sample version of b*\n    bstar = np.cov(xs, discountedpayoffs, ddof=1)[0, 1] / np.var(xs, ddof=1)\n    # print(\"In cv bstar=\", bstar)\n    # Define z= Y(bstar)\n    z = discountedpayoffs - bstar * (xs - S0)\n    # Compute MC price\n    price = np.mean(z)\n    # Compute confidence interval next\n    standarddev_rv = np.std(z, ddof=1)\n    standarddev_cvest = standarddev_rv / np.sqrt(samplesize)\n    aepsilon = norm.ppf(1.0 - myepsilon * 0.5)\n    # Left boundary of CI\n    ci_left = price - aepsilon * standarddev_cvest\n    # Right boundary of CI\n    ci_right = price + aepsilon * standarddev_cvest\n    # Compute the sqared correation rhosquared\n    tmpcov = np.cov(xs, discountedpayoffs, ddof=1)[0, 1]\n    tmpvarx = np.var(xs, ddof=1)\n    tmpvary = np.var(discountedpayoffs, ddof=1)\n    rhosquared = (tmpcov ** 2) / (tmpvarx * tmpvary)\n    return price, standarddev_cvest, ci_left, ci_right, rhosquared\n\nprint('----------------')\nCVresults = bs_put_cv(rng, S0, K, T, r, sigma, samplesize, myepsilon)\nprint('CV price: {:.4f} and stdev of CV est: {:.4f}'.format(CVresults[0], CVresults[1]))\nprint('CI based on CV is (\"{:.4f}, {:.4f})'.format(CVresults[2], CVresults[3]))\nprint('Note that rhosquared is {:.4f}'.format(CVresults[4]))\n\n----------------\nCV price: 2.6669 and stdev of CV est: 0.0050\nCI based on CV is (\"2.6572, 2.6766)\nNote that rhosquared is 0.6570\n\n\n\n\nSensitivity of the variances of the estimators of the time-0 put option price with respect to the strike price\n\n# Next we compare the standarddeviation of the MC, AV and the CV estimator\n# Defining some model parameters\nS0 = 50.0\nsigma = 0.3\nT = 1.0\nr = 0.05\nsigma = 0.3\nhalfsamplesize = 10000 \nsamplesize = 2 * halfsamplesize\nmyepsilon = 0.05\n\nnumberofK = 100\nKs = np.linspace(start=1, stop=120, num=numberofK)\n\nstdMC = np.zeros(numberofK)\nstdCV = np.zeros(numberofK)\nstdAV = np.zeros(numberofK)\nanalyticalprice = np.zeros(numberofK)\n\n# Generate the terminal stock prices\nmystockprice = terminal_stockprice(rng, S0, T, r, sigma, samplesize)\nmyavstockprices = terminal_stockprice_av(rng, S0, T, r, sigma, halfsamplesize)\nmyavstockprice = myavstockprices[2]  \n\nfor i in range(numberofK):\n    K = Ks[i]\n    # Compute discounted payoffs\n    discountedpayoffs = np.exp(- r * T) * np.maximum(K-mystockprice, 0)\n    # Compute standard dev of MC estimator \n    stdMC[i] = np.std(discountedpayoffs, ddof=1) / np.sqrt(samplesize)\n\n    # Use discounted stock as control\n    xs = np.exp(- r * T) * mystockprice\n    # Compute sample version of b*\n    bstar = np.cov(xs, discountedpayoffs, ddof=1)[0, 1] / np.var(xs, ddof=1)\n    # Define z= Y(bstar)\n    z = discountedpayoffs - bstar * (xs - S0)\n    # Compute standard dev of CV estimator \n    stdCV[i] = np.std(z, ddof=1) / np.sqrt(samplesize)\n       \n    #AV: \n    mystockprice = myavstockprice\n    discountedpayoffs = np.exp(- r * T) *  np.maximum(K-mystockprice, 0)\n    # Compute standard dev of AV estimator \n    stdAV[i] = np.std(discountedpayoffs, ddof=1) / np.sqrt(samplesize)\n    \n    # Compute the analytical put price for different strikes as well\n    analyticalprice[i] = black_scholes_put(S0, K, r, T, sigma)\n    \n\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\nax1.plot(Ks, stdMC, lw=3, c=\"blue\", label=\"Std of MC estimator\")\nax1.plot(Ks, stdCV, lw=3, c=\"red\", label=\"Std of CV estimator\", linestyle = \"dashed\")\nax1.plot(Ks, stdAV, lw=3, c=\"green\", label=\"Std of AV estimator\", linestyle = \"dashed\")\nax1.set_ylabel(\"Standard deviation\")\nax1.set_xlabel(r\"$K$\")\nax1.set_title(\"Comparison of standard deviations\")\nax1.legend(loc = \"best\")\n\nax2.plot(Ks, analyticalprice, lw=3, c=\"blue\", label=\"analytical put price\")\nax2.set_ylabel(\"Put price\")\nax2.set_xlabel(r\"$K$\")\nax2.set_title(\"Time - 0 put price\")\nax2.legend(loc = \"best\")\nplt.show()"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_2.html",
    "href": "Documents_files/ME200/Solutions_Assignment_2.html",
    "title": "Solutions to Assignment 2",
    "section": "",
    "text": "ME200, Johannes Ruf and Luitgard Veraart"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_2.html#exercise-6",
    "href": "Documents_files/ME200/Solutions_Assignment_2.html#exercise-6",
    "title": "Solutions to Assignment 2",
    "section": "Exercise 6",
    "text": "Exercise 6\nGiven an experiment with sample space \\(S\\), a random variable (r.v.) is a function from the sample space \\(S\\) to the real numbers.\nA random variable \\(X\\) is said to be discrete if there is a finite list of values \\(a_1,a_2,\\cdots,a_n\\) or an infinite list of values \\(a_1,a_2,\\cdots\\) such that\n\\[P[X = a_j  \\text{ for some } j] = 1.\\]\nAn r.v. has a continuous distribution if its CDF is differentiable."
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_2.html#exercise-7",
    "href": "Documents_files/ME200/Solutions_Assignment_2.html#exercise-7",
    "title": "Solutions to Assignment 2",
    "section": "Exercise 7",
    "text": "Exercise 7\nNo. For example, let \\(X\\) and \\(Y\\) be independent and set \\(Z = X\\)."
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_2.html#exercise-8",
    "href": "Documents_files/ME200/Solutions_Assignment_2.html#exercise-8",
    "title": "Solutions to Assignment 2",
    "section": "Exercise 8",
    "text": "Exercise 8\n\nLet \\(Y\\) be uniformly distributed on \\([0,1]\\) and set \\(X = Y-1\\). Then\n\\[\n  P\\left[\\left\\{Y &gt; \\frac{1}{2}\\right\\} \\cap \\left\\{X &gt; \\frac{-1}{2}\\right\\} \\right] = \\frac{1}{2} \\neq \\frac{1}{4} =\n  P\\left[\\left\\{Y &gt; \\frac{1}{2}\\right\\}\\right]  P\\left[ \\left\\{X &gt; \\frac{-1}{2}\\right\\} \\right].\n\\]\nLet \\(X = 0\\) and \\(Y = 1\\) be deterministic."
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_2.html#exercise-9",
    "href": "Documents_files/ME200/Solutions_Assignment_2.html#exercise-9",
    "title": "Solutions to Assignment 2",
    "section": "Exercise 9",
    "text": "Exercise 9\nWrite \\(G\\) for the CDF of \\(Y\\). Then\n\\[\n    G(y) = P[Y \\leq y] = P[\\mu + \\sigma X \\leq y] = P\\left[X \\leq \\frac{y - \\mu}{\\sigma}\\right]\n      = F\\left(\\frac{y - \\mu}{\\sigma}\\right).\n\\]\n## Exercise 10\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nsample_size1 = 100\nsample_size2 = 1000\nsample_size3 = 100000\n\n\nsamples1 = np.random.random(size=sample_size1)\nsamples2 = np.random.random(size=sample_size2)\nsamples3 = np.random.random(size=sample_size3)\n\n\nfig, ax = plt.subplots(ncols=3, figsize=(20, 7))    # puts three plots next to each other; play with the code\n\nax[0].hist(samples1, bins=10)\nax[0].set_title(f\"Histogram: sample size = {sample_size1}\")\nax[0].set_ylabel(f\"Frequency\");\n\nax[1].hist(samples2, bins=30)\nax[1].set_title(f\"Histogram: sample size = {sample_size2}\")\nax[1].set_ylabel(\"Frequency\");\n\nax[2].hist(samples3, bins=100)\nax[2].set_title(f\"Histogram: sample size = {sample_size3}\")\nax[2].set_ylabel(\"Frequency\");\n\n\n\n\nOf course, you were not expected to do such a complicated plot. But have a look at the code to understand how one put several plots next to each other.\nAs we increase the sample size, the histograms look more like the PDF of a uniform (namely constant on [0,1])."
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_2.html#exercise-11",
    "href": "Documents_files/ME200/Solutions_Assignment_2.html#exercise-11",
    "title": "Solutions to Assignment 2",
    "section": "Exercise 11",
    "text": "Exercise 11\n\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\n\nmu1 = 0.5\nmu2 = 2\n\n\nx = np.linspace(0, 20, 1000)\npdf1 = stats.expon.pdf(x, scale=1/mu1)    \npdf2 = stats.expon.pdf(x, scale=1/mu2)\n# Note that for the scale we have to take the reciprocal of the parameter.\n\n\nfig, ax = plt.subplots(figsize=(7, 5))    \n\nax.plot(x, pdf1, label=f'Exponential with mu = {mu1}')\nax.plot(x, pdf2, color='red', label=f'Exponential with mu = {mu2}')\nax.set_title(\"PDFs of exponentially distributed random variables\");\nax.legend();\n\n\n\n\nIn the previous plot we put in two PDFs. Have a look how this was done, and how the legend was introduced."
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_2.html#exercise-12",
    "href": "Documents_files/ME200/Solutions_Assignment_2.html#exercise-12",
    "title": "Solutions to Assignment 2",
    "section": "Exercise 12",
    "text": "Exercise 12\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nmu = 2\n\n\nsample_size = 10000\n\n\ny = np.random.exponential(size=sample_size, scale=1/mu)\ndu = np.random.choice([-1, 1], size=sample_size)\n\nsamples = du * y\n\n\nfig, ax = plt.subplots(figsize=(7, 5))\nax.hist(samples, bins=100);\nax.set_title('Histogram of double exponentially distributed r.v.s');"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_5.html",
    "href": "Documents_files/ME200/Solutions_Assignment_5.html",
    "title": "Solutions to Assignment 5",
    "section": "",
    "text": "ME200, Johannes Ruf and Luitgard Veraart"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_5.html#exercise-23",
    "href": "Documents_files/ME200/Solutions_Assignment_5.html#exercise-23",
    "title": "Solutions to Assignment 5",
    "section": "Exercise 23",
    "text": "Exercise 23\nWe would like to sample from a target distribution which has corresponding probability density \\(f\\). Suppose there is a density \\(g\\) from which we know how to generate samples from and for which\n\\[\nf(x) \\leq c g(x)  \\quad \\textrm{ for all } x \\in  R,\n\\]\nfor a constant \\(c\\).\nThen, the Von Neumann’s acceptance-rejection algorithm is given by * Generate \\(X\\) from the density \\(g\\). * Generate \\(U \\sim {\\rm Unif}(0, 1)\\). * If $ U $, then accept \\(X\\) and return it. Otherwise, go back to step 1."
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_5.html#exercise-24",
    "href": "Documents_files/ME200/Solutions_Assignment_5.html#exercise-24",
    "title": "Solutions to Assignment 5",
    "section": "Exercise 24",
    "text": "Exercise 24\nIf \\(X\\) is a standard normally distributed random variable then \\(Y = \\sigma * X + \\mu\\) is \\(\\mathcal{N}(\\mu, \\sigma^2)\\)-distribued."
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_5.html#exercise-25",
    "href": "Documents_files/ME200/Solutions_Assignment_5.html#exercise-25",
    "title": "Solutions to Assignment 5",
    "section": "Exercise 25",
    "text": "Exercise 25\nRecall from the lecture notes that we reject a sample if\n\\[\nU &gt; e^{-\\frac{(|X|-1)^2}{2}} ,\n\\]\nwhere \\(U\\) is uniformly distributed and \\(X\\) is doubly exponentially distributed.\nLet’s first simulate doubly exponential samples as in Exercise 12, but now with a seeded random number generator:\n\nimport numpy as np\n\n\nrng = np.random.default_rng(seed=998877)\n\n\nsample_size = 100_000\n\n\nnormal_samples = []\nfor _ in range(sample_size):\n    u = rng.random()\n    \n    y = rng.exponential()   \n    du = rng.choice([-1, 1])\n    cand = du * y\n    \n    while u &gt; np.exp(-(abs(cand) - 1)**2 / 2):\n        u = rng.random()\n\n        y = rng.exponential()   \n        du = rng.choice([-1, 1])\n        cand = du * y\n    normal_samples.append(cand)\n\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots()\nax.hist(normal_samples, bins=100);\nax.set_title('Histogram of normally distributed r.v.s (via acceptance-rejection)');"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_5.html#exercise-26-fibonacci-part-ii",
    "href": "Documents_files/ME200/Solutions_Assignment_5.html#exercise-26-fibonacci-part-ii",
    "title": "Solutions to Assignment 5",
    "section": "Exercise 26: Fibonacci, part II",
    "text": "Exercise 26: Fibonacci, part II\n\nimport numpy as np\n\n\ndef fib2(k):\n    numbers = np.array([1, 1])\n    \n    while numbers[-2] + numbers[-1] &lt;= k:\n        numbers = np.append(numbers, numbers[-1] + numbers[-2])\n    return numbers\n\n\nfib2(1000)\n\narray([  1,   1,   2,   3,   5,   8,  13,  21,  34,  55,  89, 144, 233,\n       377, 610, 987])"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_5.html#exercise-27-box-muller",
    "href": "Documents_files/ME200/Solutions_Assignment_5.html#exercise-27-box-muller",
    "title": "Solutions to Assignment 5",
    "section": "Exercise 27: Box-Muller",
    "text": "Exercise 27: Box-Muller\n\nimport numpy as np\n\n\nsample_size = 100_000\n\n\nrng = np.random.default_rng(seed=12345)\nu1 = rng.random(size=sample_size // 2)\nu2 = rng.random(size=sample_size // 2)\n\nr = - 2 * np.log(u1)    # exponentially distributed \nθ = 2 * np.pi * u2\n\nx1 = np.sqrt(r) * np.cos(θ)\nx2 = np.sqrt(r) * np.sin(θ)\n\nsamples = np.concatenate((x1, x2))\n\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots()\nax.hist(samples, bins=100);\nax.set_title('Histogram of normally distributed r.v.s (via Box-Muller)');"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_10.html",
    "href": "Documents_files/ME200/Solutions_Assignment_10.html",
    "title": "Solutions to Assignment 10",
    "section": "",
    "text": "ME200, Johannes Ruf and Luitgard Veraart"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_10.html#exercise-40-central-limit-theorem",
    "href": "Documents_files/ME200/Solutions_Assignment_10.html#exercise-40-central-limit-theorem",
    "title": "Solutions to Assignment 10",
    "section": "Exercise 40: (Central Limit Theorem)",
    "text": "Exercise 40: (Central Limit Theorem)\nLet \\(X_1, X_2, \\ldots\\) be a sequence of independent and identically distributed random variables having finite mean \\(\\mu\\) and finite variance \\(\\sigma^2\\). Then, for all \\(x \\in \\mathbb{R}\\) \\[\\begin{align*}\n\\lim_{n \\to \\infty} \\textrm{P} \\left[ \\frac{\\frac{1}{n} \\sum_{i=1}^n X_i - \\mu}{\\sqrt{\\frac{\\sigma^2}{n}}} \\leq x \\right] = \\Phi(x) = \\int_{- \\infty}^x \\frac{1}{\\sqrt{2 \\pi}} e^{- \\frac{y^2}{2}} dy.\n\\end{align*}\\]"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_10.html#exercise-41-illustration-of-the-central-limit-theorem",
    "href": "Documents_files/ME200/Solutions_Assignment_10.html#exercise-41-illustration-of-the-central-limit-theorem",
    "title": "Solutions to Assignment 10",
    "section": "Exercise 41: (Illustration of the Central Limit Theorem)",
    "text": "Exercise 41: (Illustration of the Central Limit Theorem)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm \nrng = np.random.default_rng(987654321)\n\n\nsamplesize = 10000\nsample = rng.beta(a=1, b=4, size=samplesize)\n\nfig, ax = plt.subplots(nrows=1, ncols=1)\nax.hist(sample, bins=100, density = True)\nplt.show()\n#ax.set_ylabel(\"density\")\n\n\n\n\nNext, we illustrate the CLT for the Beta distribution. To do so we consider samplesize=10000 realisations of $ $, where \\(X_1, \\ldots, X_n\\) are i.i.d. random variables from the \\(Beta(a, b)\\) distribution with mean \\(\\mu=\\frac{a}{a+b}\\) and variance \\(\\sigma^2 = \\frac{ab}{(a+b)^2 (a+b+1)}\\) for different choices of \\(n\\). In particular, we consider \\(n \\in \\{1, 5, 15\\}\\) (stored in the array nsCLT below). We plot the empirical cumulative distribution functions (ECDFs) of $ $ for the three different values of \\(n\\) and compare them to the CDF of the standard Normal distribution.\n\na = 1\nb = 4\nbetamean = a / (a + b)\nbetavariance = a * b / (((a + b)**2) * (a + b + 1))\n\nsamplesize = 10000\nnsCLT = np.array([1, 5, 15])\nmycolours = [\"black\", \"blue\", \"green\"]\nmylabels = [\"$\\hat{F}_1(x)$\", \"$\\hat{F}_5(x)$\", \"$\\hat{F}_{15}(x)$\"]\n\nxs = np.linspace(start=-3, stop=3, num=100)\nfig, ax = plt.subplots(nrows=1, ncols=1)\nfor i in range(nsCLT.size):\n    n = nsCLT[i]\n    sample = rng.beta(a=1, b=4, size=samplesize * n)\n    sample = sample.reshape(samplesize, n)\n    xbar = (sample.mean(axis=1) - betamean) / np.sqrt(betavariance/n) #compute the row means axis=1\n    ax.hist(xbar, bins=100, density = True, cumulative=True, histtype=\"step\", color=mycolours[i], label = mylabels[i])\nax.set_xlim([-3, 3])\nax.set_ylim([0, 1])\nax.set_xlabel(\"x\")\nax.set_ylabel(\"(E)CDF\")\nax.plot(xs, norm.cdf(xs, loc=0, scale=1), color=\"red\", label=\"$\\Phi(x)$\")\nax.set_title(\"Illustration of CLT for Beta distribution\")\nax.legend(loc = \"best\")\nplt.show()\n\n\n\n\nNext, we repeat the analysis above, but plot the histogram/density rather than the (E)CDF.\n\nfig, ax = plt.subplots(nrows=1, ncols=1)\nfor i in range(nsCLT.size):\n    n = nsCLT[i]\n    sample = rng.beta(a=1, b=4, size=samplesize * n)\n    sample = sample.reshape(samplesize, n)\n    xbar = (sample.mean(axis=1) - betamean) / np.sqrt(betavariance/n) #compute the row means axis=1\n    ax.hist(xbar, bins=100, density = True, cumulative=False, histtype=\"barstacked\", color=mycolours[i], label = nsCLT[i])\nax.set_xlim([-3, 3])\nax.set_ylim([0, 1])\nax.set_xlabel(\"x\")\nax.set_ylabel(\"density\")\nax.plot(xs, norm.pdf(xs, loc=0, scale=1), color=\"red\", label=\"$\\phi(x)$\")\nax.set_title(\"Illustration of CLT for Beta distribution\")\nax.legend(loc = \"best\")\nplt.show()"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_10.html#exercise-42-black-scholes-option-pricing-formula---european-call-option",
    "href": "Documents_files/ME200/Solutions_Assignment_10.html#exercise-42-black-scholes-option-pricing-formula---european-call-option",
    "title": "Solutions to Assignment 10",
    "section": "Exercise 42: (Black-Scholes option pricing formula - European call option)",
    "text": "Exercise 42: (Black-Scholes option pricing formula - European call option)\nLet \\(X \\sim \\textrm{N}(0, 1)\\). Then,\n\\[\\begin{align*}\n\\textrm{E}\\left[ e^{-rT} (S_T - K)^+ \\right] &=\n\\textrm{E}\\left[ e^{-rT}(S_0 \\exp \\left( (r - \\frac{\\sigma^2}{2})T + \\sigma \\sqrt{T} X \\right) - K)^+\\right]\\\\\n&= \\int_{-\\infty}^{+\\infty} e^{-rT} (S_0 \\exp\\left( (r - \\frac{\\sigma^2}{2})T + \\sigma \\sqrt{T} x\\right) - K)^+ \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx=(*).\n\\end{align*}\\]\nNow observe that \\[\\begin{align*}\nS_0 \\exp\\left(   (r - \\frac{\\sigma^2}{2})T + \\sigma \\sqrt{T} x\\right) \\geq K\n\\Longleftrightarrow\nx \\geq - \\frac{\\log \\left(\\frac{S_0}{K} \\right) + \\left(r - \\frac{\\sigma^2}{2}\\right)T}{\\sigma \\sqrt{T}}=-(D_1 - \\sigma \\sqrt{T})=-D_1 + \\sigma \\sqrt{T}.\n\\end{align*}\\]\nHence,\n\\[\\begin{align*}\n(*)&= \\int_{-D_1+ \\sigma \\sqrt{T}}^{+\\infty} e^{-rT} (S_0 \\exp\\left( (r - \\frac{\\sigma^2}{2})T + \\sigma \\sqrt{T} x\\right) - K) \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx\\\\\n&=e^{-rT} S_0 \\int_{-D_1+ \\sigma \\sqrt{T}}^{+\\infty}  \\exp\\left( (r - \\frac{\\sigma^2}{2})T + \\sigma \\sqrt{T} x\\right) \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx\n- e^{-rT} K \\int_{-D_1+ \\sigma \\sqrt{T}}^{+\\infty}  \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx.\n\\end{align*}\\]\nWe now look at the two integrals separately. We start with the first one:\n\\[\\begin{align*}\n&\\int_{-D_1+ \\sigma \\sqrt{T}}^{+\\infty}  \\exp\\left( (r - \\frac{\\sigma^2}{2})T + \\sigma \\sqrt{T} x\\right) \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx\\\\\n&= \\exp\\left( (r - \\frac{\\sigma^2}{2})T \\right)\n\\int_{-D_1+ \\sigma \\sqrt{T}}^{+\\infty}  \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left( -\\frac{1}{2} \\left( x^2 -2\\sigma \\sqrt{T} x + \\sigma^2 T - \\sigma^2 T \\right) \\right) dx\\\\\n&=\\exp\\left( (r - \\frac{\\sigma^2}{2})T  + \\frac{\\sigma^2T}{2}\\right)\n\\int_{-D_1+ \\sigma \\sqrt{T}}^{\\infty} \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left( -\\frac{1}{2} \\left( x -\\sigma \\sqrt{T}) \\right)^2 \\right)dx\\\\\n&=e^{rT} (1-\\Phi(-D_1+\\sigma \\sqrt{T}-\\sigma \\sqrt{T}))\\\\\n&=e^{rT} \\Phi(D_1).\n\\end{align*}\\]\nNext we evaluate the second integral:\n\\[\\begin{align*}\n\\int_{-D_1+ \\sigma \\sqrt{T}}^{+\\infty}  \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx\n=1-\\Phi(-D_1 + \\sigma \\sqrt{T})=\\Phi(D_1 - \\sigma \\sqrt{T}).\n\\end{align*}\\] Combining these results, gives \\[\\begin{align*}\n\\textrm{E}\\left[ e^{-rT} (S_T - K)^+ \\right]\n=  S_0 \\Phi(D_1) - K e^{-rT} \\Phi(D_1 - \\sigma \\sqrt{T}).\n\\end{align*}\\]"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_12.html",
    "href": "Documents_files/ME200/Solutions_Assignment_12.html",
    "title": "Solutions to Assignment 12",
    "section": "",
    "text": "ME200, Johannes Ruf and Luitgard Veraart\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport time"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_12.html#exercise-44",
    "href": "Documents_files/ME200/Solutions_Assignment_12.html#exercise-44",
    "title": "Solutions to Assignment 12",
    "section": "Exercise 44",
    "text": "Exercise 44\nLet \\[\\begin{align*}\n    D_1 & = \\frac{\\log \\left(\\frac{S_t}{K} \\right) + \\left(r + \\frac{\\sigma^2}{2} \\right)(T-t)}{\\sigma \\sqrt{T-t}}, &\n    D_2 &= D_1 - \\sigma \\sqrt{T-t}.\n\\end{align*}\\] We show that\n\\[\\begin{align*}\n    S_t \\varphi (D_1) = K e^{-r(T - t)} \\varphi(D_2),  \n\\end{align*}\\] where \\(\\varphi(x) = \\frac{1}{\\sqrt{2 \\pi}} \\exp(-\\frac{x^2}{2})\\).\nProof: To see that the statement is true, consider \\[\\begin{align*}\n        \\log\\left( \\frac{\\varphi(D_1)}{ \\varphi(D_2)}\\right) & = \\log\\left(\\frac{\\frac{e^{-D_1^2/2}}{\\sqrt{2 \\pi}}}{\\frac{e^{-D_2^2/2}}{\\sqrt{2 \\pi}}}\\right)\n        = \\log(\\exp(\\frac{1}{2} (D_2^2 - D_1^2))) \\\\\n        & = \\frac{1}{2} (D_2^2 - D_1^2) =  \n        \\frac{1}{2} (D_2 - D_1)(D_2 + D_1)\\\\\n        &= \\frac{1}{2} (- \\sigma \\sqrt{T - t}) (2 D_1 - \\sigma \\sqrt{T-t})\\\\\n        &= -D_1 \\sigma \\sqrt{T-t} + \\frac{\\sigma^2 (T-t)}{2}\\\\\n        &= -\\log\\left(\\frac{S_t}{K}\\right) - (r + \\frac{\\sigma^2}{2})(T-t) + \\frac{\\sigma^2 (T-t)}{2}\\\\\n        &= \\log\\left(\\frac{K}{S_t}\\right) - r (T-t).\n    \\end{align*}\\] Then we take exponentials on both sides of the equation and obtain \\[\\begin{align*}\n        \\frac{\\varphi(D_1)}{\\varphi(D_2)} = \\exp\\left( \\log\\left(\\frac{K}{S_t} \\right) - r (T-t) \\right)\n        = \\frac{K}{S_t} \\exp(-r(T-t))\n    \\end{align*}\\] which is equivalent to \\[\\begin{align*}\n        S_t \\varphi(D_1) = Ke^{-r(T-t)} \\varphi(D_2).\n    \\end{align*}\\]"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_12.html#exercise-45",
    "href": "Documents_files/ME200/Solutions_Assignment_12.html#exercise-45",
    "title": "Solutions to Assignment 12",
    "section": "Exercise 45",
    "text": "Exercise 45\n\n#Function returning the price of European Call option in Black-Scholes model\ndef black_scholes_call(S, K, r, tau, sigma):\n    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * tau)/(sigma * np.sqrt(tau))\n    d2 = d1 - sigma * np.sqrt(tau)\n    tmp1 = S * norm.cdf(d1, loc=0, scale=1)\n    tmp2 = K * np.exp(-r * tau) * norm.cdf(d2, loc=0, scale=1)\n    price = tmp1 - tmp2\n    return price\n\n\nmyprice = black_scholes_call(S=50, K=50, r=0.05, tau=1.0, sigma=0.3)\nprint('Price of European call option is: {:.4f}'.format(myprice))\n\nPrice of European call option is: 7.1156\n\n\n\n#Function computing implied volatilities using the Bisection method\ndef impliedvol_bisection(S, K, r, tau, marketprice, a, b, tolerance):\n    \n    f_a = black_scholes_call(S, K, r, tau, a) - marketprice\n    f_b = black_scholes_call(S, K, r, tau, b) - marketprice \n    \n    if f_a*f_b &gt;= 0: \n        print(\"Choose new interval [a, b]!\")\n        return \n    \n    l_n = a\n    r_n = b\n    \n    while (np.abs(r_n-l_n) &gt; tolerance):\n        \n        f_a = black_scholes_call(S, K, r, tau, l_n) - marketprice\n        f_b = black_scholes_call(S, K, r, tau, r_n) - marketprice \n        \n        y = (l_n + r_n) / 2\n        f_y = black_scholes_call(S, K, r, tau, y) - marketprice \n        \n        if f_a * f_y &lt; 0: \n            r_n = y\n        elif f_b * f_y &lt; 0: \n            l_n = y\n        elif f_y == 0:  \n            return y\n        else: \n            print(\"Error in bisection method.\")\n    return ((r_n + l_n) / 2)\n    \niv = impliedvol_bisection(S=50, K=50, r=0.05, tau=1.0, marketprice = 7.11562, a=0.1, b=0.6, tolerance = 0.0000001)\nprint(iv)\n\n0.29999960064888\n\n\n\n# Functions copied from the worksheet for Lecture 12 using the Newton method for comparison\ndef bs_vega(S, K, r, tau, sigma):\n    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * tau)/(sigma * np.sqrt(tau))\n    vega = S * np.sqrt(tau) * norm.pdf(d1, loc=0, scale=1)\n    return vega\n\ndef find_impliedvol_Newton(S, K, r, tau, marketprice, initialguess):\n    sigmaold = initialguess\n    maxiteration = 100\n    mydiff = 1\n    for i in range(maxiteration):\n            callprice = black_scholes_call(S, K, r, tau, sigmaold)\n            vega = bs_vega(S, K, r, tau, sigmaold)\n            if (vega &lt; 0.000000001):\n                return \"Error in vega\"\n            else: \n                sigmanew =  sigmaold - (callprice - marketprice) / vega\n                mydiff = np.abs(sigmaold - sigmanew)\n                sigmaold = sigmanew\n            \n            if (mydiff &lt; 0.0000001): \n                return sigmaold\n    return \"Max iteration reached\"\n\n\n# Example for computing implied volatility\n\nteststrikes = np.array([60, 70, 80, 90, 100, 110, 120])\ntestprices = np.array([22, 11, 4, 1, 0.4, 0.1, 0.05])\nS0=80\ntestimpliedvols1 = np.zeros(7)\ntestimpliedvols2 = np.zeros(7)\n\n# Computing implied volatities using Newton's method        \nstart = time.time()\nfor i in range(teststrikes.size):\n    testimpliedvols1[i] = find_impliedvol_Newton(S=S0, K=teststrikes[i], r=0.0, tau=1.0, marketprice=testprices[i], initialguess=0.5)\nruntime = time.time()-start\nprint('Computation time for Newton method is {:.8f}'.format(runtime))\n\n# Computing implied volatities using Bisection method   \nstart = time.time()\nfor i in range(teststrikes.size):\n    testimpliedvols2[i] = impliedvol_bisection(S=S0, K=teststrikes[i], r=0.0, tau=1.0, marketprice = testprices[i], a=0.00001, b=0.5, tolerance = 0.000000001)\nruntime = time.time()-start\nprint('Computation time for Bisection method is {:.8f}'.format(runtime))\n        \n# Plotting implied volatilities   \nfig, ax = plt.subplots(nrows=1, ncols=1)\nax.plot(teststrikes, testimpliedvols1, label=\"Implied vol based on Newton method\")\nax.plot(teststrikes, testimpliedvols2, color=\"red\", linestyle=\"dashed\", label=\"Implied vol based on Bisection method\")\nax.set_xlabel(\"Strike\")\nax.set_ylabel(\"Implied volatilities\") \nax.legend(loc=\"upper right\"); \n   \n\nComputation time for Newton method is 0.01595759\nComputation time for Bisection method is 0.09425735\n\n\n\n\n\nAbove we compared the computational time of both methods, and find that the Newton method is faster here, which is an advantage. One possible disadvantage of the Netwon method is that one needs to compute compute the derivative of the function, which is not required for the Bisection method."
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_7.html",
    "href": "Documents_files/ME200/Solutions_Assignment_7.html",
    "title": "Solutions to Assignment 7",
    "section": "",
    "text": "ME200, Johannes Ruf and Luitgard Veraart"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_7.html#exercise-31-checking-that-the-risk-neutral-probability-is-a-probability",
    "href": "Documents_files/ME200/Solutions_Assignment_7.html#exercise-31-checking-that-the-risk-neutral-probability-is-a-probability",
    "title": "Solutions to Assignment 7",
    "section": "Exercise 31: (Checking that the risk-neutral probability is a probability)",
    "text": "Exercise 31: (Checking that the risk-neutral probability is a probability)\nTo see that \\(\\tilde{p} = \\frac{1+r-d}{u-d} \\in (0, 1)\\) observe that by assumption \\(u&gt;d&gt;0\\) and hence \\(u-d&gt;0\\). Furthermore by assumption \\(1+r &gt; d\\) and hence \\(1+r-d &gt;0\\). Hence, \\(\\tilde{p}&gt;0\\). Furthermore, \\[\\begin{align*}\n\\frac{1+r - d}{u-d} &lt; 1\n\\Leftrightarrow 1+ r - d &lt; u - d\n\\Leftrightarrow 1+r &lt; u\n\\end{align*}\\] which is satisfied by assumption of the no-arbitrage condition. Hence, indeed \\(\\tilde{p} \\in (0, 1)\\).\nFurthermore,\n\\[\\begin{align*}\n1-\\tilde{p} = \\frac{u-d - (1+r -d)}{u-d} = \\frac{u - 1 - r}{u-d}.\n\\end{align*}\\]"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_7.html#exercise-32-sufficient-condition-for-no-arbitrage",
    "href": "Documents_files/ME200/Solutions_Assignment_7.html#exercise-32-sufficient-condition-for-no-arbitrage",
    "title": "Solutions to Assignment 7",
    "section": "Exercise 32: (Sufficient condition for no-arbitrage)",
    "text": "Exercise 32: (Sufficient condition for no-arbitrage)\nLet \\(d &lt; 1+r &lt; u\\). We show that there exists no trading strategy \\(\\varphi\\) satisfying \\[\\begin{align*}\nX_0^{\\varphi} = 0, \\quad P[X_1^{\\varphi} \\geq 0]=1, \\quad P[X_1^{\\varphi} &gt; 0]&gt;0.\n\\end{align*}\\]\nObserve that \\[\\begin{align*}\nd &lt; 1+r &lt; u \\Longleftrightarrow \\frac{d}{1+r} &lt; 1 &lt; \\frac{u}{1+r} \\\\\n\\Longleftrightarrow \\frac{dS_0}{1+r} &lt; S_0 &lt; \\frac{uS_0}{1+r}\n\\Longleftrightarrow \\frac{S_1(T)}{1+r} &lt; S_0 &lt; \\frac{S_1(H)}{1+r} \\\\\n\\Longleftrightarrow \\frac{S_1(T)}{1+r} - S_0 &lt; 0 &lt; \\frac{S_1(H)}{1+r} - S_0\n\\end{align*}\\] and the last inequalities implies that there is no arbitrage. To see this consider any trading strategy \\(\\varphi=(\\beta_0, \\Delta_0)\\) with \\(X_0^{\\varphi}=\\beta_0 B_0 + \\Delta_0 S_0 = \\beta_0 + \\Delta_0 S_0 = 0\\). Then, \\(\\beta_0 = - \\Delta_0 S_0\\) and \\[\\begin{align*}\nX_1^{\\varphi} = -\\Delta_0 S_0 B_1 + \\Delta_0 S_1 =\n\\Delta_0 (1+r) \\left(- S_0 + \\frac{S_1}{1+r}\\right).\n\\end{align*}\\] We have just seen that \\(-S_0 + \\frac{S_1}{1+r}\\) can take both a positive and a negative value with positive probability and hence \\(P[X_1^{\\varphi} \\geq 0] = 1\\) cannot hold and hence there is no arbitrage in the one-period binomial model."
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_7.html#exercise-33-pricing-a-european-put-option",
    "href": "Documents_files/ME200/Solutions_Assignment_7.html#exercise-33-pricing-a-european-put-option",
    "title": "Solutions to Assignment 7",
    "section": "Exercise 33: (Pricing a European put option)",
    "text": "Exercise 33: (Pricing a European put option)\nWith \\(S_0=4\\), \\(u=2\\), \\(d=\\frac{1}{2}\\), \\(r=\\frac{1}{4}\\) the time-0 price of the European put option with strike price \\(K=5\\) is given by\n\\[\\begin{align*}\nV_0 &= \\frac{1}{1+r} \\left( V_1(H) \\frac{1+r-d}{u-d}  + V_1(T) \\frac{u-1-r}{u-d}\\right)\\\\\n&= \\frac{1}{1+\\frac{1}{4}} \\left( V_1(H) \\frac{1}{2} + V_1(T) \\frac{1}{2}\\right)\\\\\n&=\\frac{4}{5} \\frac{1}{2} \\left((K-uS_0)^+ + (K-dS_0)^+ \\right)\\\\\n&=\\frac{2}{5} \\left( (5-8)^+ + (5 - 2)^+ \\right)\\\\\n&=\\frac{2}{5} 3 = \\frac{6}{5}.\n\\end{align*}\\]\nWe can implement this in Python as follows:\n\nimport numpy as np \n\n\ndef putprice(S0, u, d, r, K): \n    ptilde = (1+r-d) / (u-d)\n    V1H = np.maximum(K - u * S0, 0)\n    V1T = np.maximum(K - d * S0, 0)\n    price = (1 / (1 + r)) * (ptilde * V1H + (1 - ptilde) * V1T)\n    return(price)\n\nprint(putprice(S0=4, u=2, d=0.5, r=0.25, K=5))\n\n1.2000000000000002\n\n\nIn this example it turns out that the time-0 call price is equal to the time-0 put price. This is not in general true. In general, the so-call put-call parity holds which in the binomial model is given by \\(C_0 - P_0 = S_0 - K/(1+r)^N\\), where \\(C_0, P_0\\) denote the price of the European call and put option, respectively. Since in this example \\(S_0=4\\), \\(K=5\\), \\((1+r)^1=5/4\\) we find indeed that \\(S_0 - K/(1+r)^N=0\\) and hence \\(C_0=P_0\\)."
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_3.html",
    "href": "Documents_files/ME200/Solutions_Assignment_3.html",
    "title": "Solutions to Assignment 3",
    "section": "",
    "text": "ME200, Johannes Ruf and Luitgard Veraart"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_3.html#exercise-13",
    "href": "Documents_files/ME200/Solutions_Assignment_3.html#exercise-13",
    "title": "Solutions to Assignment 3",
    "section": "Exercise 13",
    "text": "Exercise 13\nThe variance of an r.v. \\(X\\) is\n\\[\n    {\\rm Var}(X) = E[(X  - E[X])^2].\n\\]\nThe square root of the variance is called the standard deviation:\n\\[\n    {\\rm SD}(X) = \\sqrt{{\\rm Var}(X)}.\n\\]"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_3.html#exercise-14",
    "href": "Documents_files/ME200/Solutions_Assignment_3.html#exercise-14",
    "title": "Solutions to Assignment 3",
    "section": "Exercise 14",
    "text": "Exercise 14\n\\[\n    {\\rm Var}(X + c) = E[(X + c  - E[X + c])^2] = E[(X   - E[X ])^2] = {\\rm Var}(X).\n\\]\n\\[\n    {\\rm Var}(c X) = E[(c X  - E[c X])^2] = E[c^2 (X   - E[X ])^2] = c ^2 E[ (X   - E[X ])^2] = c^2 {\\rm Var}(X).\n\\]"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_3.html#exercise-15",
    "href": "Documents_files/ME200/Solutions_Assignment_3.html#exercise-15",
    "title": "Solutions to Assignment 3",
    "section": "Exercise 15",
    "text": "Exercise 15\nLet \\(X\\) be exponentially distributed with parameter \\(\\mu\\). Then integration by parts yields\n\\[\n  E[X] = \\int_0^\\infty x \\mu \\mathrm e^{-\\mu x} dx = \\left. - x  \\mathrm e^{-\\mu x}\\right|_0^\\infty\n    + \\int_0^\\infty \\mathrm e^{-\\mu x} dx = 0 -  \\left. \\frac{1}{\\mu} \\mathrm e^{-\\mu x}\\right|_0^\\infty = \\frac{1}{\\mu}.\n\\]"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_3.html#exercise-16",
    "href": "Documents_files/ME200/Solutions_Assignment_3.html#exercise-16",
    "title": "Solutions to Assignment 3",
    "section": "Exercise 16",
    "text": "Exercise 16\n\nimport numpy as np\n\n\nsample_size = 100_000_000\nmu = 4\nstd = 3\n\n\nx = np.random.normal(size=sample_size, loc=mu, scale=std)\n\nSample mean:\n\nx.mean()\n\n3.9999144535186435\n\n\nSample standard deviation:\n\nx.std()\n\n3.0004925779793066\n\n\n## Exercise 17\nThe two functions indeed satisfy the condition \\(g''(x) \\geq 0\\). Thus we can use them to check whether the inequality holds.\n\nimport numpy as np\n\n\nsample_size = 1000\n\nWe will use samples from Unif(0,1) and from N(0,1) here, but of course we should try out many different possibilities.\n\nsamples_normal = np.random.standard_normal(size=sample_size)\nsamples_uniform = np.random.random(size=sample_size)\n\nLet’s first try \\(g(x)=x^2\\):\n\nsamples_normal.mean()**2 &lt;= (samples_normal**2).mean() \n\nTrue\n\n\n\nsamples_uniform.mean()**2 &lt;= (samples_uniform**2).mean() \n\nTrue\n\n\nLet’s no try \\(g(x)=x^4 + 2 * x^2\\):\n\nlhs = samples_normal.mean()**4 + 2 * samples_normal.mean()**2\nrhs = (samples_normal**4 + 2 * samples_normal**2).mean()\nlhs &lt;= rhs\n\nTrue\n\n\n\nlhs = samples_uniform.mean()**4 + 2 * samples_uniform.mean()**2\nrhs = (samples_uniform**4 + 2 * samples_uniform**2).mean()\nlhs &lt;= rhs\n\nTrue\n\n\nWe always get True, which confirms the inequality. (Of course, we usually would need to prove it but we won’t have time to prove Jensen’s inequality in this course.)\nTomorrow we will introduce Python functions. Using such functions would simplify the code, especially for the second example."
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_1.html",
    "href": "Documents_files/ME200/Solutions_Assignment_1.html",
    "title": "Solutions to Assignment 1",
    "section": "",
    "text": "ME200, Johannes Ruf and Luitgard Veraart"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_1.html#exercise-1",
    "href": "Documents_files/ME200/Solutions_Assignment_1.html#exercise-1",
    "title": "Solutions to Assignment 1",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nLabel the dice A, B, and C, and consider each die to be a sub-experiment.\nBy the multiplication rule, there are \\(6^3\\) possible outcomes for ordered pairs of the form (value of A, value of B, value of C), and they are equally likely by symmetry.\nOf these, (1, 1, 3), (1, 2, 2), (1, 3, 1), (2, 1, 2), (2, 2, 1), (3, 1, 1) are favorable to a sum of 5.\n(5, 6, 6), (6, 5, 6), (6, 6, 5) are favorable to a sum of 17.\nTherefore a sum of 5 is more likely than a sum of 17. The probability is \\(6/6^3\\) for the former, and \\(3/6^3\\) for the latter."
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_1.html#exercise-2",
    "href": "Documents_files/ME200/Solutions_Assignment_1.html#exercise-2",
    "title": "Solutions to Assignment 1",
    "section": "Exercise 2",
    "text": "Exercise 2\n\nThere are \\(n \\choose 2\\) games being played in total.\nThere are \\(2^{n \\choose 2}\\) possible outcome lists."
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_1.html#exercise-3",
    "href": "Documents_files/ME200/Solutions_Assignment_1.html#exercise-3",
    "title": "Solutions to Assignment 1",
    "section": "Exercise 3",
    "text": "Exercise 3\n\nimport numpy as np\n\n\npoker = np.array([-30, 40, 10, -10, 40])\nroulette = np.array([10, 20, -60, -10, 0])\n\nOverall winnings in poker:\n\npoker.sum()\n\n50\n\n\nOverall winnings in roulette:\n\nroulette.sum()\n\n-40\n\n\nWinnings/losses on each of the five days:\n\npoker + roulette\n\narray([-20,  60, -50, -20,  40])\n\n\n\n[poker[i]+roulette[i] for i in range(len(poker))]\n\n[-20, 60, -50, -20, 40]"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_1.html#exercise-4",
    "href": "Documents_files/ME200/Solutions_Assignment_1.html#exercise-4",
    "title": "Solutions to Assignment 1",
    "section": "Exercise 4",
    "text": "Exercise 4\n\nimport numpy as np\n\n\nprob = np.array([0.1, 0.2, 0.1, 0.05, 0.55])\nv = np.array([5, 6, 7, 8, 9])     #v = np.arange(5, 10)\nk = 8\n\n\nnp.random.choice(v, k, p=prob)\n\narray([6, 9, 9, 9, 9, 6, 8, 5])"
  },
  {
    "objectID": "Documents_files/ME200/Solutions_Assignment_1.html#exercise-5",
    "href": "Documents_files/ME200/Solutions_Assignment_1.html#exercise-5",
    "title": "Solutions to Assignment 1",
    "section": "Exercise 5",
    "text": "Exercise 5\n\nimport math\n\n\nk = 5\nn = 10\n\n\nsum=0\nfor n in range(1,100):\n    for k in range(1, n):\n        sum+=math.comb(n, k) != math.comb(n, n-k)\n\n\nsum\n\n0\n\n\nWe should check that we always get ‘True’ for different values of k and n."
  },
  {
    "objectID": "Learning/Python/CF/Easy/1758A.html",
    "href": "Learning/Python/CF/Easy/1758A.html",
    "title": "1758A",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\n Click the title to see the text of the problem on Codeforces"
  },
  {
    "objectID": "Learning/Python/Useful/Greatest_authors.html",
    "href": "Learning/Python/Useful/Greatest_authors.html",
    "title": "Greatest Authors",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\nI appreciate a lot this project by Shane Sherman, which is an attempt to create a list of “best books” based on the compound information found on various highly accredited lists (there are obvious critiques, about western, white, male biases etc., but I don’t think this is relevant in the context of this page). What I am going to do in this page is to have an idea of which authors have more works considered the best.\nOne can proceed as follows: download the .csv file from the website of the aforementioned project. Modify the .csv file such that the first row reads:\n\n\n\nPosition\nTitle\nAuthor\nYear\n\n\n\n\n\nIn the same folder where you saved the .csv file, you can create a .py file with the following code.\n1import pandas as pd\n\n2df = pd.read_csv(\"tgb_1.csv\")\nmylist = df[\"Author\"]\n\n3splittings = [100, 500, 1000, len(mylist)]\nN = len(splittings)\nPartial_lists = [mylist[:i] for i in splittings]\n\n4Counts = [pd.Series(Partial_lists[j]).value_counts() for j in range(N)]\n\n5Lower = [0 for i in range(N)]\nAt_least = 10\nfor i in range(N):\n    for j in range(30, -1, -1):\n        if len(Counts[i][Counts[i]&gt;=j])&gt;=At_least:\n            Lower[i]=j\n            break\nFinal_list_authors = [Counts[i][Counts[i]&gt;=Lower[i]] for i in range(N)]\n\n6for i in range(N):\n    print(\"These are the authors that have the most (at least \"+str(Lower[i])+\") publications\\namongst the first \"+str(splittings[i])+\"\"\" many \"Best books\" \"\"\")\n    print(Final_list_authors[i].to_string()+\"\\n\")\n\n1\n\nImport the pandas library. Useful for statistics.\n\n2\n\nRead the .csv file and save in a pandas array the column with header Author (this is why a modification of the file was necessary).\n\n3\n\nWe create 4 lists. The first analyses the first 100 books, the second the first 500 books and so on. Partial_lists is a list that contains in entry i a pandas list with the authors of the first splittings[i]-many books.\n\n4\n\nWe count the occurrencies in each of the partial lists.\n\n5\n\nWe select in each list Partial_lists[i] the first authors. These are the authors that have at least Lower[i] many books in that list. Lower[i] is choosen in such a way that the authors as as few as possible above At_least. So for example in the first list, if we set Lower[0]=3 we would only obtain 5 authors, this is why we set Lower[0]=2.\n\n6\n\nPrinting.\n\n\nThe output is:\nThese are the authors that have the most (at least 2) publications\namongst the first 100 many \"Best books\" \nErnest Hemingway      4\nFranz Kafka           4\nFyodor Dostoyevsky    4\nWilliam Faulkner      3\nVirginia Woolf        3\nVladimir Nabokov      2\nSophocles             2\nJames Joyce           2\nStendhal              2\nCharles Dickens       2\nJane Austen           2\nGeorge Orwell         2\nGustave Flaubert      2\nHomer                 2\nLeo Tolstoy           2\n\nThese are the authors that have the most (at least 4) publications\namongst the first 500 many \"Best books\" \nSophocles              7\nCharles Dickens        7\nErnest Hemingway       6\nWilliam Shakespeare    6\nC. S. Lewis            6\nJohn Galsworthy        5\nAeschylus              5\nFyodor Dostoyevsky     5\nSamuel Beckett         4\nJohn Updike            4\nSaul Bellow            4\nHonoré de Balzac       4\nWilliam Faulkner       4\nJoseph Conrad          4\nVirginia Woolf         4\nEdith Wharton          4\nJohn Dos Passos        4\nD. H. Lawrence         4\nThomas Mann            4\nJames Joyce            4\nFranz Kafka            4\n\nThese are the authors that have the most (at least 6) publications\namongst the first 1000 many \"Best books\" \nWilliam Shakespeare    11\nCharles Dickens         9\nWilliam Faulkner        9\nSamuel Beckett          8\nSophocles               7\nJ. K Rowling            7\nC. S. Lewis             6\nPhilip Roth             6\nHenry James             6\nErnest Hemingway        6\n\nThese are the authors that have the most (at least 8) publications\namongst the first 2706 many \"Best books\" \nWilliam Shakespeare    19\nWilliam Faulkner       13\nCharles Dickens        13\nUnknown                11\nHenry James            11\nIris Murdoch           10\nMargaret Atwood        10\nPhilip Roth            10\nJohn Updike             8\nSamuel Beckett          8\nErnest Hemingway        8\nJ M Coetzee             8\nMolière                 8\nAlice Munro             8\nFaulkner, Hemingway and Dickens are the only authors present in all 4 lists. Sheakerspeare appears in all the list with the exception of the first one, and has 1.5 times the second author (Faulkner and Dickens) books in the complete list, while Sophocles is also doing quite well. The only women appearing are Virginia Woold, Jane Austen, Edith Wharton, J.K. Rowling, Iris Murdoch, Margaret Atwood and Alice Munro; with Woolf and Austen being the only ones in the first list.\nChatGPT is a mess, but it seems that the only author not born in Europe (including Russia) or US/Canada is Coetzee (South Africa). This seems to indicate that these lists are incredibly biased.\nFor more analysis, you can try to run this for a more complete print.\nfor i in range(N):\n    print(\"#\"*77+\"\\nThese are the authors that have the most (at least \"+str(Lower[i])+\") publications\\namongst the first \"+str(splittings[i])+\"\"\" many \"Best books\"\\n\"\"\"+\"#\"*77)\n    for j, A in zip(Final_list_authors[i], Final_list_authors[i].index):\n        print(\"%\"*77+\"\\n\"+A + \" (\"+str(j)+\")\")\n        L=df[:splittings[i]][df[:splittings[i]][\"Author\"]==A][\"Title\"]\n        Num=df[:splittings[i]][df[:splittings[i]][\"Author\"]==A][\"Number\"]\n        Num=list(Num)\n        T=list(L)\n        for k in range(len(Num)):\n            print(\"\\t\", str(int(Num[k]))+\" \", \"\\t\", T[k])\n        print(\"%\"*77+\"\\n\")\n    print(\"\\n\\n\")\nAn example of the first part of the output is:\n#############################################################################\nThese are the authors that have the most (at least 2) publications\namongst the first 100 many \"Best books\"\n############################################################################# \n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nErnest Hemingway (4)\n     44      The Sun Also Rises \n     57      The Old Man and the Sea\n     60      For Whom the Bell Tolls\n     84      A Farewell to Arms\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nFranz Kafka (4)\n     33      The Trial\n     61      The Complete Stories of Franz Kafka\n     62      The Metamorphosis\n     86      The Castle\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nFyodor Dostoyevsky (4)\n     13      The Brothers Karamazov \n     14      Crime and Punishment \n     54      The Idiot\n     69      The Possessed\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nWilliam Faulkner (3)\n     25      The Sound and the Fury\n     30      Absalom, Absalom!\n     67      As I Lay Dying\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nVirginia Woolf (3)\n     22      To the Lighthouse \n     38      Mrs. Dalloway \n     79      Orlando: A Biography\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nVladimir Nabokov (2)\n     12      Lolita \n     65      Pale Fire \n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nSophocles (2)\n     51      Oedipus the King\n     66      Antigone\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nJames Joyce (2)\n     2       Ulysses\n     49      A Portrait of the Artist as a Young Man \n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nStendhal (2)\n     34      The Red and the Black\n     93      The Charterhouse of Parma\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nCharles Dickens (2)\n     27      Great Expectations\n     45      David Copperfield\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nJane Austen (2)\n     17      Pride and Prejudice\n     59      Emma\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nGeorge Orwell (2)\n     26      Nineteen Eighty Four\n     78      Animal Farm\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nGustave Flaubert (2)\n     10      Madame Bovary\n     87      A Sentimental Education\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nHomer (2)\n     9       The Odyssey\n     21      The Iliad\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nLeo Tolstoy (2)\n     7       War and Peace\n     19      Anna Karenina\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
  },
  {
    "objectID": "Learning/Python/Useful/Arxiv.html",
    "href": "Learning/Python/Useful/Arxiv.html",
    "title": "Everyday email",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\nEvery morning, I receive an email from Arxiv. Arxiv is an online platform used in the scientific community to share new results, quite similar to a Facebook, LinkedIn or Instagram for nerds scientists. These emails are super cool: they contain all the results published on the platform about your area(s) of interest in the last 24hrs, and a link to the relevant papers. This is quite useful and it allows you to keep track of the recent results without too much effort.\nHowever, there is a (small) problem: every new article has a page and a pdf. The page only contains the abstract, the pdf contains the whole article. Here’s the pickle: in the email is only contained the link to the page of the article, and not to the pdf.\nSo every morning I have to click the link to the papers I’m interested in, and THEN click again in the page to find the pdf. TWO CLICKS TO READ AN ARTICLE? That’s too much. From today, no more!\n\ndef get_pdf_url(line: \"line of email with url\") -&gt; \"url of the pdf\":\n    ### We assume a lot about the structure of the line of the link.\n    if \"/abs/\" not in line:\n        return \"https://www.google.com\"\n    sta = line.split(\"/abs/\")[1]\n    end = sta.find(\" , \")\n    return \"https://arxiv.org/pdf/\"+sta[:end]+\".pdf\"\n\nimport webbrowser\ntoday = open(\"./today.txt\",\"r\")\nfor line in today.readlines():\n    if \"https\" in line:\n        url = get_pdf_url(line)\n        webbrowser.open(url)\nTo use this code is enough to:\n\nSave the code above in a file with filename ending in .py,\nEvery morning, copy your email in a file with name “today.txt” in the same folder as the Python file,\nExecute the Python file.\n\nMake sure you have installed webbrowser with pip install webbrowser in the terminal."
  },
  {
    "objectID": "Learning/Learning.html",
    "href": "Learning/Learning.html",
    "title": "Learning",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\nThe goal of this section is to collect resources and thougths, and to record my learning progresses regarding topics I am not proficient in, mainly Competitive Coding, Machine Learning, Reinforcement Learning.\nThese are my main motivations behind this decision.\n\nI did not find online any resource tailored to my need,\nI hope this might help other people to get close to these topics,\nThe best way to learn is to teach.\n\n Caveat: I know Python is NOT the best language for competitive coding, but since my objective for now is not perfection, I will mainly use Python for now. \nPlease do let me know if you find any mistakes."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Homepage",
    "section": "",
    "text": "I am currently a PhD student in the department of Mathematics of LSE; my interests are mainly in Graph Theory and at the moment I am working on Ramsey Theory. Outside the areas of my PhD, I am passionate but not proficient about a lot of other topics, mainly Machine Learning, Neurosciences and Philosophy.\n\n\n\n\n\n\n\n\n\n\n\nContact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\n\n\n\n\n\n\nFavourite quotes\n\n\n\n\n\nNew Quote"
  },
  {
    "objectID": "Teaching/Teaching.html",
    "href": "Teaching/Teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "These pages contain some of the material I used to aid my teaching during my years at LSE. Some of the material I created myself, some was given by the various lecturers of the courses, some I found online and I propose here again in a selected format.\nFor a quick overview of my teaching, you can consult the following table.\n\nCourses taught {.striped .hover tbl-colwidths=“[10,40,20,10,20]”}   \n\n\n\n\n\n\n\n\n\nCode\nName\nYear(s)\nDepartment\nMain Lecturer\n\n\n\n\nMA102/3\nIntroduction to Abstract Mathematics\n2020/21; 21/22\nMaths\nPeter Allen\n\n\nMA210\nDiscrete Mathematics\n2021/22\nMaths\nPeter Allen\n\n\nMA423\nFundamentals of Operations Research\n2021/22\nMaths\nAhmad Abdi\n\n\nME200\nComputational Methods in Financial Mathematics\n2023\nMaths\nLuitgard Veraart\n\n\nST310\nMachine Learning\n2022/23\nStats\nJoshua Loftus\n\n\nST455\nReinforcement Learning\n2022/23\nStats\nChengchun Shi\n\n\n\nFor privacy reasons, I won’t add my students’ feedbacks in this page. If you are interested in additional information about my teaching, please contact me."
  },
  {
    "objectID": "Teaching/ST310.html",
    "href": "Teaching/ST310.html",
    "title": "ST310 (Machine Learning)",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nInternet is a great resource. Use it. Some resources I like:\n\nMIT OpenCourseWare,\nStanford,\nHarvard.\n\n\n\n\n\n\n\n\n\n\nBeware, the Crime!\n\n\n\nIt is illegal to download articles and books from pages like LibGen, Sci-hub or from Telegram bots like @scihubot. Also, DO NOT use VPN to protect your freedom of education (Opera offers a free VPN).\n\n🙃\n\n\n\n\n\n\nCourse content (Official)\n\nThe primary focus of this course is on the core machine learning techniques in the context of high-dimensional or large datasets (i.e. big data). The first part of the course covers elementary and important statistical methods including nearest neighbours, linear regression, logistic regression, regularisation, cross-validation, and variable selection. The second part of the course deals with more advanced machine learning methods including regression and classification trees, random forests, bagging, boosting, deep neural networks, k-means clustering and hierarchical clustering. The course will also introduce causal inference motivated by analogy between double machine learning and two-stage least squares. All the topics will be delivered using illustrative real data examples. Students will also gain hands-on experience using R or Python (programming languages and software environments for data analysis, computing and visualisation).\n\nMaterial and solutions\n\nI did not write the following material, which was prepared by Joshua Loftus. My role was simply to present the content of the seminars and the solutions in class.\n\n\n\nWeek\nSeminar Material\n\n\n\n\nWeek 1\nSeminar\n\n\nWeek 2\nSeminar\n\n\nWeek 3\nSeminar\n\n\nWeek 4\n\npart 1\npart 2\n\n\n\nWeek 5\n\npart 1\npart 2\n\n\n\nWeek 6\n\npart 1\npart 2\n\n\n\nWeek 7\n\npart 1\npart 2\n\n\n\nWeek 8\n\npart 1\npart 2\npart 3\npart 4\n\n\n\nWeek 9\nSeminar\n\n\nWeek 10\nSeminar"
  },
  {
    "objectID": "Teaching/Pre-sessionals/Maths3.html",
    "href": "Teaching/Pre-sessionals/Maths3.html",
    "title": "Pre-sessionals - Maths 3",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nInternet is a great resource. Use it. Some resources I like:\n\nMIT OpenCourseWare,\nStanford,\nHarvard.\n\n\n\n\n\n\n\n\n\n\nBeware, the Crime!\n\n\n\nIt is illegal to download articles and books from pages like LibGen, Sci-hub or from Telegram bots like @scihubot. Also, DO NOT use VPN to protect your freedom of education (Opera offers a free VPN).\n\n🙃\n\n\n\n\n\n\nSystems of Linear Equations\n\n\nIntroduction to Systems of Equations\n\nWhat is a System of Equations?\n\nDefinition of a system of equations as a set of equations with common variables.\nExample: Discuss a system representing the total cost of items purchased.\n\nMethods for Solving Systems\n\nBriefly explain graphing, substitution, and elimination methods.\nHighlight when each method is most suitable.\nExercise: Identify which method is best for a given system.\n\nImportance and Applications\n\nEmphasize the significance of systems of equations in solving real-world problems.\nMention applications in various fields.\nExercise: Brainstorm other scenarios where systems could be applied.\n\n\nGraphical Solution\n\nSolving Systems Graphically\n\nExplain the concept of solution points as intersections of graphs.\nExample: Solve the system \\(2x + y = 5\\) and \\(3x -y = 1\\) graphically.\nExercise: Graph and solve simple systems of equations.\n\nInterpreting Solutions on Graphs\n\nDiscuss the significance of unique solutions, no solutions, and infinite solutions.\nInterpret the graphical meaning of these cases.\nExercise: Analyze different scenarios on graphs.\n\nAdvantages and Limitations\n\nCompare graphical method with other methods.\nDiscuss accuracy and limitations of graphical solutions.\n\n\nSubstitution and Elimination Methods\n\nSolving Systems using Substitution\n\nExplain the substitution method step by step.\nExample: Solve the system \\(3x -2y = 8\\) and \\(x + y = 3\\) using substitution.\nExercise: Practice solving systems using the substitution method.\n\nSolving Systems using Elimination\n\nExplain the elimination (addition) method step by step.\nExample: Solve the system \\(2x + 3y = 7\\) and \\(4x -y = 5\\) using elimination.\nExercise: Practice solving systems using the elimination method.\n\n\nApplications of Systems of Equations\n\nReal-World Examples of Systems\n\nDiscuss examples from fields like economics, chemistry, and engineering.\nExample: Discuss a mixture problem involving two solutions.\nExercise: Brainstorm more real-world examples.\n\nUsing Systems to Solve Practical Problems\n\nExplain how to set up and solve practical problems using systems.\nExample: Solve a money-related problem involving different types of coins.\nExercise: Solve practical problems related to mixtures, interest, or other scenarios.\n\nReflection on Problem-Solving\n\nHighlight the problem-solving process and the role of systems.\nEncourage students to think critically and apply these methods.\n\n\nConclusion and Recap\n\nSummarize the key concepts covered in the lecture.\nHighlight the importance of functions in various fields.\n\nQ&A Session"
  },
  {
    "objectID": "Teaching/Pre-sessionals/Maths2.html",
    "href": "Teaching/Pre-sessionals/Maths2.html",
    "title": "Pre-sessionals - Maths 2",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nInternet is a great resource. Use it. Some resources I like:\n\nMIT OpenCourseWare,\nStanford,\nHarvard.\n\n\n\n\n\n\n\n\n\n\nBeware, the Crime!\n\n\n\nIt is illegal to download articles and books from pages like LibGen, Sci-hub or from Telegram bots like @scihubot. Also, DO NOT use VPN to protect your freedom of education (Opera offers a free VPN).\n\n🙃\n\n\n\n\n\n\nIntroduction to Derivatives\n\n\nIntroduction to Derivatives\n\nWhat is a Derivative?\n\nDefinition of a derivative as the instantaneous rate of change.\nNotation: \\(f'(x)\\), \\(\\frac{dy}{dx}\\).\nExample: Calculate the derivative of \\(f(x) = 3x^2 - 2x + 5\\).\nExercise: Find the derivative of given functions.\n\nThe Concept of Instantaneous Rate of Change\n\nUnderstanding how derivatives relate to slopes of tangent lines.\nExample: Interpret the derivative of a position function as velocity.\nExercise: Interpret derivatives in real-world contexts.\n\nNotation and Interpretation\n\nDiscuss the meaning of \\(f'(x)\\) and \\(\\frac{dy}{dx}\\) in context.\nEmphasize the connection between slope and rate of change.\nExercise: Match graphs of functions with their derivatives.\n\n\nDerivatives of Linear and Quadratic Functions\n\nFinding the Derivative of Linear Functions\n\nDeriving the derivative of linear functions.\nExample: Find the derivative of \\(L(x) = 2x + 4\\).\nExercise: Differentiate linear functions with different coefficients.\n\nFinding the Derivative of Quadratic Functions\n\nDifferentiating quadratic functions using the power rule.\nExample: Differentiate \\(q(x) = -3x^2 + 5x - 1\\).\nExercise: Derive quadratic functions with various coefficients.\n\nTangent Lines and Rates of Change\n\nConnecting derivatives to slopes of tangent lines.\nCalculating slopes of tangent lines at specific points.\nExample: Find the equation of the tangent line to \\(f(x) = x^2\\) at \\(x = 2\\).\n\n\nBasic Rules of Differentiation\n\nPower Rule for Differentiation\n\nStatement and derivation of the power rule.\nExample: Differentiate \\(g(x) = 4x^3 - 2x^2 + 7x\\) using the power rule.\nExercise: Apply the power rule to various functions.\n\nConstant Rule and Sum Rule\n\nUsing the constant rule and sum rule to differentiate functions.\nExample: Find the derivative of \\(h(x) = 3 + 2x^2 - 5x\\).\nExercise: Differentiate expressions involving constants and sums.\n\nDifferentiating Polynomial Functions\n\nApplying differentiation rules to polynomial functions.\nExample: Differentiate \\(p(x) = 6x^4 + 2x^3 - 9x^2 + 5\\).\nExercise: Differentiate given polynomial functions.\n\n\nApplications of Derivatives\n\nFinding Maxima and Minima\n\nUsing derivatives to locate critical points.\nIdentifying maxima, minima, and points of inflection.\nExample: Find the critical points of \\(g(x) = 2x^3 - 9x^2 + 12x\\).\n\nTangent Lines as Approximations\n\nUsing tangent lines for linear approximations.\nExample: Estimate \\(\\sqrt{9.2}\\) using the tangent line at \\(x = 9\\) for \\(f(x) = \\sqrt{x}\\).\n\nSimple Optimization Problems\n\nApplying derivatives to solve basic optimization problems.\nExample: Find the dimensions of a rectangle with maximum area given a fixed perimeter.\n\n\nConclusion and Recap\n\nSummarize the key concepts covered in the lecture.\nHighlight the importance of functions in various fields.\nEncourage students to practice and explore more examples.\n\nQ&A Session"
  },
  {
    "objectID": "Teaching/Pre-sessionals/Maths1.html",
    "href": "Teaching/Pre-sessionals/Maths1.html",
    "title": "Pre-sessionals - Maths 1",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nInternet is a great resource. Use it. Some resources I like:\n\nMIT OpenCourseWare,\nStanford,\nHarvard.\n\n\n\n\n\n\n\n\n\n\nBeware, the Crime!\n\n\n\nIt is illegal to download articles and books from pages like LibGen, Sci-hub or from Telegram bots like @scihubot. Also, DO NOT use VPN to protect your freedom of education (Opera offers a free VPN).\n\n🙃\n\n\n\n\n\n\nIntroduction to Functions and Linear Equations\n\n\nIntroduction to Functions\n\nWhat is a function?\n\nDefinition: A function is a relation between a set of inputs (domain) and a set of possible outputs (range), such that each input is related to exactly one output.\nExample: Temperature conversion function.\nExercise: Identify whether given relations are functions or not.\n\nDomain and Range of a function\n\nDefinition of domain and range.\nExample: Find the domain and range of the function \\(f(x) = \\sqrt{x}\\).\nExercise: Determine the domain and range of a given function.\n\nNotation and Terminology\n\nNotation: \\(f(x)\\) represents the output of function \\(f\\) for input \\(x\\).\nTerminology: Input, output, independent variable, dependent variable, etc.\nExercise: Translate word problems into function notation.\n\n\nLinear Functions\n\nDefinition of Linear Functions\n\nA linear function is a function whose graph is a straight line.\nExample: \\(f(x) = 2x + 3\\) is a linear function.\nExercise: Determine whether given functions are linear or not.\n\nGraphing Linear Functions\n\nPlotting points and connecting with a line.\nExample: Graph the function \\(f(x) = -0.5x + 2\\).\nExercise: Graph a set of linear functions.\n\nSlope and \\(y\\)-Intercept\n\nDefinition of slope and \\(y\\)-intercept.\nCalculation of slope and \\(y\\)-intercept from an equation.\nExample: Find the slope and \\(y\\)-intercept of \\(f(x) = 3x - 1\\).\nExercise: Calculate slope and \\(y\\)-intercept of given functions.\n\n\nPolynomial Functions of Degree 2\n\nDefinition of Polynomial Functions\n\nA polynomial function is a function consisting of terms with non-negative integer powers.\nExample: \\(f(x) = 2x^2 - 4x + 1\\) is a polynomial function.\nExercise: Identify polynomial functions among given expressions.\n\nQuadratic Functions\n\nDefinition and standard form: \\(f(x) = ax^2 + bx + c\\).\nExample: Identify coefficients of \\(a\\), \\(b\\), and \\(c\\) in \\(f(x) = 5x^2 - 2x + 7\\).\nExercise: Write quadratic functions in standard form.\n\nGraphing Quadratic Functions\n\nPlotting quadratic curves.\nFinding vertex and axis of symmetry.\nExample: Graph \\(f(x) = x^2 - 4x + 3\\) and find its vertex.\nExercise: Graph given quadratic functions and locate their vertices.\n\n\nApplications of Functions\n\nReal-World Examples of Functions\n\nDistance-time and temperature-time functions.\nExample: Express the height of an object in terms of time.\nExercise: Identify functions in everyday scenarios.\n\nModeling with Linear and Quadratic Functions\n\nUsing linear functions for proportional relationships.\nUsing quadratic functions for parabolic motion.\nExample: Model the height of a ball thrown vertically upward.\nExercise: Model a real-world scenario using a linear or quadratic function.\n\nSimple Problems Involving Functions\n\nSolving basic problems using functions.\nExample: Find the time when a car reaches a certain distance.\nExercise: Solve problems involving linear and quadratic functions.\n\n\nConclusion and Recap\n\nSummarize the key concepts covered in the lecture.\nHighlight the importance of functions in various fields.\nEncourage students to practice and explore more examples.\n\nQ&A Session"
  },
  {
    "objectID": "Teaching/Pre-sessionals/Maths4.html",
    "href": "Teaching/Pre-sessionals/Maths4.html",
    "title": "Pre-sessionals - Maths 4",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nInternet is a great resource. Use it. Some resources I like:\n\nMIT OpenCourseWare,\nStanford,\nHarvard.\n\n\n\n\n\n\n\n\n\n\nBeware, the Crime!\n\n\n\nIt is illegal to download articles and books from pages like LibGen, Sci-hub or from Telegram bots like @scihubot. Also, DO NOT use VPN to protect your freedom of education (Opera offers a free VPN).\n\n🙃\n\n\n\n\n\n\nSupply and Demand Problems\n\n\nIntroduction to Supply and Demand\n\nBasic Concepts of Supply and Demand\n\nDefine supply and demand as fundamental economic concepts.\nExplain how they determine prices and quantities in markets.\nExample: Discuss the relationship between gas prices and demand during holidays.\n\nEquilibrium Point and Market Equilibrium\n\nDefine equilibrium point as the intersection of supply and demand curves.\nEmphasize the role of equilibrium in establishing market prices.\nExercise: Analyze a simple supply and demand graph to find equilibrium.\n\n\nSupply and Demand Equations\n\nModeling Supply and Demand using Linear Equations\n\nDiscuss how linear equations can represent supply and demand relationships.\nExample: Construct a linear demand equation based on given data.\nExercise: Write linear supply and demand equations from provided information.\n\nEquilibrium Price and Quantity\n\nExplain how to find equilibrium price and quantity from supply and demand equations.\nExample: Solve for equilibrium point using a supply and demand equation.\nExercise: Calculate equilibrium price and quantity for different scenarios.\n\n\nShifts in Supply and Demand\n\nFactors Causing Shifts in Supply and Demand Curves\n\nIdentify factors that can lead to shifts in supply and demand curves.\nDiscuss examples such as changes in consumer preferences or technological advancements.\nExercise: Identify possible shifts due to external factors.\n\nEffects on Equilibrium Price and Quantity\n\nExplain how shifts affect equilibrium price and quantity.\nDiscuss scenarios of price increases, decreases, and quantity changes.\nExample: Analyze the impact of a decrease in production costs on equilibrium.\nExercise: Predict the outcomes of shifts in supply and demand curves.\n\n\nApplications to Real-World Scenarios\n\nApplying Supply and Demand Analysis to Real-World Scenarios\n\nPresent scenarios like price ceilings, shortages, and surpluses.\nDiscuss the implications of government interventions on markets.\nExercise: Analyze the effects of a price ceiling on a graph.\n\nUnderstanding Market Dynamics and Changes\n\nEmphasize the importance of understanding supply and demand dynamics in decision-making.\nDiscuss how businesses and policymakers use these concepts.\nExample: Explain how supply and demand analysis can help plan for seasonal products.\n\n\nConclusion and Recap\n\nSummarize the key concepts covered in the lecture.\nHighlight the importance of functions in various fields.\n\nQ&A Session"
  },
  {
    "objectID": "Teaching/Pre-sessionals/Stats1.html",
    "href": "Teaching/Pre-sessionals/Stats1.html",
    "title": "Pre-sessionals - Stats 1",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nInternet is a great resource. Use it. Some resources I like:\n\nMIT OpenCourseWare,\nStanford,\nHarvard.\n\n\n\n\n\n\n\n\n\n\nBeware, the Crime!\n\n\n\nIt is illegal to download articles and books from pages like LibGen, Sci-hub or from Telegram bots like @scihubot. Also, DO NOT use VPN to protect your freedom of education (Opera offers a free VPN).\n\n🙃\n\n\n\n\n\n\nIntroduction to Basic Statistics\n\n\nIntroduction to Statistics\n\nWhat is Statistics and its Importance\n\nDefine statistics as the study of data collection, analysis, and interpretation.\nExplain the importance of statistics in decision-making and research.\nExample: Discuss how statistics are used in medical research.\n\nDescriptive vs. Inferential Statistics\n\nDifferentiate between descriptive and inferential statistics.\nEmphasize the role of each in understanding and drawing conclusions from data.\nExercise: Provide scenarios and determine whether descriptive or inferential statistics would be used.\n\nTypes of Data: Categorical and Numerical\n\nExplain the distinction between categorical and numerical data.\nProvide examples of each type of data.\nExercise: Classify given data sets as categorical or numerical.\n\n\nMeasures of Central Tendency\n\nMean, Median, and Mode\n\nDefine and explain the concepts of mean, median, and mode.\nIllustrate how to calculate each measure.\nExample: Calculate mean, median, and mode for a set of exam scores.\nExercise: Compute mean, median, and mode for different data sets.\n\nCalculating and Interpreting Each Measure\n\nDiscuss the interpretation of mean, median, and mode in terms of centrality.\nHighlight scenarios where each measure is useful.\nExercise: Analyze the implications of outliers on measures of central tendency.\n\nReal-World Examples\n\nProvide real-world examples where mean, median, and mode are applied.\nDiscuss their relevance in various contexts, such as finance or education.\nExercise: Analyze a dataset from a real-world scenario and calculate central tendency measures.\n\n\nMeasures of Dispersion\n\nRange, Variance, and Standard Deviation\n\nDefine and explain the concepts of range, variance, and standard deviation.\nDemonstrate how to calculate each measure.\nExample: Calculate the range, variance, and standard deviation for a data set.\nExercise: Compute range, variance, and standard deviation for different datasets.\n\nInterpreting Variability\n\nDiscuss the importance of measures of dispersion in understanding data spread.\nExplain how variability affects the interpretation of central tendency measures.\nExercise: Compare and contrast datasets with different measures of dispersion.\n\nVariance and Standard Deviation for Populations vs. Samples\n\nExplain the difference between calculating variance and standard deviation for populations and samples.\nDiscuss when to use the population formula versus the sample formula.\nExample: Calculate the population and sample variance and standard deviation for a dataset.\n\n\nQuantiles and Percentiles\n\nDefinition of Quantiles and Percentiles\n\nDefine quantiles and percentiles as measures of position in a dataset.\nExplain how they divide data into equal parts.\nExercise: Calculate the quartiles and percentiles for a dataset.\n\nCalculation and Interpretation\n\nDiscuss how to calculate quantiles and percentiles using order statistics.\nInterpret the meaning of specific quantiles and percentiles.\nExample: Calculate the interquartile range and 75th percentile for a dataset.\nExercise: Calculate and interpret quantiles and percentiles for various datasets.\n\nBox Plots and Their Use in Visualizing Quantiles\n\nExplain how box plots represent the five-number summary and outliers.\nDiscuss the components of a box plot (whiskers, box, median, outliers).\nExample: Create a box plot for a dataset and analyze its features.\nExercise: Construct box plots for given datasets and identify characteristics.\n\n\nConclusion and Recap\n\nSummarize the key concepts covered in the lecture.\nHighlight the importance of functions in various fields.\n\nQ&A Session"
  },
  {
    "objectID": "Teaching/Pre-sessionals/Stats2.html",
    "href": "Teaching/Pre-sessionals/Stats2.html",
    "title": "Pre-sessionals - Stats 2",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nInternet is a great resource. Use it. Some resources I like:\n\nMIT OpenCourseWare,\nStanford,\nHarvard.\n\n\n\n\n\n\n\n\n\n\nBeware, the Crime!\n\n\n\nIt is illegal to download articles and books from pages like LibGen, Sci-hub or from Telegram bots like @scihubot. Also, DO NOT use VPN to protect your freedom of education (Opera offers a free VPN).\n\n🙃\n\n\n\n\n\n\nExploring Data and Plots\n\n\nHistograms and Frequency Distributions\n\nCreating Histograms\n\nDefine histograms as graphical representations of data distributions.\nExplain the concept of bins and their role in constructing histograms.\nExample: Create a histogram for a set of exam scores.\nExercise: Construct a histogram for a given dataset.\n\nUnderstanding Frequency Distributions\n\nDefine frequency distributions as tables summarizing data frequency.\nExplain how to organize data into intervals and record frequencies.\nExample: Create a frequency distribution table for a dataset.\nExercise: Create frequency distribution tables for various datasets.\n\nChoosing Appropriate Bin Sizes\n\nDiscuss considerations for selecting bin sizes in histograms.\nExplain the trade-off between too few and too many bins.\nExercise: Determine suitable bin sizes for different datasets.\n\n\nBar Plots and Pie Charts\n\nConstructing Bar Plots for Categorical Data\n\nDefine bar plots as visual representations of categorical data.\nExplain how to create vertical and horizontal bar plots.\nExample: Construct a bar plot for survey responses.\nExercise: Create bar plots for given categorical data.\n\nInterpreting Pie Charts\n\nDefine pie charts as circular representations of parts of a whole.\nExplain how to calculate angles and percentages for each category.\nExample: Interpret a pie chart depicting distribution of expenses.\nExercise: Interpret and analyze pie charts.\n\nUse Cases and Limitations of Each Plot\n\nDiscuss when to use bar plots and pie charts based on data characteristics.\nHighlight limitations and potential misinterpretations of these plots.\nExercise: Determine which plot is more suitable for a given dataset.\n\n\nBox Plots (Box-and-Whisker Plots)\n\nDefinition and Components of a Box Plot\n\nDefine box plots as visualizations of the five-number summary.\nExplain the components: median, quartiles, whiskers, and outliers.\nExample: Describe the features of a box plot.\nExercise: Identify components of box plots from provided data.\n\nCreating Box Plots for Numerical Data\n\nExplain how to create a box plot using numerical data.\nDiscuss the process of identifying quartiles and outliers.\nExample: Create a box plot for a dataset of test scores.\nExercise: Construct box plots for given numerical datasets.\n\nIdentifying Median, Quartiles, Outliers, and Range\n\nDemonstrate how to locate median, quartiles, and outliers on a box plot.\nExplain how to calculate the interquartile range (IQR).\nExercise: Analyze box plots and calculate IQR for different datasets.\n\n\nScatter Plots and Correlation\n\nCreating Scatter Plots\n\nDefine scatter plots as representations of relationships between two numerical variables.\nExplain how to plot data points and interpret patterns.\nExample: Create a scatter plot for height and weight data.\nExercise: Create scatter plots for various pairs of numerical variables.\n\nPositive, Negative, and No Correlation\n\nDefine positive, negative, and no correlation between variables.\nExplain how to visually identify correlation patterns in scatter plots.\nExample: Interpret the correlation between study hours and exam scores.\nExercise: Determine correlation types in given scatter plots.\n\nCalculating and Interpreting Correlation Coefficients\n\nDefine the correlation coefficient and its range.\nExplain how to calculate and interpret correlation coefficients.\nExample: Calculate and interpret the correlation coefficient for a dataset.\nExercise: Calculate correlation coefficients and analyze their meanings.\n\n\nConclusion and Recap\n\nSummarize the key concepts covered in the lecture.\nHighlight the importance of functions in various fields.\n\nQ&A Session"
  },
  {
    "objectID": "Teaching/Pre-sessionals/Stats3.html",
    "href": "Teaching/Pre-sessionals/Stats3.html",
    "title": "Pre-sessionals - Stats 3",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nInternet is a great resource. Use it. Some resources I like:\n\nMIT OpenCourseWare,\nStanford,\nHarvard.\n\n\n\n\n\n\n\n\n\n\nBeware, the Crime!\n\n\n\nIt is illegal to download articles and books from pages like LibGen, Sci-hub or from Telegram bots like @scihubot. Also, DO NOT use VPN to protect your freedom of education (Opera offers a free VPN).\n\n🙃\n\n\n\n\n\n\nProbability and Reading Table\n\n\nIntroduction to Probability\n\nBasic Concepts of Probability\n\nDefine probability as the measure of likelihood of an event.\nIntroduce the concept of sample space and events.\nExample: Tossing a fair coin as a simple probability experiment.\n\nProbability as a Ratio\n\nExplain how probability is calculated as a ratio of favorable outcomes to total outcomes.\nDiscuss the range of probability from 0 to 1.\nExercise: Calculate the probability of rolling a specific number on a fair six-sided die.\n\nProbability vs. Odds\n\nDefine odds as the ratio of favorable outcomes to unfavorable outcomes.\nCompare and contrast probability and odds.\nExample: Compare the probability and odds of drawing a red card from a deck.\n\n\nProbability Distributions\n\nDiscrete vs. Continuous Probability Distributions\n\nDifferentiate between discrete and continuous random variables.\nDiscuss examples of each type of distribution.\nExercise: Identify whether given scenarios involve discrete or continuous random variables.\n\nProbability Mass Function (PMF) and Probability Density Function (PDF)\n\nExplain the concepts of PMF for discrete distributions and PDF for continuous distributions.\nDiscuss how they represent probabilities of specific outcomes and ranges.\nExample: Calculate and interpret the PMF of a binomial distribution.\nExercise: Determine the PDF of a given continuous distribution.\n\nExamples of Uniform, Binomial, and Normal Distributions\n\nIntroduce uniform, binomial, and normal distributions.\nExplain scenarios where each distribution is applicable.\nExample: Describe the characteristics of a normal distribution.\nExercise: Identify situations where specific distributions might be observed.\n\n\nReading Data Tables\n\nUnderstanding Data Tables and Formats\n\nDefine data tables as organized representations of data.\nExplain columns, rows, and headings in data tables.\nExercise: Interpret the structure of a given data table.\n\nExtracting Information from Frequency Tables\n\nDiscuss frequency tables and their role in summarizing categorical data.\nExplain how to calculate relative frequencies and percentages from frequency tables.\nExample: Calculate relative frequencies from a frequency table of survey responses.\nExercise: Calculate relative frequencies for various categories in a given frequency table.\n\nInterpreting Data Presented in Tabular Form\n\nGuide students in extracting meaningful insights from data tables.\nDiscuss patterns, trends, and comparisons that can be made using data tables.\nExample: Interpret a frequency table of ages in a population.\nExercise: Analyze and draw conclusions from a provided data table.\n\n\nConclusion and Recap\n\nSummarize the key concepts covered in the lecture.\nHighlight the importance of functions in various fields.\n\nQ&A Session"
  },
  {
    "objectID": "Teaching/FM250.html",
    "href": "Teaching/FM250.html",
    "title": "FM250 (Finance)",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nInternet is a great resource. Use it. Some resources I like:\n\nMIT OpenCourseWare,\nStanford,\nHarvard.\n\n\n\n\n\n\n\n\n\n\nBeware, the Crime!\n\n\n\nIt is illegal to download articles and books from pages like LibGen, Sci-hub or from Telegram bots like @scihubot. Also, DO NOT use VPN to protect your freedom of education (Opera offers a free VPN).\n\n🙃\n\n\n\n\n\n\nCourse content (Official)\n\n\nNet Present Value technique,\nIntroduction to portfolio theory,\nThe Capital Asset Pricing Model (CAPM),\nStock market efficiency,\nForward and futures contracts, option pricing,\nInvestment decisions and the significance of real options,\nCapital structure and dividend decisions,\nCapital restructuring: Initial Public Offering.\n\n\nCourse material (Partially mine)\n\nOf the following material, I only created the notes in the last column. The solution, assignment and lecture material were created by the lecturers. The lectures were presented by Professors Ashwini Agrawal and Linyan Zhu. My role was simply to present the solution to exercises in class. I created the notes that you can find in the last column.\n\n\n\nDay\nMaterial\nAssignment\nSolution\nPersonal notes\n\n\n\n\nDay 1\nSlides\nProblem set 1\nSolutions\nNotes 1\n\n\nDay 2\nSlides\nProblem set 2\nSolutions\nNotes 2\n\n\nDay 3\nSlides\nProblem set 3\nSolutions\nNotes 3\n\n\nDay 4\nSlides\nProblem set 4\nSolutions\nNotes 4\n\n\nDay 5\nSlides\nProblem set 5\nSolutions\nNotes 5\n\n\nDay 6\nSlides\nProblem set 6\nSolutions\nExam\n\n\nDay 7\nSlides\nProblem set 7\nSolutions\nNotes 7\n\n\nDay 8\nSlides\nProblem set 8\n\n8.1-8.2Notes\n\n\nDay 9\nSlides\nProblem set 9\nSolutions\nNotes 9\n\n\nDay 10\nSlides\nProblem set 10\nSolutions\nNotes 10\n\n\nDay 11\nSlides\nProblem set 11\nSolutions\nNotes 11\n\n\nDay 12\nSlides\nProblem set 12\nSolutions\nNotes 12"
  },
  {
    "objectID": "Teaching/MA423.html",
    "href": "Teaching/MA423.html",
    "title": "MA423 (Fundamentals of Operations Research)",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nInternet is a great resource. Use it. Some resources I like:\n\nMIT OpenCourseWare,\nStanford,\nHarvard.\n\n\n\n\n\n\n\n\n\n\nBeware, the Crime!\n\n\n\nIt is illegal to download articles and books from pages like LibGen, Sci-hub or from Telegram bots like @scihubot. Also, DO NOT use VPN to protect your freedom of education (Opera offers a free VPN).\n\n🙃\n\n\n\n\n\nThis file is all I have left from what I prepared for this course. I shall be more careful in keeping my documents in the future.\n\nCourse content (Official)\n\nAn introduction to a range of Operations Research techniques, covering: foundations of linear programming, including the simplex method and duality; integer programming; markov chains; queueing theory; dynamic programming; deterministic and stochastic inventory models; game theory."
  },
  {
    "objectID": "Teaching/ME306.html",
    "href": "Teaching/ME306.html",
    "title": "ME306 (Real Analysis)",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nInternet is a great resource. Use it. Some resources I like:\n\nMIT OpenCourseWare,\nStanford,\nHarvard.\n\n\n\n\n\n\n\n\n\n\nBeware, the Crime!\n\n\n\nIt is illegal to download articles and books from pages like LibGen, Sci-hub or from Telegram bots like @scihubot. Also, DO NOT use VPN to protect your freedom of education (Opera offers a free VPN).\n\n🙃\n\n\n\n\n\nI do not have anymore any material I created during the teaching of this course. I can quote however the course overview. Because the material is quite basic, I don’t think it’s worth the effort to go and try to find more details about what was done and taught.\n\nCourse content (Official)\n\nThe course provides a rigorous, but accessible, treatment of real analysis and analysis on metric spaces and will be delivered by formal lectures supported by interactive classes.\n\nBasics: proof, logic, sets and functions,\nReal numbers and sequences,\nFunctions, limits and continuity,\nInfinite series,\nMetric and normed spaces,\nConvergence, completeness and compactness,\nContinuity in metric spaces,\nThe derivative,\nConvexity,\nFixed point theorems."
  },
  {
    "objectID": "Teaching/MA210.html",
    "href": "Teaching/MA210.html",
    "title": "MA210 (Discrete Mathematics)",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nInternet is a great resource. Use it. Some resources I like:\n\nMIT OpenCourseWare,\nStanford,\nHarvard.\n\n\n\n\n\n\n\n\n\n\nBeware, the Crime!\n\n\n\nIt is illegal to download articles and books from pages like LibGen, Sci-hub or from Telegram bots like @scihubot. Also, DO NOT use VPN to protect your freedom of education (Opera offers a free VPN).\n\n🙃\n\n\n\n\n\n\nCourse content (Official)\n\nThis is a course covering a number of concepts and techniques of discrete mathematics. Topics covered: Counting: selections; inclusion-exclusion; generating functions; recurrence relations. Graph Theory: basic concepts; walks, paths, tours and cycles; trees and forests; colourings. Coding theory: basic concepts; linear codes.\n\nMaterial (Official and mine)\n\nThe following material should help the student preparing for this course. It contains both lecture notes, problem sets, and some solutions prepared by me.\n\nLecture Notes and Assignments\n\n\n\n\nWeeks Covered\nAuthor\nLink\n\n\n\n\n1-2\nPeter Allen\nlink\n\n\n3-4\nPeter Allen\nlink\n\n\n5-6\nPeter Allen\nlink\n\n\n7-8\nPeter Allen\nlink\n\n\n9-10\nPeter Allen\nlink\n\n\n\n\nMy solutions\n\nThe text of the exercises is in the links above.\n\n\n\nWeek\nSolution\n\n\n\n\nWeek 1\nMy solutions\n\n\nWeek 2\nMy solutions\n\n\nWeek 3\nMy solutions\n\n\nWeek 4\nMy solutions\n\n\nWeek 5\nMy solutions\n\n\nWeek 6\nMy solutions\n\n\nWeek 7\nMy solutions\n\n\nWeek 8\nMy solutions\n\n\nWeek 9\nMy solutions"
  },
  {
    "objectID": "Teaching/MA102_MA103.html",
    "href": "Teaching/MA102_MA103.html",
    "title": "MA102 - MA103 (Introduction to Abstract Mathematics)",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nInternet is a great resource. Use it. Some resources I like:\n\nMIT OpenCourseWare,\nStanford,\nHarvard.\n\n\n\n\n\n\n\n\n\n\nBeware, the Crime!\n\n\n\nIt is illegal to download articles and books from pages like LibGen, Sci-hub or from Telegram bots like @scihubot. Also, DO NOT use VPN to protect your freedom of education (Opera offers a free VPN).\n\n🙃\n\n\n\n\n\n\nCourse content (Official)\n\nThe course is an introduction to the use of formal definitions and proofs in mathematics, and to basic results of elementary set theory, number theory, linear algebra, algebra and analysis. Specific topics covered are as follows: Logic, sets and functions, relations, real numbers, infimum and supremum, sequences, limits and continuity, integers, prime numbers, greatest common divisor and modular arithmetic, complex numbers, groups and vector spaces.\n\nMaterial (Official and mine)\n\nThe following material should help the student preparing for this course. It contains both lecture notes, problem sets, and some solutions prepared by me.\n\nLecture Notes\n\n\n\n\nWeeks Covered\nAuthor\nLink\n\n\n\n\n1-5\nPeter Allen\nlink\n\n\n6-15\nAmol Sasane\nlink\n\n\n16-20\nAmol Sasane\nlink\n\n\n\n\nMy solutions (survived)\n\n\n\n\nWeek\nProblem set\nSolution\n\n\n\n\nWeek 1\nAssignment 1\nMy solutions\n\n\nWeek 2\nAssignment 2\nMy solutions\n\n\nWeek 3\nAssignment 3\nMy solutions\n\n\nWeek 4\nAssignment 4\nMy solutions\n\n\nWeek 5\nAssignment 5\nMy solutions\n\n\nWeek 7\nAssignment 7\nMy solutions\n\n\nWeek 10\nAssignment 10\nMy solutions"
  },
  {
    "objectID": "Teaching/ST455.html",
    "href": "Teaching/ST455.html",
    "title": "ST455 (Reinforcement Learning)",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nInternet is a great resource. Use it. Some resources I like:\n\nMIT OpenCourseWare,\nStanford,\nHarvard.\n\n\n\n\n\n\n\n\n\n\nBeware, the Crime!\n\n\n\nIt is illegal to download articles and books from pages like LibGen, Sci-hub or from Telegram bots like @scihubot. Also, DO NOT use VPN to protect your freedom of education (Opera offers a free VPN).\n\n🙃\n\n\n\n\n\n\nCourse content (Official)\n\nThis course is about reinforcement learning, covering the fundamental concepts of reinforcement learning framework and solution methods. The focus is on the underlying methodology as well as practical implementation and evaluation using software code. The course will cover the following topics:\n\nIntroduction: course overview.\nFoundations of reinforcement learning: Markov decision process, Bellman optimality equation, the existence of optimal stationary policy\nDynamic programing and Monte Carlo methods: policy evaluation, policy improvement, policy iteration, value iteration based on dynamic programming, and Monte Carlo methods for reinforcement learning, including Monte Carlo estimation and Monte Carlo control.\nTemporal difference learning: temporal difference learning, temporal difference prediction, Sarsa, Q-learning and n-step temporal difference predictions, TD(lambda).\nOn-policy prediction and control with approximation: types of function approximators (value and action-value function approximator), gradient based methods for value function prediction, convergence guarantees with linear function approximator, and semi-gradient n-step Sarsa.\nQ-learning type algorithms with function approximation: q-learning with linear function approximator, fitted q-iteration, deep q-network, double deep q-learning, convergence analysis.\nPolicy gradient methods: policy approximation, REINFORCE, actor-critic methods that combine policy function approximation with action-value function approximation.\nTrust-region policy optimization: monotonic improvement guarantee, trust-region policy optimization.\nBatch off-policy evaluation: importance sampling-based method, doubly robust method, marginalized importance sampling, double reinforcement learning.\nBatch policy optimisation: recent advances in offline reinforcement learning algorithms.\n\n\nMaterial and solutions\n\nThe material follows somehow the content of the famous book by Sutton and Barto. I did not write the following material. My role was simply to present the content of the seminars and the solutions in class.\n\n\n\nWeek\nLecture Notes\nAssignment\nSolutions\nSeminar Material\n\n\n\n\nWeek 1\nNotes\nAssignment\nSolutions\n\nPart 1\nPart 2\n\n\n\nWeek 2\nNotes\nAssignment\nSolutions\n\nPart 1\nPart 2\n\n\n\nWeek 3\nNotes\n\nSolutions\nMaterial\n\n\nWeek 4\nNotes\n\nSolutions\nMaterial\n\n\nWeek 5\nNotes\n\n\nMaterial\n\n\nWeek 6\nReading week\n\n\n\n\n\nWeek 7\nNotes\n\nSolutions\nMaterial\n\n\nWeek 8\nNotes\n\nSolutions\nMaterial\n\n\nWeek 9\nNotes\n\nSolutions\n\nPart 1\nPart 2\n\n\n\nWeek 10\nNotes\n\nSolutions\n\n\n\nWeek 11\nNotes\n\n\n\nPart 1\nPart 2"
  },
  {
    "objectID": "Teaching/Pre-sessionals.html",
    "href": "Teaching/Pre-sessionals.html",
    "title": "Pre-sessionals for Management",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nInternet is a great resource. Use it. Some resources I like:\n\nMIT OpenCourseWare,\nStanford,\nHarvard.\n\n\n\n\n\n\n\n\n\n\nBeware, the Crime!\n\n\n\nIt is illegal to download articles and books from pages like LibGen, Sci-hub or from Telegram bots like @scihubot. Also, DO NOT use VPN to protect your freedom of education (Opera offers a free VPN).\n\n🙃\n\n\n\n\n\nThis Pre-sessional course consists of two section, four hours each. In the first section, we are gonig to see an introduction to fundamental mathematical tools, in the second, we are going to examin fundamentals of statistics.\nA summary of the course is as follows:\n\nVery genericMathsStatistics\n\n\n\nMaths\n\n\nIntroduction to Functions and Linear Equations\nIntroduction to Derivatives\nSystems of Linear Equations\nSupply and Demand Problems\n\n\nStatistics\n\n\nIntroduction to Basic Statistics\nExploring Data and Plots\nProbability and Reading Table\n\n\n\n\nMaths\n\nLecture 1: Introduction to Functions and Linear Equations (1 hour)\n\nIntroduction to Functions:\n\nWhat is a function?\nDomain and range of a function.\nNotation: \\(f(x)\\), domain, range, etc.\n\nLinear Functions:\n\nDefinition of linear functions.\nGraphing linear functions.\nFinding the slope and \\(y\\)-intercept.\nWriting equations in slope-intercept form: \\(y = mx + b\\).\n\nPolynomial Functions of Degree 2:\n\nDefinition of polynomial functions.\nQuadratic functions: \\(f(x) = ax^2 + bx + c\\).\nGraphing quadratic functions.\nFinding the vertex, axis of symmetry, and intercepts.\n\nApplications of Functions:\n\nReal-world examples of functions.\nModeling with linear and quadratic functions.\nSimple problems involving functions.\n\n\nLecture 2: Introduction to Derivatives (1 hour)\n\nIntroduction to Derivatives:\n\nWhat is a derivative?\nThe concept of instantaneous rate of change.\nNotation: \\(f'(x)\\), \\(df/dx\\).\n\nDerivatives of Linear and Quadratic Functions:\n\nFinding the derivative of linear functions.\nFinding the derivative of quadratic functions.\nTangent lines and rates of change.\n\nBasic Rules of Differentiation:\n\nPower rule for differentiation.\nConstant rule and sum rule.\nDifferentiating polynomial functions.\n\nApplications of Derivatives:\n\nFinding maxima and minima using derivatives.\nTangent lines as approximations.\nSimple optimization problems.\n\n\nLecture 3: Systems of Linear Equations (1 hour)\n\nIntroduction to Systems of Equations:\n\nWhat is a system of equations?\nMethods for solving systems: graphing, substitution, elimination.\n\nGraphical Solution:\n\nSolving systems graphically.\nInterpreting solutions on graphs.\n\nSubstitution and Elimination Methods:\n\nSolving systems using substitution method.\nSolving systems using elimination (addition) method.\n\nApplications of Systems of Equations:\n\nReal-world examples of systems.\nUsing systems to solve practical problems (e.g., mixtures, interest).\n\n\nLecture 4: Supply and Demand Problems (1 hour)\n\nIntroduction to Supply and Demand:\n\nBasic concepts of supply and demand.\nEquilibrium point and market equilibrium.\n\nSupply and Demand Equations:\n\nModeling supply and demand using linear equations.\nEquilibrium price and quantity.\n\nShifts in Supply and Demand:\n\nFactors causing shifts in supply and demand curves.\nEffects on equilibrium price and quantity.\n\nApplications to Real-World Scenarios:\n\nApplying supply and demand analysis to real-world scenarios (e.g., price ceilings, shortages, surpluses).\nUnderstanding market dynamics and changes.\n\n\n\n\n\nStatistics\n\nLecture 1: Introduction to Basic Statistics (2 hours)\n\nIntroduction to Statistics:\n\nWhat is statistics and its importance.\nDescriptive vs. inferential statistics.\nTypes of data: categorical and numerical.\n\nMeasures of Central Tendency:\n\nMean, median, and mode.\nCalculating and interpreting each measure.\nReal-world examples.\n\nMeasures of Dispersion:\n\nRange, variance, and standard deviation.\nInterpreting variability.\nVariance and standard deviation for populations vs. samples.\n\nQuantiles and Percentiles:\n\nDefinition of quantiles and percentiles.\nCalculation and interpretation.\nBox plots and their use in visualizing quantiles.\n\n\nLecture 2: Exploring Data and Plots (1 hour)\n\nHistograms and Frequency Distributions:\n\nCreating histograms.\nUnderstanding frequency distributions.\nChoosing appropriate bin sizes.\n\nBar Plots and Pie Charts:\n\nConstructing bar plots for categorical data.\nInterpreting pie charts.\nUse cases and limitations of each plot.\n\nBox Plots (Box-and-Whisker Plots):\n\nDefinition and components of a box plot.\nCreating box plots for numerical data.\nIdentifying median, quartiles, outliers, and range.\n\nScatter Plots and Correlation:\n\nCreating scatter plots.\nPositive, negative, and no correlation.\nCalculating and interpreting correlation coefficients.\n\n\nLecture 3: Probability and Reading Tables (1 hour)\n\nIntroduction to Probability:\n\nBasic concepts of probability.\nProbability as a ratio.\nProbability vs. odds.\n\nProbability Distributions:\n\nDiscrete vs. continuous probability distributions.\nProbability mass function (PMF) and probability density function (PDF).\nExamples: uniform, binomial, and normal distributions.\n\nReading Data Tables:\n\nUnderstanding data tables and formats.\nExtracting information from frequency tables.\nInterpreting data presented in tabular form.\n\n\n\n\n\n\nAssessmentsMathsStatistics\n\n\nYou can find here some questions that will assess your previous knowledge of these topics. Do not worry if you cannot answer these, as we will go through the knowledge required to answer them during the lectures.\n\n\nPre-Lecture Assessment: Mathematics Knowledge Check\n\nWhich of the following equations represents a quadratic function?\n\n\\(y = 3x + 2\\)\n\\(y = x^3 - 2x\\)\n\\(y = 2x - 5\\)\n\\(y = 4x^2 - 7x + 1\\)\n\nWhat is the y-intercept of the linear function \\(y = 2x - 3\\)?\n\n\\((2, -3)\\)\n\\((-2, 3)\\)\n\\((0, -3)\\)\n\\((0, 2)\\)\n\nIf \\(f(x) = 4x^3 - 2x^2 + 7x - 1\\), what is the derivative \\(f'(x)\\)?\n\n\\(12x^2 - 4x + 7\\)\n\\(12x^2 - 4x - 7\\)\n\\(12x^3 - 4x^2 + 7\\)\n\\(12x^3 - 4x^2 - 7\\)\n\nHow many solutions can a system of linear equations have if the lines are parallel and distinct?\n\nOne solution\nTwo solutions\nInfinite solutions\nNo solution\n\nSolve the quadratic equation for \\(x\\): \\(x^2 - 9 = 0\\).\n\n\\(x = -3\\)\n\\(x = 0\\)\n\\(x = 3\\)\n\\(x = \\pm 3\\)\n\nWhich of the following equations represents a vertical line?\n\n\\(y = 2x + 1\\)\n\\(x = -3\\)\n\\(y = x^2 - 4\\)\n\\(y = -2x\\)\n\nGiven the system of two equations: \\(2x + y = 5\\) and \\(3x + 2y = 8\\). What is the solution \\((x, y)\\)?\n\n\\((1, 3)\\)\n\\((2, 1)\\)\n\\((3, 2)\\)\n\\((-1, -3)\\)\n\nWhat is the slope of the line passing through the points \\((2, 4)\\) and \\((4, 8)\\)?\n\n\\(2\\)\n\\(4\\)\n\\(1/2\\)\n\\(1\\)\n\nWhat is the mean of the numbers \\(5, 8, 10, 12\\), and \\(15\\)?\n\n\\(8\\)\n\\(10\\)\n\\(11\\)\n\\(12\\)\n\n\n\n\nPre-Lecture Assessment: Statistics Knowledge Check\n\nWhat is the mean of the numbers 5, 8, 10, 12, and 15?\n\n8\n10\n11\n12\n\nIf the range of a data set is 20 and the smallest value is 5, what is the largest value?\n\n20\n25\n15\n10\n\nWhich statistical measure is most affected by outliers?\n\nMean\nMedian\nMode\nRange\n\nWhat is the purpose of a scatter plot?\n\nTo display categorical data.\nTo show the relationship between two numerical variables.\nTo represent the distribution of a single variable.\nTo compare different groups of data.\n\nWhich of the following represents a positively skewed distribution?\n\nMean &lt; Median\nMean &gt; Median\nMean = Median\nNo relationship between Mean and Median\n\nWhat does a correlation coefficient of -0.85 indicate?\n\nStrong positive correlation\nModerate negative correlation\nWeak positive correlation\nStrong negative correlation\n\nIf two events are independent, what is the probability of both events occurring?\n\nP(A and B) = P(A) + P(B)\nP(A and B) = P(A) * P(B)\nP(A and B) = P(A) - P(B)\nP(A and B) = P(A) / P(B)\n\nWhat is the main purpose of calculating quantiles in statistics?\n\nTo find the mean of the data.\nTo identify the median of the data.\nTo measure the spread of the data.\nTo divide the data into equal parts.\n\nWhat is the difference between variance and standard deviation?\n\nVariance is the square root of standard deviation.\nVariance measures spread, while standard deviation measures average deviation.\nVariance is always positive, while standard deviation can be negative.\nVariance is used for categorical data, while standard deviation is used for numerical data.\n\nIn a normal distribution, what percentage of data falls within one standard deviation from the mean?\n\nApproximately 34%\nApproximately 68%\nApproximately 95%\nApproximately 99.7%"
  },
  {
    "objectID": "Teaching/ME200.html",
    "href": "Teaching/ME200.html",
    "title": "ME200 (Computational Methods in Financial Mathematics)",
    "section": "",
    "text": "Contact\n\n\n\n\nName: Domenico Mergoni\nEmail: d.mergoni -at- lse.ac.uk\nWork: London School of Economics\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nInternet is a great resource. Use it. Some resources I like:\n\nMIT OpenCourseWare,\nStanford,\nHarvard.\n\n\n\n\n\n\n\n\n\n\nBeware, the Crime!\n\n\n\nIt is illegal to download articles and books from pages like LibGen, Sci-hub or from Telegram bots like @scihubot. Also, DO NOT use VPN to protect your freedom of education (Opera offers a free VPN).\n\n🙃\n\n\n\n\n\n\nCourse content (Official)\n\n\nMethods for generating samples from a given probability distribution,\nMonte Carlo estimation,\nVariance reduction techniques,\nThe binomial asset pricing model and the concept of no-arbitrage,\nThe Black-Scholes option pricing model as a limit of the binomial model,\nApplication of Monte Carlo methods to pricing financial derivatives,\nIntroduction to programming in Python,\nIntroduction to option pricing with multiple periods in financial markets.\n\n\nLecture Notes\n\nThese are the lecture notes, by Johannes Ruf and Luitgard Veraart. This is the collection of the assignments due for this course.\n\nSolutions\n\nI did not write the following solutions. My role was simply to present them in class. The text of the exercises is in the link above.\n\n\n\nDay\nSolution\n\n\n\n\nDay 1\nSolutions\n\n\nDay 2\nSolutions\n\n\nDay 3\nSolutions\n\n\nDay 4\nSolutions\n\n\nDay 5\nSolutions\n\n\nDay 6\nSolutions\n\n\nDay 7\nSolutions\n\n\nDay 8\nSolutions\n\n\nDay 9\nSolutions\n\n\nDay 10\nSolutions\n\n\nDay 11\nSolutions\n\n\nDay 12\nSolutions"
  }
]