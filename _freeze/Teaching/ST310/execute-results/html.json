{
  "hash": "bd1420a1a0f9d00870c8a3dac83c33a9",
  "result": {
    "markdown": "---\ntitle: \"ST310 <small><small><small>(Machine Learning)</small></small></small>\"\nformat: html\n---\n::: column-margin\n::: callout-note\n## Contact\n- Name: Domenico Mergoni\n- Email: d.mergoni -at- lse.ac.uk\n- Work: [London School of Economics](https://www.lse.ac.uk/mathematics)\n:::\n:::\n\n::: column-margin\n::: callout-tip\n## Tip\n\nInternet is a great resource. Use it. Some resources I like:\n\n1.  [MIT OpenCourseWare](https://www.youtube.com/c/mitocw/featured),\n2.  [Stanford](https://www.youtube.com/c/stanford/featured),\n3.  [Harvard](https://www.youtube.com/c/harvard).\n:::\n:::\n\n::: column-margin\n::: callout-warning\n## Beware, the Crime!\n\nIt is illegal to download articles and books from pages like [LibGen](http://libgen.li/), [Sci-hub](https://sci-hub.se/) or from Telegram bots like [\\@scihubot](http://t.me/scihubot). Also, DO NOT use VPN to protect your freedom of education ([Opera](https://www.opera.com) offers a free VPN).\n:::\n:::\n\n\n\n\n<h3>Course content <small><small><small>(Official)</small></small></small></h3>\n\nThe primary focus of this course is on the core machine learning techniques in the context of high-dimensional or large datasets (i.e. big data). The first part of the course covers elementary and important statistical methods including nearest neighbours, linear regression, logistic regression, regularisation, cross-validation, and variable selection. The second part of the course deals with more advanced machine learning methods including regression and classification trees, random forests, bagging, boosting, deep neural networks, k-means clustering and hierarchical clustering. The course will also introduce causal inference motivated by analogy between double machine learning and two-stage least squares. All the topics will be delivered using illustrative real data examples. Students will also gain hands-on experience using R or Python (programming languages and software environments for data analysis, computing and visualisation).\n\n\n\n\n::: {.cell file='../Documents_files/ST310/Week1.Rmd' code-folding='true'}\n\n```{.r .cell-code}\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse) # should've already install.packages(\"tidyverse\")\nlibrary(gapminder) # might need to install.packages(\"gapminder\")\nlibrary(broom) # might need to install.packages(\"broom\")\ntheme_set(theme_minimal()) \n```\n\nIn the examples below you may need to replace `...` before running the code.\n\nClick the triangle in the upper right of a code chunk to run that chunk\n\n## Repeating the gapminder analysis\n\nAfter running `library(gapminder)` you can access the dataset. Try typing `View(gapminder)` in the Console window below.\n\n#### Create a `gapminder` scatterplot (`geom_point`) using data from the year 2002\n\nFirst create an object with the right subset of data.\n\n```{r}\ngm2002 <- gapminder |>\n  filter(...)\n```\n\nNow use this data to create the scatterplot.\n\n```{r}\ngm_scatterplot <- \n  ggplot(gm2002, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point()\ngm_scatterplot\n```\n\n#### Create an `lm` model to predict `lifeExp` from `gdpPercap`\n\n```{r}\nmodel_lm <- lm(lifeExp ~ gdpPercap, data = gm2002)\npredictions_lm <- augment(model_lm)\n```\n\n#### Create a `loess` model to do the same\n\n```{r}\nmodel_loess <- loess(lifeExp ~ gdpPercap, span = .75, data = gm2002)\npredictions_loess <- augment(model_loess)\n```\n\n#### Plot showing the two models\n\n```{r}\ngm_scatterplot +\n  geom_line(data = predictions_lm, size = 1,\n            color = \"blue\",\n            linetype = \"dashed\",\n            aes(x = gdpPercap, y = .fitted)) +\n  geom_line(data = predictions_loess, size = 1,\n            color = \"green\",\n            aes(x = gdpPercap, y = .fitted))  \n```\n\nNow try changing `span` in the `loess()` function to some other number (strictly) between 0 and 1, and re-run the code chunks. First re-run the code chunk that fits the model with `loess()`, then re-run the chunk that plots the predictions. Observe how the fitted curve is different.\n\n#### Compute the mean square of `residuals()` of each model.\n\n```{r}\nmean(residuals(model_lm)^2)\nmean(...)\n```\n\nWhich model is better? Why?\n\n### Predicting on new data\n\nModels are supposed to capture/use structure in the data that corresponds to structure in the real world. And if the real world isn't misbehaving, that structure should be somewhat stable.\n\nFor example, suppose the relationship changed dramatically from one time period to another time period. Then it would be less useful/interesting to have a model fit on data at one time period, because the same model might have a poor fit on data from a different time period.\n\nLet's explore this with our `gapminder` models\n\n#### Predictions on different years\n\nCreate a dataset for the year 1997\n\n```{r}\ngm1997 <- ...\n```\n\nGet predictions for 1997 from both models by predicting with the `newdata` argument\n\n```{r}\nlm_predict1997 <- augment(model_lm, newdata = gm1997)\nloess_predict1997 <- augment(model_loess, newdata = gm1997)\n```\n\n#### Check MSE\n\n```{r}\nlm_predict1997 |> \n  summarize(MSE = mean(.resid^2))\n```\n\n```{r}\n... |> \n  summarize(MSE = mean(.resid^2))\n```\n\n**Question**: Is it surprising which model does better? Why or why not?\n\nThe more complex, `loess` model performs better than the linear model even when tested on different data. Apparently the association between `gdpPercap` and `lifeExp` is fairly stable over time.\n\n## Simulation\n\n### Generate data from a non-linear function\n\nChoose a function. Be creative! Type `?sqrt` in the Console. Also try `?log` and `?sin`. \n\n```{r}\nf <- function(x) x^2 # change this\n```\n\n#### Data generating process\n\nCome back here, change things, and re-run the code chunks below to experiment.\n\nType `?Distributions` in the Console to see other random variables you can choose from. The function name starting with `r` generates a random sample, e.g. `rexp` generates a sample from an exponential random variable.\n\n```{r}\nsimulate_data <- function(n) {\n  x <- runif(n, min = 0, max = 3) # change\n  errors <- rnorm(n, sd = 1) # change\n  y <- ... # what should this be?\n  data.frame(x = x, y = y)\n}\n```\n\nGenerate and plot data\n\n```{r}\nn <- 100 # change\nsim_data <- simulate_data(n)\nsim_plot <- ggplot(sim_data, aes(x = x, y = y)) +\n  geom_point()\nsim_plot\n```\n\n#### Fit `lm` and `loess` models to the simulated data and plot them\n\n(Hint: copy and paste your earlier code, change `gm2002` and other variable names)\n\n```{r}\nmodel_sim_lm <- lm(y ~ x, data = sim_data)\npredictions_sim_lm <- augment(model_sim_lm)\nmodel_sim_loess <- loess(y ~ x,\n                         span = .75, # change\n                         data = sim_data)\npredictions_sim_loess <- augment(model_sim_loess)\n```\n\n```{r}\nsim_plot +\n  geom_line(data = predictions_sim_lm, size = 1,\n            color = \"blue\",\n            linetype = \"dashed\",\n            aes(x = x, y = .fitted)) +\n  geom_line(data = predictions_sim_loess, size = 1,\n            color = \"green\",\n            linetype = \"dotdash\",\n            aes(x = x, y = .fitted)) +\n  geom_function(fun = f, color = \"purple\", size = 1)\n```\n\n```{r}\ncoef(model_sim_lm)\n```\n\n```{r}\ncoef(model_sim_lm)[2]\n```\n\n**Question**: How is this slope related to the true function `f`?\n\n\n### Sampling distribution of an estimator\n\n```{r}\nn_iters <- 5\nsimulate_slope <- function(n) {\n  new_data <- simulate_data(n)\n  new_model <- lm(y ~ x, new_data)\n  return(...) # output the estimated coefficient\n}\nreplicate(n_iters, simulate_slope(n))\n```\n\n#### Histogram of many estimates\n\n```{r}\nn_iters <- 200\nmany_slopes <- replicate(...) \nqplot(many_slopes, bins = 20)\n```\n\n\n**Question**: What do you notice about this distribution, and why?\n\n\n#### Experiment\n\nGo back and change some inputs in previous code chunks and re-run all the chunks after that change.\n\nCan you find anything that seems like it makes an important, qualitative difference in the conclusions?\n```\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}