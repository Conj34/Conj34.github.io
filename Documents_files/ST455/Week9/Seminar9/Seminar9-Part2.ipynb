{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "684dd2a1",
   "metadata": {},
   "source": [
    "**Actor-Critic methods**\n",
    "\n",
    "Actor-Critic methods are policy gradient methods that represent the policy function independent of the value function. \n",
    "\n",
    "A policy function (or policy) returns a probability distribution over actions that the agent can take based on the given state.\n",
    "A value function determines the expected return for an agent starting at a given state and acting according to a particular policy forever after.\n",
    "\n",
    "In the Actor-Critic method, the policy is referred to as the *actor* that proposes a set of possible actions given a state, and the estimated value function is referred to as the *critic*, which evaluates actions taken by the *actor* based on the given policy.\n",
    "\n",
    "In this tutorial, both the *Actor* and *Critic* will be represented using neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502381ca",
   "metadata": {},
   "source": [
    "**CartPole-v1**\n",
    "\n",
    "In the [CartPole-v1 environment](https://gym.openai.com/envs/CartPole-v1), a pole is attached to a cart moving along a frictionless track. \n",
    "The pole starts upright and the goal of the agent is to prevent it from falling over by applying a force of -1 or +1 to the cart. \n",
    "A reward of +1 is given for every time step the pole remains upright.\n",
    "An episode ends when (1) the pole is more than 15 degrees from vertical or (2) the cart moves more than 2.4 units from the center.\n",
    "\n",
    "<center>\n",
    "  <figure>\n",
    "    <img src=\"graphs/cartpole-v0.gif\">\n",
    "    <figcaption>\n",
    "      Trained actor-critic model in Cartpole-v1 environment\n",
    "    </figcaption>\n",
    "  </figure>\n",
    "</center>\n",
    "\n",
    "The problem is considered \"solved\" when the average total reward for the episode reaches 495 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169aebdd",
   "metadata": {},
   "source": [
    "The code below is from this [repository](https://github.com/yc930401/Actor-Critic-pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d663710",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import necessary packages and configure global settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "244d12b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, os\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\").unwrapped\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9717fc",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The *Actor* and *Critic* will be modeled using neural networks that generates the action probabilities and critic value respectively.  \n",
    "\n",
    "During the forward pass, the model will take in the state as the input and will output both action probabilities and critic value $V$, which models the state-dependent [value function](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#value-functions). The goal is to train a model that chooses actions based on a policy $\\pi$ that maximizes expected [return](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#reward-and-return).\n",
    "\n",
    "For Cartpole-v1, there are four values representing the state: cart position, cart-velocity, pole angle and pole velocity respectively. The agent can take two actions to push the cart left (0) and right (1) respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30320e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.linear3 = nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.linear3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e237e1d9",
   "metadata": {},
   "source": [
    "### Computing expected returns\n",
    "\n",
    "The sequence of rewards for each timestep $t$, $\\{r_{t}\\}^{T}_{t=1}$ collected during one episode is converted into a sequence of expected returns $\\{G_{t}\\}^{T}_{t=1}$ in which the sum of rewards is taken from the current timestep $t$ to $T$ and each reward is multiplied with an exponentially decaying discount factor $\\gamma$:\n",
    "\n",
    "$$G_{t} = \\sum^{T}_{t'=t} \\gamma^{t'-t}r_{t'}$$\n",
    "\n",
    "Since $\\gamma\\in(0,1)$, rewards further out from the current timestep are given less weight.\n",
    "\n",
    "Intuitively, expected return simply implies that rewards now are better than rewards later. In a mathematical sense, it is to ensure that the sum of the rewards converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c550e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a37422",
   "metadata": {},
   "source": [
    "### The actor-critic loss\n",
    "\n",
    "#### Actor loss\n",
    "\n",
    "The actor loss is based on [policy gradients with the critic as a state dependent baseline](https://www.youtube.com/watch?v=EKqxumCuAAY&t=62m23s) and computed with single-sample (per-episode) estimates.\n",
    "\n",
    "$$L_{actor} = -\\sum^{T}_{t=1} \\log\\pi_{\\theta}(a_{t} | s_{t})[G(s_{t}, a_{t})  - V^{\\pi}_{\\theta}(s_{t})]$$\n",
    "\n",
    "where:\n",
    "- $T$: the number of timesteps per episode, which can vary per episode\n",
    "- $s_{t}$: the state at timestep $t$\n",
    "- $a_{t}$: chosen action at timestep $t$ given state $s$\n",
    "- $\\pi_{\\theta}$: is the policy (actor) parameterized by $\\theta$\n",
    "- $V^{\\pi}_{\\theta}$: is the value function (critic) also parameterized by $\\theta$\n",
    "- $G = G_{t}$: the expected return for a given state, action pair at timestep $t$\n",
    "\n",
    "A negative term is added to the sum since the idea is to maximize the probabilities of actions yielding higher rewards by minimizing the combined loss.\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Advantage\n",
    "\n",
    "The $G - V$ term in our $L_{actor}$ formulation is called the [advantage](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#advantage-functions), which indicates how much better an action is given a particular state over a random action selected according to the policy $\\pi$ for that state.\n",
    "\n",
    "While it's possible to exclude a baseline, this may result in high variance during training. And the nice thing about choosing the critic $V$ as a baseline is that it trained to be as close as possible to $G$, leading to a lower variance.\n",
    "\n",
    "In addition, without the critic, the algorithm would try to increase probabilities for actions taken on a particular state based on expected return, which may not make much of a difference if the relative probabilities between actions remain the same.\n",
    "\n",
    "For instance, suppose that two actions for a given state would yield the same expected return. Without the critic, the algorithm would try to raise the probability of these actions based on the objective $J$. With the critic, it may turn out that there's no advantage ($G - V = 0$) and thus no benefit gained in increasing the actions' probabilities and the algorithm would set the gradients to zero.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Critic loss\n",
    "\n",
    "Training $V$ to be as close possible to $G$ can be set up as a regression problem with the following loss function:\n",
    "\n",
    "$$L_{critic} = L_{\\delta}(G, V^{\\pi}_{\\theta})$$\n",
    "\n",
    "where $L_{\\delta}$ is the squared-error loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c03485c",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "To train the agent, you will follow these steps:\n",
    "\n",
    "1. Run the agent on the environment to collect training data per episode.\n",
    "2. Compute expected return at each time step.\n",
    "3. Compute the loss for the actor and critic models.\n",
    "4. Compute gradients and update network parameters.\n",
    "5. Repeat 1-4 until either success criterion or max episodes has been reached.\n",
    "\n",
    "Note that this method differs algorithmically from the lecture and also from the Sutton-Barto Book. The weights are updated online after each step, while the implementation in this Notebook only updates parameters in an offline manner at the end of an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fcfa32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(actor, critic, n_iters):\n",
    "    optimizerA = optim.Adam(actor.parameters())\n",
    "    optimizerC = optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        state = state[0]\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = compute_returns(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc7b794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shic6/opt/anaconda3/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py:211: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Score: 12\n",
      "Iteration: 1, Score: 13\n",
      "Iteration: 2, Score: 32\n",
      "Iteration: 3, Score: 33\n",
      "Iteration: 4, Score: 19\n",
      "Iteration: 5, Score: 21\n",
      "Iteration: 6, Score: 15\n",
      "Iteration: 7, Score: 24\n",
      "Iteration: 8, Score: 11\n",
      "Iteration: 9, Score: 20\n",
      "Iteration: 10, Score: 9\n",
      "Iteration: 11, Score: 14\n",
      "Iteration: 12, Score: 44\n",
      "Iteration: 13, Score: 87\n",
      "Iteration: 14, Score: 37\n",
      "Iteration: 15, Score: 16\n",
      "Iteration: 16, Score: 10\n",
      "Iteration: 17, Score: 11\n",
      "Iteration: 18, Score: 15\n",
      "Iteration: 19, Score: 16\n",
      "Iteration: 20, Score: 50\n",
      "Iteration: 21, Score: 39\n",
      "Iteration: 22, Score: 28\n",
      "Iteration: 23, Score: 13\n",
      "Iteration: 24, Score: 12\n",
      "Iteration: 25, Score: 10\n",
      "Iteration: 26, Score: 27\n",
      "Iteration: 27, Score: 12\n",
      "Iteration: 28, Score: 53\n",
      "Iteration: 29, Score: 12\n",
      "Iteration: 30, Score: 30\n",
      "Iteration: 31, Score: 48\n",
      "Iteration: 32, Score: 32\n",
      "Iteration: 33, Score: 14\n",
      "Iteration: 34, Score: 30\n",
      "Iteration: 35, Score: 27\n",
      "Iteration: 36, Score: 25\n",
      "Iteration: 37, Score: 63\n",
      "Iteration: 38, Score: 25\n",
      "Iteration: 39, Score: 26\n",
      "Iteration: 40, Score: 43\n",
      "Iteration: 41, Score: 10\n",
      "Iteration: 42, Score: 11\n",
      "Iteration: 43, Score: 51\n",
      "Iteration: 44, Score: 13\n",
      "Iteration: 45, Score: 15\n",
      "Iteration: 46, Score: 48\n",
      "Iteration: 47, Score: 58\n",
      "Iteration: 48, Score: 21\n",
      "Iteration: 49, Score: 59\n",
      "Iteration: 50, Score: 29\n",
      "Iteration: 51, Score: 12\n",
      "Iteration: 52, Score: 47\n",
      "Iteration: 53, Score: 43\n",
      "Iteration: 54, Score: 17\n",
      "Iteration: 55, Score: 18\n",
      "Iteration: 56, Score: 12\n",
      "Iteration: 57, Score: 19\n",
      "Iteration: 58, Score: 30\n",
      "Iteration: 59, Score: 14\n",
      "Iteration: 60, Score: 53\n",
      "Iteration: 61, Score: 20\n",
      "Iteration: 62, Score: 42\n",
      "Iteration: 63, Score: 58\n",
      "Iteration: 64, Score: 55\n",
      "Iteration: 65, Score: 50\n",
      "Iteration: 66, Score: 25\n",
      "Iteration: 67, Score: 27\n",
      "Iteration: 68, Score: 36\n",
      "Iteration: 69, Score: 27\n",
      "Iteration: 70, Score: 83\n",
      "Iteration: 71, Score: 90\n",
      "Iteration: 72, Score: 20\n",
      "Iteration: 73, Score: 26\n",
      "Iteration: 74, Score: 90\n",
      "Iteration: 75, Score: 107\n",
      "Iteration: 76, Score: 83\n",
      "Iteration: 77, Score: 65\n",
      "Iteration: 78, Score: 110\n",
      "Iteration: 79, Score: 14\n",
      "Iteration: 80, Score: 63\n",
      "Iteration: 81, Score: 109\n",
      "Iteration: 82, Score: 26\n",
      "Iteration: 83, Score: 42\n",
      "Iteration: 84, Score: 42\n",
      "Iteration: 85, Score: 176\n",
      "Iteration: 86, Score: 66\n",
      "Iteration: 87, Score: 34\n",
      "Iteration: 88, Score: 157\n",
      "Iteration: 89, Score: 251\n",
      "Iteration: 90, Score: 159\n",
      "Iteration: 91, Score: 202\n",
      "Iteration: 92, Score: 132\n",
      "Iteration: 93, Score: 196\n",
      "Iteration: 94, Score: 70\n",
      "Iteration: 95, Score: 80\n",
      "Iteration: 96, Score: 147\n",
      "Iteration: 97, Score: 173\n",
      "Iteration: 98, Score: 112\n",
      "Iteration: 99, Score: 146\n"
     ]
    }
   ],
   "source": [
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "trainIters(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b2d2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
