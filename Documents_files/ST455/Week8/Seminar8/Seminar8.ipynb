{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acoustic-lighter",
   "metadata": {},
   "source": [
    "# LSE ST455: Reinforcement Learning\n",
    "\n",
    "## Seminar 8: Deep Q Networks Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-connecticut",
   "metadata": {},
   "source": [
    "# Deep Q-Networks (DQN)\n",
    "\n",
    "![](graphs/dqn_benchmarks.png)\n",
    "\n",
    "\n",
    "This exercise aims to replicate the results from the paper [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236) published in Nature. This paper develops a DQN that can learn successful policy directly from high-dimensional inputs using end-to-end reinforcement learning. The original DQN algorithm is implemented on the Atari games. Note that as training for Breakout takes many hours we will not be able to analyze the results in class. As such, we will implement the algorithm on LunarLander instead. \n",
    "\n",
    "We first review the DQN algorithm. \n",
    "\n",
    "### Basic ideas of DQN\n",
    "\n",
    "The $Q$ function for Deep Q-Networks is approximated by a neural network architecture\n",
    "\n",
    "![](graphs/neural_network.png)\n",
    "\n",
    "#### Experience replay via minibatches\n",
    "\n",
    "Recall that past state-action-reward tuples are stored in a memory. Random samples are collected to form minibatches on which the parameter updates (e.g. via Stochastic Gradient Descent) are computed. The size of the minibatches trades of a strong variance (too few samples) and computational cost (too many samples)\n",
    "\n",
    "<img src=\"graphs/minibatch.png\" alt=\"Drawing\" style=\"width: 750px;\"/>\n",
    "<img src=\"graphs/experience_replay.png\" alt=\"Drawing\" style=\"width: 750px;\"/>\n",
    "\n",
    "#### Using two cloned networks - Policy and Target Networks\n",
    "\n",
    "![](graphs/target_network.png)\n",
    "\n",
    "A source that may cause instability is the correlation between the estimates of the action-value function and the target. \n",
    "For example, the DQN loss function at iteration $t$ is defined as\n",
    "\n",
    "$$\\mathcal{L}_t(\\theta_t)=\\mathbb{E}_{(s,a,r,s'){\\sim}U(D)}[(r+{\\gamma}\\max_{a'}\\hat{Q}(s',a';\\theta_t)-\\hat{Q}(s,a;\\theta_t))^2]$$\n",
    "\n",
    "where $r+{\\gamma}\\max_{a'}\\hat{Q}(s',a';\\theta_t)$ is the Q-learning target. \n",
    "In DQN, both $Q(s',a';\\theta_t)$ that estimates the Q-learning target and \n",
    "$Q(s,a;\\theta_t)$ that generates actions are parametrised using convolutional neural \n",
    "networks where $\\theta$ are the network parameters. \n",
    "The network that estimates the Q-learning target is usually called *target network* and the one that generates actions is called *policy network*. \n",
    "\n",
    "DQN stabilies learning by freezing the target network, i.e., \n",
    "\n",
    "$$\\mathcal{L}_t(\\theta_t)=\\mathbb{E}_{(s,a,r,s'){\\sim}U(D)}[(r+{\\gamma}\\max_{a'}\\hat{Q}(s',a';\\theta_t^{-})-\\hat{Q}(s,a;\\theta_t))^2].$$\n",
    "\n",
    "During training, the policy network parameters $\\theta_t$ \n",
    "are updated per time step, while the target network parameters $\\theta_t^{-}$ are only updated every $T_{\\textrm{target}}$ steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-johnson",
   "metadata": {},
   "source": [
    "### Lunar Lander\n",
    "\n",
    "> The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector.\n",
    "Reward for moving from the top of the screen to the landing pad and zero speed is about 100..140 points.\n",
    "If the lander moves away from the landing pad it loses reward. The episode finishes if the lander crashes or\n",
    "comes to rest, receiving an additional -100 or +100 points. Each leg with ground contact is +10 points.\n",
    "Firing the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame.\n",
    "Solved is 200 points.\n",
    "Landing outside the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land\n",
    "on its first attempt. Please see the [source code](https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py) for details.\n",
    "\n",
    "<img src=\"graphs/lunar_lander.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267128ff",
   "metadata": {},
   "source": [
    "We will use `PyTorch` to implement the DQN algorithm. To install PyTorch via Anaconda, use the following conda command:\n",
    "\n",
    "`conda install pytorch torchvision -c pytorch`\n",
    "\n",
    "You also need to install gym [Box2D](https://www.gymlibrary.dev/environments/box2d/). Try\n",
    "\n",
    "`pip install 'gym[box2d]'`\n",
    "\n",
    "The code below is from this [notebook](https://github.com/goodboychan/goodboychan.github.io/blob/main/_notebooks/2021-05-07-DQN-LunarLander.ipynb). \n",
    "\n",
    "We first import some necessary packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "authorized-valve",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import base64, io\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a101ca3",
   "metadata": {},
   "source": [
    "We next initialize the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fde2f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (8,)\n",
      "Number of actions:  4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3743b2d5",
   "metadata": {},
   "source": [
    "We next define the neural network architecture. We consider neural networks with 2 hidden layers, 64 nodes per layer and ReLU activation function. The output layer produces the Q-values under all actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d3611c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.fc1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a316f",
   "metadata": {},
   "source": [
    "We next define some hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a6a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "# See more information about CUBA https://developer.nvidia.com/cuda-zone#:~:text=CUDA%C2%AE%20is%20a%20parallel,harnessing%20the%20power%20of%20GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f74504",
   "metadata": {},
   "source": [
    "We next define the agent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0612b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        # https://stackoverflow.com/questions/62261793/what-happens-when-we-call-cpu-data-numpy-on-a-pytorch-tensor\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        # Obtain random minibatch of tuples from D\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ## Compute and minimize the loss\n",
    "        ### Extract next maximum estimated value from target network\n",
    "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        ### Calculate target value from bellman equation\n",
    "        q_targets = rewards + gamma * q_targets_next * (1 - dones)\n",
    "        ### Calculate expected value from local network\n",
    "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        ### Loss calculation (we used Mean squared error)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1677a9bd",
   "metadata": {},
   "source": [
    "The soft update is used to improve the stability. See this [post](https://ai.stackexchange.com/questions/21485/how-and-when-should-we-update-the-q-target-in-deep-q-learning). We next define the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e60fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d92f83c",
   "metadata": {},
   "source": [
    "Notice that we use deque here to store the data. deque is very similar to list. It is preferred over a list in the cases where we need quicker append and pop operations from both the ends of the container, as deque provides an O(1) time complexity for append and pop operations as compared to list which provides O(n) time complexity. In addition, if the number of items in the input iterable is greater than buffer size, then deque discards the left-most items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-dubai",
   "metadata": {},
   "source": [
    "Next, we train the DQN agent. Again the tradeoff is between performance and running time. The current parameters solve the environment and training runs for about 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bff3bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -208.70\n",
      "Episode 200\tAverage Score: -147.92\n",
      "Episode 300\tAverage Score: -98.427\n",
      "Episode 400\tAverage Score: -16.06\n",
      "Episode 500\tAverage Score: -60.48\n",
      "Episode 600\tAverage Score: 2.5977\n",
      "Episode 700\tAverage Score: 10.847\n",
      "Episode 800\tAverage Score: 52.43\n",
      "Episode 900\tAverage Score: 102.87\n",
      "Episode 1000\tAverage Score: 133.31\n",
      "Episode 1100\tAverage Score: 107.60\n",
      "Episode 1200\tAverage Score: 87.560\n",
      "Episode 1252\tAverage Score: 201.91\n",
      "Environment solved in 1152 episodes!\tAverage Score: 201.91\n"
     ]
    }
   ],
   "source": [
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        state = state[0]\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "agent = Agent(state_size=8, action_size=4, seed=0)\n",
    "scores = dqn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b979cfb",
   "metadata": {},
   "source": [
    "Finally, we plot the scores and visualize the optimal policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "empirical-friday",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
